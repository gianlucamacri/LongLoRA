[{"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nTo build Video Question Answering VideoQA) systems in daily activities,  seeking capable of assisting humans answers from long-form videos with diverse and complex Existing multi-modal VQA models achieve events is a must. promising performance video images or  short clips; on of large-scale   multi- especially with the recent success However; when extending these meth- modal pre-training: challenges arise: long-form videos; ods On the to new sampling strategy is com- one hand, a dense video using putationally prohibitive. On the other hand, methods rely ing on sparse sampling struggle in scenarios where multi- event and multi-granularity visual reasoning are required. new model named Multi- In this work; we introduce modal Iterative Spatial-temporal Transformer (MIST) to adapt pre-trained models for long-form VideoQA: better MIST decomposes traditional dense spatial Specifically; temporal self-attention into cascaded segment and region selection modules that adaptively select frames and image regions that are closely relevant to the question itself Vi- sual concepts at different granularities are then processed efficiently through an attention module. In addition,  MIST iteratively conducts selection and attention over multiple to support reasoning over multiple events. The exper- layers imental results on four VideoQA datasets, including AGQA, MIST achieves NExT-QA, STAR, and Env-QA, show that state-of-the-art performance and is superior at efficiency The code is available at qithub. com; showlab_ mist.\nMain challenges of long-form VideoQA. The ques- Figure tions for long-form VideoQA usually involve multi-event; multi- grained, and causality reasoning:\nfunctions, the systems   should be able to understand and seek the answer from long-form videos with diverse events about users activities.\nCompared to understanding and reasoning over   short videos, many unique challenges arise when the duration of Fig: the video increases, as shown in 1: 1) Multi-event rea- soning: The long-form videos usually record much more The questions about these videos thus naturally re- events_ the systems to perform complex temporal reasoning Iquire e.g,, multi-event reasoning (Ql in Fig: 1), causality (Q3), 2) Interactions among different granularities of visual etc. concepts. The questions of short-clip videos usually involve the interactions of objects or actions that happened simulta - neously, while questions for long-form videos could involve more complex interactions of objects, relations, and events Fig: across different events; e.g,, Q2 in 1_\n1. Introduction\nOne of the ultimate goals of Video Question Answering (VideoQA) systems is to assist people in solving problems in everyday life [13,27,41], e.g , helping users find some- reminding them thing; they and assisting them what did, accomplishing complex tasks, etc_ To achieve such while\nvision-language methods [2,7, 10, 24,29, 31 Current over images or short clips span- 32,51,52] excel at QA they ning several seconds. In other words, excel at learn- ing multi-modal  correspondences single cap between tion with one or few Their tremendous progress events is   fueled by 1) pre-training large- these years over on short-clip-language scale image-language [22,37,38] and\nCurrently at Google Brain_\nshown in Fig: 2 MIST comes from simple finding that for long-form VideoQA, it is not necessary t0 consider the like details of all events in a video, what dense self-attention over all patches do_ The model only needs to consider the general content of all events and focuses on the de question-related Thus, MIST de- tails of a few events_ composes dense joint spatial-temporal self-attention into question-conditioned cascade segment and region selection along spatial-temporal  self-attention module with over multi-modal multi-grained features. The cascade selection reduces the computation cost and benefits the performance by focusing 0 the question-related segments and regions. over segments and image patches, bet- The self-attention ter captures interactions among different granularities of vi- In addition, through iteratively conducting sual concepts_ selection and self-attention, MIST can reason over multiple events and better perform temporal and causal reasoning:\nexperiments on  several  VideoQA datasets We conduct with relatively longer videos, AGQA 14], NExT-QA [44] STAR [42], and Env-QA [11], with an average video du- The experimental results 12s to 44s_ ration varies from perfor approach   achieves show that state-of-the-art our mance. Further ablation studies verify the effectiveness of components. Moreover; quantitative and qualitative key the results also show that our method provides higher efficiency and reasonable evidence for answering questions.\nDiagrammatic illustration of MIST Figure It revises 2 layer standard  spatial-temporal self-attention into two modules a cascade selection module that dynamically eliminates question- irrelevant image regions, and a self-attention layer reasoning over multi-modal multi-grained visual concepts. The proposed mod- ules further iterate multiple times t0 reason over different events \n2. Related Work\ndatasets [2,33], and 2) end-to-end multi-modal Transform- align- ers [1-3,10,37,40], which is superior at learning the ments between images with texts.\nVideo question   answering: Video Question Answer ing is one typical type of vision-language task studied for Some datasets [20, 47] focus on short clips many years. about daily human activities, eg-, sports, household work MovieQA [39], Some others,  such TVQA [25], etc. as long videos cropped and Social-IQ [50], mainly focus on from movies or TV series for evaluating the understanding of the plot and social interactions, where subtitles play an essential role_ [11,14,42,44] aim to evaluate Recently; more complex spatial-temporal reasoning long-form over videos, e.g , causality, sequential order; etc Current works achieve promising results on the first two types of bench- marks, while struggle on the last one, which is our focus_\nthese multi-modal Transformers  rely However; on the increasing dense self-attention with the computation cost exponentially over time especially when adapting to long- To make the dense self-attention computation- form videos_ ally feasible in processing videos, almost all current state- sample-based of-the-art pre-trained Transformers are sparse methods, e.g;, [2,40] only sample 3 or 4 frames per video regardless of its length. If we simply adapt these pre-trained models to long-form videos with the same sampling strat- egy, there will be a domain gap between the pre-training and downstream  VideoQA tasks. In pre-training, the sparsely sampled frames of a short video depict a coherent action they likely to be random shots for part of events while are long video Recently, some early attempts process the in video hierarchically [5], which splits the video into several segments and performs QA oly o aggregated segment- efficiency issue, but is still level features_ It can ease the hard to capture complex interactions among multi-grained concepts.   Thus, leveraging the advantages of models pre- images Or short videos and addressing the chal- trained from lenges of long-form VideoQA is worth exploring:\nterms of methodology, early-stage works  proposed In various LSTM or Graph Neural Network-based models to capture cross-modal [28,35, 54] o motion-appearance in- 45] integrates graph teraction [12,23]. One recent work modeling into Transformers to explicitly capture the objects and their relations in videos_ In addition, with the great vision-language Transformers, many success of pre-trained works [2,10,40] directly fine-tune the pre-trained model on simple yet ef- downstream VideoQA tasks_ [5] proposes fective fine-tuning strategy to hierarchically process videos with pre-trained Transformers_\nIn this paper; a new model named Multi- we propose modal Iterative Spatial-temporal TTransformer (MIST), as\nCompared to previous works, this paper is an early at-\ntempt to specifically focus 0n the challenges of long-form VideoQA for Transformers-based methods. Specifically, we revise the self-attention mechanism to better perform multi-event, multi-grained visual concepts reasoning:\nwhere y is the predicted answer chosen from the candidate answers (i.e,, answer vocabulary Or provides choices), de- noted and 0 is the set of trainable parameters of as A, VideoQA model F.\nTransferring pre-trained models downstream to Many transfer  pre-trained tasks. works   try vision- to language  Transformers, such CLIP [37], into down- as object tasks, detection image [15], stream e.g , gen video-text eration [36], and retrieval [9, 32, 48, 53] CLIPAClip [32] proposes various aggregation methods for pooling; Transformer; CLIP features, eg;, to better mean represent a video. CLIP2 Video [9] proposes a temporal dif- ference block to better capture motion information. Sim above  methods, we preserve the strengths of ilar to the pre-trained models and improve their weaknesses on down- long- stream tasks, but this on another one, works focus form VideoQA where the main focus is on multi-event and multi-granularity reasoning\nFig: pipeline of our proposed Multi 3 In we present the Transformer; MIST. Modal Iterative Spatial-temporal MIST answers the question in three steps: 1) utilize pre trained model to extract the input features, 2) iteratively per- form self-attention over a selected set of features to perform multi-event reasoning, 3) predict the answer based on the obtained video, question, and answer features_\n3.1. Input Representation\nExisting vision-language Transformers are good at rep resenting images To adapt them to handle or short clips. the long-form video, we first split the video into K uniform length segments,  where each segment contains T frames In addition, each frame is divide into N patches_ Note that_ for the simplicity of notation, the [CLS] token for image patches and frames are counted in N and T.\nLong-form video modeling: With the success of great short-term video understanding in recent years, some pio neer works [8,43] have started to focus on long-form video They modeling for action recognition Or localization tasks increasing the efficiency f processing long mainly focus on video features. [8] proposes short-term feature extraction and long-term memory mechanisms that can eliminate the during need for processing redundant video frames training: [30] proposes to replace of the video with compact parts audio cues to succinctly summarize dynamic audio events and are cheap to process. [16] introduces structured multi to improve effi scale temporal decoder for self-attention kciency: The above methods utilize the natural characteris tics of videos to reduce the computation. In contrast, this paper considers the characteristics of QA tasks to use the guide to reduce computation. question as\nThe vision-language Transformer; like CLIP All-in-one with frozen parameters, extracts patch-level features of all cK }, where zk z2 RTxNxD {2l , segments, x is the feature of k-th segment; where D is the dimension of each patch-level feature. The patch-level visual token fea- tures will be used to obtain frame and segment features in following modules the Since the segment features are sep arately extracted, to indicate their temporal positions in the whole video, we add position embedding Pt \u20ac {o(i)li [0, K T]} for each token with their frame index:\nFor the text part, the question is tokenized as a sequence of words, and then fed into the vision-language Transformer word-level features Xw {W1, } , where W1 to WM get corresponds to [CLS] and WM are words in question. W2 .\nIterative Attention.  Many existing works [4,6, 17, 34] improving computation efficiency. are for Some of them propose similar iterative attention mechanisms t0 ours. [34] image classification  model to itera- a recurrent proposes tively attending on a sequence of regions at high resolution Perceiver 17] revises  self-attention in Transformer to an asymmetric attention to iteratively distill inputs into a tight feature, allowing it to handle large inputs. TimeSformer [ proposes various self-attention schemes for video classifica- tion models to separately apply temporal and spatial atten- Our model differs in utilizing multi-modal correspon- tion dence (i.e , vision and question) to iterative attention_ guide\n3.2. Iterative Spatial-Temporal Attention Layer\nThe Iterative   Spatial-Temporal Attention layer (ISTA aims to iteratively select the segments and regions among questions and then perform long video conditioned on Specifically, multi-event   reasoning over   selected ones ISTA contains three steps: segment selection, region selec - tion, and spatial-temporal self-attention; as shown in Fig:\nSegment Selection: Given a of image patch features set T, we calculate the features of segments and the question segments by perform- then select the patch features of Topk ing cross-modal temporal attention and differentiable top-k selection.\n3. Method\nSpecifically, to perform temporal attention, the frame features are first obtained by pooling the features in spa- tial dimension: the t-th frame feature in k-th segment is calculated as fk pool(zk,1,xk,2, where wt,n in- xt, N), dicates n-th patch at t-th frame of k-th segment. Then, the are obtained by pooling frames features segment features\nThe goal of a VideoQA task is to predict the answer for a given video V and a question q, formulated as follows:\nFigure 3_ Architecture of   MIST MIST first divides video into several segments and utilizes the pre-trained (PT) video encoder to extract the feature of each one. Then, MIST iteratively performs self-attention over a selected set of features to reason over multiple events_ Finally, it predicts the answer by comparing the combination of video and question features with answer candidate features_ Note that the \"PT Video Encoder\" in the figure can also be image-based encoders_\nX {wk K_1' and question features g, we first perform cross-modal temporal attention among S given q, and then conduct top-k feature selection over X; as formulated:\nlinear where g4 and gs projection layers for different are differentiable top-k selec- types of features, selector is spatial features of Topk tion function to choose the seg The top-k selection can be implemented by expand ments_ ing the Gumbel-Softmax   trick [18] or based on optimal [46] for ranking and sorting: transport  formulations In simply conduct Gumbel-Softmax sampling this paper; we Topk times with replacement to achieve top-k selection. Note that we sample the segments  with replacement be- question could only involve one cause, in some cases, the hope We the model learns t0 enhance the most segment: related segment in such cases by re-sampling it, instead of forcing it to select an irrelevant segment; as sampling with- replacement will do. See supplementfor more discus- out The output of the module is sion about Top-k selection: xTxNxD {wklk: \u20ac B} \u20ac RTopk Xt where B is the set of = Topk segments selected indexes_\nKey Figure 4 components of Iterative Spatial-Temporal Atten- tion Layer:   Since region selection follows the same architecture segment selection, we only show its inputs and outputs_ as\nalong the temporal dimension: fk, fl ff). sk: pooll ( question feature is similarly obtained by pooling the The I(W1;- The pooling func- word features, q pool( WM ). tions can be chosen from mean pooling, first token pooling; specific type of simple MLP layer; etc. according to the vision-language Transformer: For example, for image- used language Transformers, like CLIP; the first token pooling extracting frame and question features and can be used for obtaining segment features. pooling over frames for mean\nRegion Selection: For the T-th sampled frame, we want to select its most relevant patches with the question. Given its region feature of one frame Xv {ck, In \u20ac [1, N],k \u20ac along with question q, perform cross-model atten- 3 we\nGiven the segment features S patch features k=l'\nExperiments 4.\ntion over spatial patches of the T-th sampled frame and se- most related patches. Topj lect the This can be formulated as:\n4.1. Datasets\nWe model four recently   proposed evaluate our on challenging datasets for the long-form VideoQA, namely AGQA 14], NExT-QA 44], STAR 42] and Env-QA [11].\nAGQA is an open-ended VideoQA benchmark for com positional spatio-temporal reasoning: We use its v2 version a more balanced distribution, as the dataset cre- which has It provides 2.27M QA pairs over 9.7K ator recommended. videos with an average length of 30 seconds NExT QA a multi-choice VideoQA benchmark for causal and tem- is reasoning: It contains a total of 5.4K videos   with poral an average length of 44s and about 52K questions STAR is another multi-choice VideoQA benchmark for Situated Reasoning: clips STAR contains 22K video with an aver- age length of 12s along with 60K questions Env-QA is an open-ended VideoQA benchmark for dynamic environment understanding: It contains 23K egocentric videos with an average length of 20 seconds collected on virtual environ- ment AIZTHOR [21] along with 8SK questions.\nembedding layers for linear feature where hg and hx are projection: The output of the given each frame is XT RTop;xD_ Finally, we stack the selected patch features of {Xclr \u20ac [1,Topk all selected frames t0 obtain Xst Tl\nSpatial-Temporal  Self-Attention. Given the selected frames and selected regions, along with the question, we self-attention layer to reason out fused aim to employ feature vector to jointly represent the question and video_\nRegarding the inputs of self-attention, since the main computation cost comes from too many patches (K xTx N about thousands of patches), we only keep the selected ones_ While for temporal information, we keep all segments as the total number is only K (usually less than 10), which doesn bring heavy cost and can benefit more comprehensive multi- event reasoning\nFor each benchmark we follow standard protocols out- works [1,5, 11, 14] for dataset processing; lined by prior metrics, and settings. Please see supplementfor details.\nSpecifically, we first add type embedding to indicate the types feature, e.g- image region, segment Or word. The type embedding is formulated Ph \u20ac {Dh(h)lh \u20ac [1,3]} to each trainable embedding indicating where Oh is feature for layer: Then, a standard multi-head attention is performed to obtain the contextual features of all input tokens, formulated as:\n4.2. Implementation Details\nOur proposed method can be built upon most of the pre- trained multi-modal Transformers. In OUr experiments, we typical types of pre-trained models, CLIP Vit try two B/32) [37] for image-language pre-training models and All- 40] for video-language pre-training in-One-Base model denoted as MIST -CLIP and MIST -AIO respectively. In MIST Topj Topk 2 and 12 in cascade selec we set tion module and the layer of ISTA L For all videos_ 2_ we sample 32 frames per video, and split them into K AdamW is utilized to optimize model training: segments. Our model is trained on NVIDIA RTX A5O00 GPUs and implemented in PyTorch:\nDx, and Dw are linear transformation where Qs\"\nTay- Iterative Execution of ISTA: A stack of L ISTA modelling multi-event interactions between a ers is used for given question and video, where the updated segment fea- tures and word features are fed into next layer: The output of each layer {XO)}L_ prediction. is used for answer\n4.3. Comparison with State-of-the-arts\n3.3. Answer Prediction\nWe compare our model with the state-of-the-art (SOTA_ VideoQA datasets (i.e-, AGQA v2, NExT; methods on four STAR, and Env-QA), as shown in Tab. and 4 re- 3 2, 13 We can see that our proposed method achieves spectively: state-of-the-art performances and outperforms the existing methods on all datasets. The performance relatively is gain limited on Env-QA, because its videos are recorded in virtual environment, AIZTHOR: There is a domain gap for CLIP feature, while previous SOTA uses the features pre- trained on virtual environment data.\nSpecifically, we mean pool the token features of all ISTA X(L) ). MeanPool(XOl layers, Xo In addition, fol- lowing the work [49], we calculate the similarity between the Xo and the feature of all candidate answers XA \u20ac A} obtained by using the pre-trained model Fi- {xa/a nally, the candidate answer with the maximal similarity is considered as the final prediction y_\nNotably, among SOTAs, TEMP[ATP] [5] uses the same MIST -CLIP feature, CLIP (ViT-B/32), And All-in- as MIST -AIO also use the same feature, 40] and All- one\nlosst During training; we optimize the softmax cross-entropy between the predicted similarity scores and ground truth\nTable 1. QA accuracies of state-of-the-art (SOTA methods on AGQA v2 test set\nQA accuracies of variants of MIST on AGQA v2 and Table 5. NExT-QA\nTable 2. QA accuracies of SOTA methods on NExT-QA val set\nlarge passes these models with margin on questions requir- ing causality O multi-event reasoning, e.g,, Sequencing in Temporal in NExT-QA, Interaction AGQA v2, Causal & & Prediction in STAR, and Event in Env-QA These re - sults demonstrate that Our proposed model can effectively address the unique challenges of long-form video QA.\n4.4. Comparison with Baselines\nlong we devise several alternative solutions for Here form video modeling to replace our proposed ISTA . Specif- ically, in our CLIP-based MIST framework, we compare ISTA against other solutions, by fine-tuning the same pre- training input representation on AGQA v2 dataset:\nTable 3. QA accuracies of SOTA methods on STAR val set:\nMeanPool: It simply takes the average of frame features as the representation of the whole video_ seqTransf Trans.-Frame: We follow the in type CLIPAClip,   utilizing perform Transformer self- to attention over frame features t0 represent the video. Trans.-Patch: This model is similar t0 Trans.-Frame, but it performs self-attention over all patch tokens Divided STA: We follow TimeSformer [4] in the video to   perform classification uni-modal two-step model Space-Time Attention over image patches.\nTable 4. QA accuracies of SOTA methods on Env-QA test set\nin-One-Base. Compared to these methods, it can be found that our two versions of models, which build upon differ- ent types of pre-trained models, achieve substantial perfor- on all datasets_ gains mance\nfrom the question type breakdown of each Moreover; dataset, compared that with AIO and we can see significant per- Temp[ATP], our model obtains a much more formance boost on questions that multi-grained vi- require sual concepts reasoning (i.e. AGQA Rel.-act , Obj.-act. on which  mainly  require information v2) than those within AGQA one frame (i.e;, Obj -rel v2 and Descriptive on on NExT-QA) In addition, we can see that ur model sur-\nFrom the results in Tab. 5 we can see that ISTA achieves substantial improvement over other variants with larger than In addition, we 3% improvement on the overall accuracy: Transformer-based find that for long-form VideoQA the prediction models are much better than the Mean- answer Pool method while in the video-text retrieval field, some- times mean pooling is even better: The reason could be that long-form video is often complex and di- the content of\nTable 6. Ablations results of ISTA on AGQA v2 and NExT-QA\nsimple method for aggregating all frame fea- verse, and pooling, may cause information loss. tures, such as mean And long-form video QA requires more powerful temporal and spatial reasoning ability to focus O some details of a video, while mean pooling only performs well on capturing overall content.\nMoreover we can see that it is helpful to consider region information in long-form QA (Divided STA and Trans. outperform But, neither dense self- Trans.-Frame) Path attention nor divided STA considers the interaction among multi-grained concepts; thus, the performance improvement is limited. And after integrating different granularities of vi- sual concepts during reasoning, our method benefits the per- All the above findings show that our method is formance. effective, and transferring pre-trained transformers to long form video QA is a challenging topic worth exploring:\nFigure 5_ Performances of MIST with different settings. (a-e) Performances of MIST with different hyper-parameters on AGQA (f) Performance of variants of MIST under different GFLOPs v2_ on AGQA v2, where GFLOPs rise with the number of sampled frames increase\n4.5. Ablation Study\nIn this section, we propose several sets of variants MIST to show the effectiveness of its key components_\ndoesn hurt performance too much The number of objects in the video frames is relatively small (compared with natu- ral scene images in image QA), and after temporal attention, the patch number has already been greatly reduced: So, the existing model is able to effectively focus O the appropri- mentioning that we can reduce objects. But; It is worth ate the computation cost by using a spatial selection module It may be useful when we face high-resolution or extremely complex videos in the future\nkey - Effect of each component in ISTA . We ablate mod- ules in ISTA layer; i.e-, Segment  Selection, Region Se- lection, or   Self-attention   layer;  denoted MIST wlo as SSIRSISTA, respectively:\nSS: It removes the Segment Selection mod- MIST wlo ule, and only performs region selection. Patch features with word features are fed into the self-attention module_ Segment Selection module_ MIST wlo_ RS: It removes All region features within selected segments are fed into self-attention layer: MIST wlo: STA: The segment features and selected region features are mean pooled as the output of ISTA\nEffects of different ISTA configurations.  In this part, we try different configurations of model architecture, in- cluding Topk; a number of selected segments select patches Topj; ISTA layers L, and the number of segments K. The results are shown in Fig: 5 (a-d).\nAGQA v2 and NExT-QA The results of these variants on We can see that removing Segment Se- are shown in Tab. 6. larger than 3% accuracy drop. The reason lection causes could be that removing it will introduce a lot of irrelevant re- gion information when predicting the answer and thus hurt the performance. 6 also shows that Segment Selection Tab: is important for multi-event reasoning because removing it hurts the performances on questions requiring temporal rea- soning; i.e., Causal and Temporal.\nFirst, Fig: 5 (a) shows that the performance is relatively The performance slightly drops Topk. under the small pgood Topk The reason could be that large k if further increases_ will introduce either some incorrect segments O repeated Incorrect segments will bring misleading infor- segments. causing performance drops. Repeated segments lead mation larger number of repeated region features, causing it to difficult for the model to focus 0 question and segment Topj For the number of selected patches information: as shown in Fig: 5 (b), we can see that with the increase of performance first increases and then reaches sta- Topj, the bility: The reason for this phenomenon could be that when selecting too few image regions, it may incorrectly filter answering questions. some regions used for And when the\ndrop In addition; the performance on both datasets is significant when removing Spatial-temporal self-attention The reason may be similar to MeanPool. We need a power- multi-grained reasoning: ful model to capture\nthat  removing  spatial Moreover; attention see we can\nFigure 6. Qualitative results of MIST on NExT-QA dataset We visualize its prediction results along with spatial-temporal attention where the frames with purple and red outlines indicate the highest temporal attention score in the first and second ISTA layers, respectively:\n4.7. Qualitative Results\nselected regions increase, though it introduces some irrele- regions, since the patch number after segment selection vant is already relatively small, the self-attention module can ef- fectively attend to relevant regions.\nWe visualize some success and failure cases from the Fig: NExT-QA dataset in It can be seen that our model 6_ explicitly select video clips and image regions relevant can to the question. We can also find that it is difficult for the model to correctly select segments and regions, when the question mainly involves some concepts related to social Existing pre-trained models may not emotions_ well un derstand the correspondence between abstract concepts and videos. However; we believe that these issues can be allevi- ated by proposing better pre-trained models on short videos and our method is easy to build upon the stronger ones_\nFor the number of ISTA layers, as shown in Fig: 5 (c) with the increase of L, the performance increases first and then reaches stability or slightly drops. It shows that stack- ing several layers of ISTA can benefit multi-event reason- In addition, the performance doesn t constantly in ing: larger L_ crease with This is probably due to (1) the datasets deeper network and (2) the large enough to train are not questions usually only involving two O three events, so con- bring sidering more benefits_ Fig: more events may not 5 (d) shows that when varying the number of video segments performance tends to suffer when the videos under- are segmentation, because, in this case, each segment spans relatively long duration, and hence the Segment Selection More importantly, all those findings im- module is useless_ ply that MIST is effective in multi-event reasoning by at- tending t0 multiple segments.\n5. Conclusion and Future Work\nThis   paper introduces Multi-modal   Iterative   Spatial- temporal Transformer for long-form VideoQA, which de- composes dense self-attention into a cascade segment and region selection module to increase the computation effi- ciency along with a self-attention layer to reason over vari- grained visual concepts. In addition, by iteratively con- ous ducting selection and attention layers; MIST better over performs multi-event reasoning: Experimental results on four VideoQA datasets show its effectiveness and advan- tages in efficiency and interpretability. For future work, al- though MIST has increased the number of sample frames the ability to capture high-frequency motion may still need In addition, patch features naturally have to be improved complex object-level reasoning: some limitations in Re- cently, there have been some pre-trained models for specifi- cally modeling actions and objects It may be interesting to try more types of pre-trained models Or even combine many reasoning: of them to achieve more general\n4.6. Computation Efficiency\nIn Fig: 5 (e), we can see that the accuracy increases sig- nificantly when  sampling more frames. It indicates that sampling more frames for long video QA tasks could be Though current datasets  don 't provide videos necessary. long or hours duration, such with several minutes videos likely to be encountered in real application scenarios. are Efficiency issues thus could be a more crucial consideration Fig: in such cases_ 5 (f), we compare GFLOPs vS. In aC- long- curacy for ourS other form video QA methods_ against It can be seen that the standard Transformer over patches is computationally expensive. The frame-based method is lightweight in computation, but its performance is limited. little extra  computation Our method requires only but achieves much better performance. It is also worth men- tioning that MIST doesn't enlarge model size for higher ef- ficiency: Compared with other methods, it oly contains some extra shallow networks for spatial-temporal attention_\nAcknowledgements.  This project is supported by the Na- Research  Foundation,  Singapore tional under its NRFF Award NRF-NRFFI3-2021-0008. The computational work for this article was partially performed 0n resources of the National Supercomputing Centre, Singapore.\nReferences \n Now the paper ends. \nQuestion: What are some potential directions to further improve the proposed model to address its current limitations?", "output": "Answer: The paper mentions several directions for improvement:\n\n1. Sampling more video frames to capture high-frequency motion information better. The current model may still miss some fine-grained details due to sparse sampling.\n\n2. Using more powerful pre-trained models that specifically model objects and actions. The current patch-level features have limitations in complex object-level reasoning. \n\n3. Combining different pre-trained models to achieve more general reasoning ability. Each model has its own strengths and weaknesses, so an ensemble may perform better.\n\n4. Handling longer videos that are more commonly seen in real-world applications. The current model focuses on efficiency but may still struggle with hours-long videos.\n\n5. Exploring spatial selection modules to reduce computation for high-resolution videos. The current model only uses region selection, but spatial selection may be useful for very complex videos.\n\nIn summary, sampling strategies, using specialized pre-trained models, and further improving efficiency for extremely long videos are potential directions to enhance the proposed model."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nRecent DEtection TRansformer-based DETR, models obtained remarkable performance. Its success cannot have be achieved without the re-introduction of multi-scale feature fusion in the encoder: However; the excessively increased to- kens in multi-scale features, especially for about 759 of low level features, are computationally inefficient; which quite hinders real applications of DETR models: In this paper; we present Lite DETR, a simple yet efficient end-to-end object detection framework that can effectively reduce the GFLOPs of the detection head by 60% while keeping 99% of the origi nal performance. Specifically; we design an efficient encoder block to update high-level features (corresponding to small resolution feature maps) and low-level features (correspond- ing to large-resolution feature maps in an interleaved way In addition, to better fuse cross-scale features, we develop key-aware deformable attention to predict more reliable attention weights. Comprehensive experiments validate the effectiveness and efficiency of the proposed Lite DETR, and the efficient encoder strategy can generalize well across existing DETR-based models: The code be available will IDEA-Research/Lite in https / /github _ com DETR\nFigure 1. Average precision axis) versus GFLOPs (X axis) for training data different detection models on COCO without extra All models except EfficientDet [29] and YOLO series [12,30] use Tiny as backbones. Specifically, two markers ResNet-50 and Swin- Tiny; respectively: on the same line use ResNet-50 and Swin- In dividual markers only use ResNet-50_ Each dashed line connects adding algorithm variants before and after algorithm. The size our of the listed models vary from 32M to 82M:\nRecently, DEtection TRansformer [1] DETR introduces Transformers into object detection, and DETR-like models have achieved promising performance on many fundamental vision tasks, such as object detection [13,36,37], instance segmentation [5,6,14], and pose estimation [26,28].\n1. Introduction\nConceptually, DETR [1] is composed of three parts: backbone, a Transformer encoder; and a Transformer de Many research works have been improving the back coder: bone and decoder parts. For example, the backbone in DETR is normally inherited and largely benefit from can pre trained classification model 10, 20]: The decoder part in DETR is the major research focus, with many research works trying to introduce proper structure to DETR query and im prove its training efficiency 11,13,18,21,36,37]. By con\nObject detection aims to detect objects of interest in im ages by localizing their bounding boxes and predicting the In the past decade, re- corresponding classification scores markable progress has been made by many classical de tection models [23, 24] based on convolutional networks_\nFeng This work was done when Li was an intern at IDEA\ngreat amount of computational original tokens and save At the end of the encoder block; we let low-level to cost: kens query all feature maps to update their representations, thus maintaining multi-scale features. In this interleaved high-level and low-level features in different way; we update frequencies for efficient computation.\ntrast, much less work has been done t0 improve the encoder part: The encoder in vanilla DETR includes six Transformer layers, encoder stacked on top of a backbone to improve its Compared with classical detection feature representation_ models, it lacks multi-scale features, which are of vital im portance for object detection, especially for detecting small objects [9,16,19,22,29]. Simply applying Transformer en coder layers on multi-scale features is not practical due to the prohibitive computational cost that is quadratic to the number of feature tokens For example, DETR uses the C5 feature map, which is 1/32 of the input image resolution, to apply the Transformer encoder: If a C3 feature (1/8 scale) is included in the multi-scale features, the number of tokens from this scale alone will be 16 times of the tokens from the The computational cost of self-attention in C5 feature map. Transformer will be 256 times high:\nMoreover; to enhance the lagged low-level feature up- key-aware deformable attention (KDA) date, we propose approach to replacing all attention layers. When performing deformable attention, for each query; it samples both keys and values from the same sampling locations in feature map. Then, it can compute more reliable attention weights by comparing the query with the sampled keys: Such an approach can also be regarded as an extended deformable sparse version of dense attention. attention or We have found KDA very effective in bringing the performance back proposed efficient encoder block: with our\nTo address this problem, Deformable DETR [37] devel deformable attention algorithm to reduce the self- ops attention complexity from quadratic to linear by compar ing each query token with only a fixed number of sampling points. Based on this efficient self-attention computation Deformable DETR introduces multi-scale features to DETR_ and the deformable encoder has been widely adopted in subsequent DETR-like models 11,13,18,36].\nTo summarize, Our contributions are as follows_\nWe propose an efficient encoder block to update high- level and low-level features in interleaved an way; significantly reduce the feature tokens for which can efficient detection. This encoder can be easily plugged into existing DETR-based models To enhance the lagged feature update, we introduce key-aware deformable attention for more reliable atten- tion weights prediction: Comprehensive experiments show that Lite DETR can reduce the detection head GFLOPs by 60% and main - tain 99% detection performance. Specifically, our Lite- DINO-SwinT achieves 53.9 AP with 159 GFLOPs.\nlarge number of query tokens intro However; due to duced from multi-scale features, the deformable encoder high computational cost  To reveal this still suffers from analytic experiments as shown problem; we conduct some 2 using a DETR-based model DINO [36] to in Table and analyze the performance bottleneck of multi-scale features_ Some interesting results can be observed. First; the low-level (high-resolution map) features account for more than 75% of all tokens. Second, direct dropping some low-level features (DINO-3scale) mainly affects the detection performance for small objects (AP_S) by a 10% drop but has little impact on large objects (AP_L).\n2. Related Work\nPreliminary: DETR [1] regards object detection as direct prediction problem and uses global loss to a set-based set force unique predictions via bipartite matching: Vanilla DETR [1] only uses single-scale features from the last stage 3 of the input image resolution), i.e. of the backbone Xfeat RNx D where Dis the feature dimension and N is the total number of flattened features These features will then be processed by the encoder with dense self-attention layers for feature fusion and enhancement: The use of an 16] in CNN-based models encoder is similar to FPN These refined features will be queried by the decoder to detect objects by predicting classification scores and regressing bounding boxes. In general, as DETR only uses high-level features that are of low resolution, these features lack rich local details that are critical for small object detection.\nInspired by the above observations, we are keen to address question: can we uSe fewer feature scales but maintain important local details? Taking advantage of the structured multi-scale features, we present an efficient DETR frame Specifically, we design a simple work; named Lite DETR including several deformable yet effective encoder block self-attention layers, be plug-and-play in any which can multi-scale DETR-base models t0 reduce 62% 78% en coder GFLOPs and maintain competitive performance. The encoder block splits the multi-scale features into high-level C6,C5,C4) and low-level features (e.g , (e.g  features C3)_ High-level and low-level features will be updated in an in- terleaved way to improve the multi-scale feature pyramid. That is, in the first few layers, we let the high-level features query all feature maps and improve their representations, but keep low-level tokens intact: Such a strategy can effectively reduce the number of query tokens to 5% 25% of the\nImproving Decoder Design of DETR: Recently, DETR- based detectors have seen more rapid progress 13,18,21,33, 36] compared to classical detectors [2,24]. As a result; DINO [36] achieved first place in COCO 2017 object detection for DETR-like model. the first time as Most works focus\nTable 1. GFLOPs of DINO based on ResNet-50 with four feature scales and three feature scales, respectively: We use ResNet-50 as the backbone and evaluate on COCO val2017 trained with 12 epochs. 100% means we use all the feature tokens, while 25% means we use three high-level features which accounts for 25% of all tokens_\nimproving the Transformer decoder in DETR for better on performance and faster convergence speed. Specifically, positional Meng et al. [21 propose to decouple content and information in the decoder to provide better spatial priors in localization. [18, 37] further design better formulations of positional queries than previous works. The one-to-one label- assignment scheme is also widely discussed in [4,11,13,36] assignment Moreover; design for a better some models [33,36,37] better decoder query initialization by utilizing dense priors from the encoder:\nTable 2 The token ratio of each feature scale in a 4-scale feature pyramid.\n3. Method\n3.1. Motivation and Analysis\nIn this part, we first analyze why existing DETR-based interesting models are still inefficient and then show some Multi-scale features are of vital importance for observations They detecting objects of diverse scales. composed of are multiple feature scales ranging from high-level (low resolu- tion) to low-level (high resolution) features. Each lower-level more tokens than its previous fea- feature map contains 4x ture level_ From Table 2, we can observe that the number of tokens in low-level features quadratically increases, whereas the three higher-level scales account for only about 25%.\nImproving Multi-Scale Feature Extraction of DETR:\nThough DETR-based models with multi-scale features have shown promising performance [36, 37], especially for small object detection, their efficiency is still a concern for many applications. In fact, multi-scale feature extraction has been efficiency widely studied in many CNN-based detectors for and effectiveness, such as FPN 16], BiFPN [29], PANET [19], and NAS-FPN [9], yet the efficiency of multi-scale DETR is under-explored. Recently, a few works [27,34,35, 37] have attempted t0 design efficient encoders.\nFurthermore, we take a DETR variant DINO [36] as a pre- liminary example. What will happen if we simply drop the low-level feature (S4 in Table 2) in its deformable encoder to reduce the computational costs? In Table 1, a reduced efficiency DINO-3scale model trades a 48 % in terms gain a 4.99 average precision (AP) of GFLOPs at the cost of and even 10.2% AP on small object detection deterioration_ objects is competitive. large That is, However; the AP on high-level tokens contain compact information and rich se- mantics to detect most objects. By contrast; a large number of low-level tokens are mainly responsible for local details to detect small objects. Meanwhile, multi-scale features con - tain many redundant tokens, especially low-level features Therefore, we would like to explore how to efficiently update multi-scale features by primarily focusing on constructing better high-level features:\nDeformable DETR [37] proposes deformable attention, which can be used in the DETR encoder t0 sparsify the val- ues in a self-attention layer by sampling only a few values for each query. The proposed deformable encoder leads to detection results with an affordable computation cost, good which has been widely acknowledged and applied in many vision tasks However; compared with single-scale detectors, the computation cost of multi-scale deformable DETR is still high for efficient usage Based on the strong deformable encoder; some works attempt to improve its efficiency. Ef- ficient DETR [33] proposes to use fewer encoder layers by leveraging encoder dense priors for decoder query initializa - tion. Sparse DETR [25] proposes to sparsely update salient tokens in the encoder to reduce the number of queries with scoring network: In fact, the encoder is responsible for feature extraction, but Sparse DETR introduces multi-layer detection loss in encoder layers, making it hard to generalize to other DETR-based models.\nprioritize high-level feature updates in In this way; we can layers, which could significantly reduce query tokens most for a more efficient multi-scale encoder: To sum Up, this work design general solution for highly efficient DETR aims to based detectors and maintain competitive performance.\nRecently, DETR++ [34] proposes t0 replace the encoder with BiFPN [29] and VIDT [27] develops a stronger decoder to remove the encoder: IMFA [35] proposes to sample sparse interesting areas of multi- scale-adaptive features from some However; the performance of these models scale features_ still largely lags behind improved detectors 13,36] based on the deformable encoder:\n3.2. Model Overview\nFollowing the multi-scale deformable DETR [37], Lite DETR is composed of a backbone, a multi-layer encoder; multi-layer decoder with prediction heads. The overall and a model framework is shown in Fig: 2. Specifically, we split\nthey Figure 2. Illustration of the Lite DETR framework We use S2 S4 to indicate the features from different backbone stages_ That is, C3 in ResNet-50 [10]. S1 is acquired by further downsampling Cs by a ratio of 0.5. In this figure, we take S1 S3 as correspond to C5 high-level features as an example. Moreover; (a) is the proposed high-level feature update discussed in Sec. 3.4 and (b) is the low-level feature cross-scale fusion discussed in Sec. 3.5. In each efficient encoder block; the multi-scale features will go through high-level feature A times and then conduct low-level feature update at the end of each block: The efficient encoder block will perform B times_ update for\n3.4. Iterative High-level Feature Cross-Scale Fusion\nthe multi-scale features from a backbone into high-level fea tures and low-level features. These features will be updated with differ in an interleaved manner (introduced in Sec. 3.3 updating frequencies (explained in Sec. 3.4 and 3.5, in ent the proposed efficient encoder block to achieve precision and efficiency trade-off. To enhance the lagged update of low level features, we further introduce a key-aware deformable KDA) approach attention described in Sec. 3.6)\nIn this module, the high-level features FH will serve as queries (Q) to extract features from all-scales, including the low-level and high-level feature tokens. This operation enhances the representation of FH with both high-level se mantics and high-resolution details. The detailed updating This operation is highly ef process is shown in Fig: 2(a). using ficient. multi-scale feature queries in For example, the first two scales or the first three scales significantly will reduce 94.1% and 75.3% queries, respectively, as shown We also use the proposed key-aware attention in Table 2 which will be discussed in Sec 3.6,to perform module KDA, attention and update tokens. Formally; the update process can be described as\n3.3. Interleaved Update\nFrom our motivation; the bottleneck towards an efficient encoder is excessive low-level features, where most of which are not informative but contain local details for small ob jects. Moreover; multi-scale features S are structured in nature, where the small number of high-level features en codes rich semantics but lack important local features for prioritize Therefore, some small objects. we propose to different scales of the features in an interleaved manner t0 precision and efficiency trade-off: We achieve a split S into low-level features FL \u20ac RNLxd,odel and high-level features Xdmodel Fi where dmodel is the channel dimen RNn \u20ac sion, and NH and NL are the corresponding token number 33%NL) FH can contain the first three O 6% (NH two scales in different settings, for clarity, we set FH to S1,82, S3 and FL to S4 by default: FH is regarded as the primary feature and is updated more frequently, whereas FL is updated less frequently: As deformable attention has linear complexity with feature queries, the small number of frequently-updated high-level features largely reduces the computational cost: As shown in Fig: 2, we stack the effi block updates cient encoder block for B times, where each updates low-level high-level features for A times but only features once at the end of the block In this way; we can full-scale feature pyramid with much lower maintain computation cost: With this interleaved update, we design two effective updating mechanisms for FL and FH -\nwhere Concat is to concatenate low-level and high-level features into full-scale features, query Q is the initial high level features, K and V are initial features from all levels_ updated and FH is the high-level tokens, and FH are the high-level features.\nJayer high-level feature update will be stacked for multi Atimes) layers for iterative feature extraction. Note (e.g , ple FH will also update Q and the correspond that the updated ing high-level features in the multi-scale feature pyramid iteratively, which makes feature update in K and in the Interestingly, this high-level  feature updating layer: next module is similar to the Transformer decoder;  where we high-level tokens to query their fea a small number of use large number of tures similar to a self-attention and query a low-level features similar t0 cross-attention\nFigure 4. Comparison of previous efficient encoders strategies in (a) Deformable DETR [37], (b) Sparse DETR [25], and (c) Trivially lusing only the first three high-level scales. (d) Preliminary efficient encoder to only update high-level features_ We also present the results of (c) and (d) in Table 5_\nattention can be formulated as\nFigure 3. Illustration of the proposed Key-aware Deformable At layer: tention KDA\n3.5. Efficient Low-level Feature Cross-Scale Fusion\nWhere the projections are parameter matrices WA WP Rdmodel X Nu and dmodel Rdmodel WV p are the reference points of the query features and Ap;p \u20ac R(Nn+NL)xNx2 S is the multi-scale feature pyramid. With the sampled off- it computes the features with function Samp( S,p + Ap; sets Ap) in the sampled locations (p + Ap) of feature pyramid Note that no key participates S with bilinear interpolation: in the original deformable attention layer; indicating that query can decide the importance of each sampled value by only its feature without comparing it with keys. As all the multi-scale features will be the queries t0 sample loca- tions and attention weights, the original model can quickly learn how to evaluate the importance of each sampled loca tion given the queries Nevertheless, the interleaved update in OUr encoder makes it difficult for the queries to decide both the attention weights and sampling locations in other asynchronous feature maps, aS shown in Fig: 5.\nAs shown in Table 2, the low-level features contain exces- sive tokens, which is a critical factor for inefficient computa- tion: Therefore, the efficient encoder updates these low-level features at a lower frequency after a sequence of high-level feature fusion. Specifically, we utilize the initial low-level queries to interact with the updated high-level features as update tokens as well as the original low-level features to Similar to the high-level feature update, their representation we use the interaction with a KDA attention layer: Formally, we have\nis from the original low-level features, FH and FL where are the updated high-level and low-level features, respec- FL; Finally, KDA layer; tively: After we can obtain the we construct the output multi-scale features S' by concate- nating the updated Iow-level and high-level features To further reduce the computational cost; we use a lightweight feed-forward network with a hidden dimension size F of the original size. is 8 in our model.\nTo better fit the efficient encoder designs, we propose KDA) approach to sam key-aware deformable attention pling both keys a query, as shown in Fig: and values for The sampled keys and values, together with the query; 3_ will then perform standard scaled dot-product attention_ Formally, we have\n3.6. Key-aware Deformable Attention\nlayer; vanilla deformable attention the query In will be split into M heads, and each head will sample K points from each of the L feature scales as value V. Therefore, the total number of values sampled for query is Nv M X L x K. The sampling offsets Ap, and their corresponding attention weights are directly predicted queries using from two linear projections denoted as WP and WA Deformable\nwhere dk is the key dimension of a head. The computational complexity of KDA is the same as the original deformable attention as we sample the same number of values for each query: In this way, KDA can predict more reliable attention weights when updating features from different scales.\nResults for single-scale DETR-based models which use larger resolution feature map with dilation (DCS) and Deformable Table 3_ DETR-based models for improving efficiency All models are based on ResNet-50. Sparse DETR is based on an improved Deformable DETR baseline that combines the components from Efficient DETR [33]. 'rho' is the keeping ratio of encoder tokens in Sparse DETR adopt Value in the parenthesis indicates the percentage of our high-level tokens compared to the original features the result from the we official Deformable DETR codebase. The meaning of different model variants is described in Sec. 4.1_\n3.7. Discussion with Sparse DETR and other Effi- cient Variants\nthe standard mean average precision (AP) result under dif- ferent IoU thresholds and object scales.\nWe evaluate the performance of Implementation details: Lite DETR on multiple DETR-based models, including De formable DETR [37], H-DETR 11],and DINO [36]. These models share a similar structure that is composed of a back multi-layer Transformer encoder; and a multi-layer bone, Therefore, simply replace their Transformer decoder we encoder with our proposed efficient module. Other model components are kept the same as the original model. In our KDA attention, for a fair comparison, we follow deformable settings attention t0 use M-8 and K=4. Other follow the We use two backbones ResNet-50 [10] and original models. Swin-T [20] pre-trained on the ImageNet-IK [7] dataset in experiments. our\nAnother efficient way is t0 reduce encoder tokens by se lecting salient tokens in the multi-scale features, like Sparse However; there are three drawbacks to this DETR [25]: kind of approach: First, it is hard to generalize across other DETR-based models since it breaks the structured feature Or ganization. Second, the selected tokens via a scoring network optimal due to limited and implicit supervision may not be Third, it introduces other components, such as multiple aux iliary encoder detection loss, to enhance its sparse encoder As the encoder is responsible for feature representation. adding detection supervision makes it difficult to extraction, apply to existing models\"\nMoreover; we illustrate the previous efficient encoders and the preliminary efficient designs in Figure 4 for a clear comparison.\nEfficient encoder variants: In our proposed efficient en coder block; three hyperparameters control the computa tional cost, including the number of high-level feature scales H used in FH, the number of efficient encoder blocks B and the number of iterative high-level feature cross-scale Therefore, we use HL-(A+ 1) X fusion A_ B to denote each variant of our Lite DETR where L is the number of low-level feature scales, and +l denotes the default efficient low-level cross-scale feature fusion at the end of each block For example, Lite-DINO H3LI-(3+1)x2 indicates we base on DINO to use three high-level feature scales (H3LI) and two efficient encoder blocks with three high-level fusion ((3+1)x2).\n4. Experiments\n4.1. Setup\nWe demonstrate the generalization capability of Our prO posed efficient encoder on a series of DETR-based models_ We also evaluate the effectiveness of each component with ablations_\nWe study Lite DETR on the challenging MS Datasets: COCO 2017 [17] detection dataset  Following the common practice, we train on the training split and report the detec tion performance on the validation split val2017. We report\n4.2. Efficiency Improvements on Deformable DETR\nproposed lite encoder to replace In Table 3 we use our the deformable encoder in Deformable DETR and build Lite\nIn our experiments on DINO [36] with a ResNet-50 backbone, adding the encoder detection loss alone will cause a 1.4 AP drop.\nResults for Deformable DETR-based models to improve efficiency with our light encoder design. We also compare with some Table 4 efficient CNN-based models and other efficient DETR-based models_ All models except EfficientDet and YOLO series are based on ResNet-50 and Swin-T pre-trained on ImageNet-IK. Percentage in the model name indicates the percentage of Our compressed tokens The meaning of different model variants is described in Sec. 4.1. compared to the original features\naddition, after plugging in our efficient encoder; the encoder GFLOPs can be reduced by 78% 62% compared to the original ones while keeping 99% of the original performance Specifically, based on Swin-Tiny, our Lite-DINO achieves 53.9 AP with only 159 GFLOPs, which also outperforms YOLO series models 12,30] under the same GFLOPs.\nWe achieve comparable performance as Deformable DETR. Deformable DETR with around 40% of the original encoder GFLOPs. We can also observe that DETR-based models single scale of larger feature maps are computationally with a inefficient and inferior t0 multi-scale models_ In iterative high-level cross-scale fusion, we can effectively adopt high- level maps with only two or three high-level maps, which can reduce the queries in an encoder layer to 5% 25% of the original tokens Compared with other efficient variants based we achieve better performance under on Deformable DETR, computational cost: For example, we outperform the same Sparse DETR-rho-0.3 by 0.7 AP with fewer GFLOPs. In addition, Sparse DETR is based on an improved baseline that combines Efficient DETR and Deformable DETR. By contrast; our Lite-Deformable DETR is simple and effective.\n4.4. Visualization of KDA\nWe also provide the visualization of our KDA attention Fig; in our interleaved encoder in 5. Compared with de- keys; formable attention, aS we introduce our KDA attention predict more reliable weights, especially on low-level can Fig: For example, in S(a) , the sampled loca- feature maps_ tions of deformable attention in S4 (denoted with triangles) Fig: are less reliable compared to KDA. In 5(b) and (c), we observe that it is difficult for deformable attention to focus meaningful regions on the largest scale map S4 in our on KDA effectively mitigates this phe- interleaved encoder: bring nomenon, which helps extract better local features to the performance of small objects back:\nEfficiency   Improvements Other   DETR- 4.3. on based Models\nCompared with other efficient variants, OUr efficient de specific detection framework and sign is not constrained to a can be easily plugged into other DETR-based models. We 11] as examples to show the take DINO [36] and H-DETR effectiveness of our efficient encoder: The results are shown in Table 4. Compared with other recently proposed efficient DETR-like models [8,35], our model achieves significantly better performance with comparable computational cost. In\n4.5. Ablation Studies\nEffectiveness of each proposed component: In Table we show the effectiveness of our proposed components 5 We choose DINO-3scale and DINO-2scale as our baseline, which only uses the first three and two high-level feature\nusing Visualization of KDA attention in our interleaved encoder: The first and second rOw are the attention maps of Figure 5_ deformable and our KDA attention. (a) We use the the center of an object from Sl (marked with in green as query and draw top 100 sampling according to their attention weights. The sampling locations on S4 are marked with a triangle shape. (b)&(c) locations on all four scales We show top 200 sampling locations on scale S3 (b) and S4 (c) for all query tokens_ The visualization shows that KDA can produce more For clarity, we only draw the locations of 200 attention weights out of all sampling top reliable attention weights on high-resolution maps_ More visualizations are shown in Appendix_ locations (Nq X M x K; Nq is the total number of multi-scale query tokens) on S3 and on S4_\noptimal choice to stack each mod In Table 6, we explore the ule in our proposed efficient block: Based on Deformable ResNet-50 backbone, DETR [37] with we vary three ar guments that influence the computational complexity and detection performance, including the number of high-level scales H used as high-level features, efficient encoder block B, and iterative high-level feature cross-scale fusion The A performance improves when we use more high-level feature scales and more encoder blocks to update the low-level fea tures  However; further increasing the module number to (2 + 1) X 4 will not improve the performance_\nTable 5_ Effectiveness of each component on COCO val2017 trained with 36 epochs The results are based on DINO with ResNet-S0 backbone trained for 36 epochs. HL means iterative high-level feature cross-scale fusion, LL means efficient low-level feature cross-scale fusion, and KDA is key-aware deformable atten tion.\n5. Conclusion\nIn this paper; we have analyzed that multi-scale features with excessive low-level features in the Transformer en- coder are the primary cause of the inefficient computation in DETR-based models. We have presented Lite DETR with efficient encoder block; which splits the encoder tokens an into high-level and low-level features These features will be updated in different frequency with cross-scale fusion to achieve precision and efficiency trade-off: To mitigate the effects of asynchronous feature, we further proposed a key-aware deformable attention, which effectively brings the detection performance of small objects back: As a result; our proposed efficient encoder can reduce computational cost by 60% while keeping 99% of the original performance. In ad dition, this efficient design can be easily plugged into many hope DETR-based detection models We Lite DETR can pro simple baseline for efficient detection in DETR-based vide a models to benefit other resource-constrained applications.\nTable 6. Ablation study on stacking different number of each module in our efficient encoder block: All the models are built upon Deformable DETR-ResNet50 and trained for 50 epochs_\nmaps. The results indicate that each of our proposed compo requires computational cost while improving nents a small the model performance by a decent margin. KDA mainly helps improve DINO performance. Specifically, these com ponents effectively bring the performance on small objects back, for example, the APs of our efficient DINO-3scale is comparable with the original DINO-4scale model.\nLimitations: In this paper we mainly focus on reducing the computational complexity and do not optimize the run-time implementation of DETR-based model. We leave this t0 our future work:\nReferences \n Now the paper ends. \nWhat are some potential directions to further improve the efficiency and effectiveness of Lite DETR?", "output": "Possible answers:\n\n1. Further optimize the implementation and run-time performance. The proposed Lite DETR mainly focuses on reducing the computational complexity in theory. Optimizing the actual run-time performance through techniques like kernels fusion, optimized matrix operations, etc. could further improve the efficiency. \n\n2. Introduce more advanced backbone networks. Lite DETR currently uses ResNet and Swin Transformers as backbones. Using more advanced and efficient networks like EfficientNets, ResNeSt, RegNets, etc. could improve the feature representation and detection performance.\n\n3. Explore more efficient attention mechanisms. Key-aware deformable attention helps improve efficiency but there are still rooms for further optimization. Methods like local attention, axial attention, RelPos attention could bring more significant efficiency gains.\n\n4. Investigate multi-scale feature extraction strategies. The current interleaved updating fashion achieves a good trade-off but is not necessarily optimal. Exploring alternative strategies like progressive feature pyramid networks can potentially yield better multi-scale fusion.\n\n5. Integrate with larger models for more advanced tasks. Lite DETR currently focuses on object detection. Integrating it with larger Transformers for instance segmentation, panoptic segmentation, and other dense prediction tasks could further demonstrate the efficiency and effectiveness of the proposed method."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n ABSTRACT\ndeep Training very neural networks is still an extremely challenging task: The layers, which common solution is t0 use shortcut connections and normalization are both crucial ingredients in the popular ResNet architecture However; there is strong evidence to suggest that ResNets behave more like ensembles of shallower networks than truly deep ones_ Recently, it was shown that deep vanilla networks (i.e. networks without normalization layers or shortcut connections) can be trained as fast as ResNets by applying certain transformations to their activation functions However; this method (called Deep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks that overfit significantly more than ResNets on we rectify this situation by developing In this work; a new type of ImageNet: transformation that is fully compatible with a variant of ReLUs Leaky ReLUs_ experiments that Our method, which introduces negligible extra com We show in putational cost, achieves validation accuracies with deep vanilla networks that are competitive with ResNets (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the validation accuracies we obtain do not get worse with depth:\nINTRODUCTION\nalgorithmic innova Thanks to many architectural and tions, the recent decade has witnessed the unprecedented deep learning in various high-profile chal success of lenges, e-g,, the ImageNet recognition task Krizhevsky et al,, 2012), the challenging board game of Go (Sil ver et al,, 2017) and human-like text generation Brown Among them, shortcut connections (He et al., 2020). et al,, 2016a; Srivastava et al,, 2015) and normalization layers (loffe & Szegedy, 2015; Ba et al, 2016) are two architectural components of modern networks that are critically important for achieving fast training at very high depths, and feature prominently in the ubiquitous ResNet architecture of He et al. (2016b).\nTop- ImageNet validation accuracy Figure I: using deep of vanilla networks initialized either EOC (with ReLU) or TAT (with LReLU) and trained with K-FAC.\nthey Despite the success of ResNets, there is significant evidence to suggest that the primary reason work so well is that they resemble ensembles of shallower networks during training Veit et al,, 2016) deep which lets them avoid the common pathologies associated with very networks (e.g. Hochreiter et al,, 2001; Duvenaud et al,, 2014). Moreover; ResNets without normalization layers could lose expressivity as the depth goes t0 infinity (Hayou et al, 2021). In this sense, the question of whether effectively trained on challenging tasks remains an open one_ truly deep networks can be efficient and\nDing As argued by Oyedotun et al. (2020) and et al: (2021), the multi-branch topology of ResNets For example, it is memory-inefficient at inference time, as the input to also has certain drawbacks In particular; the shortcut every residual block has to be kept in memory until the final addition: branches in ResNet-50 account for about 40% of the memory usage by feature maps. Also, the classical interpretation of why deep networks perform well because of the hierarchical feature they representations produce does not strictly apply to ResNets, due to their aforementioned tendency to behave like ensembles of shallower networks. Beyond the drawbacks of ResNets, training vanilla deep neural networks which we define as networks without shortcut connections 0r\nnormalization layers) is an interesting research problem in its own right, and finding a solution could discovering new model architectures. However; recent progress in this direction has open the to path fully succeeded in matching the generalization performance of ResNets. not\nmean-field analysis of deep MLPs to choose variances for the initial Schoenholz et al. (2017) used weights and bias parameters, and showed that the resulting method called Edge of Chaos (EOC) Building on EOC, allowed vanilla networks t0 be trained at very high depths on small datasets_ and incorporating dynamical isometry theory, Xiao et al. (2018) was able to train vanilla networks depths of up to 10,000. with Tanh units While impressive, these EOC-initialized networks at significantly slower than standard ResNets of the same depth, and also exhibited significantly trained generalization performance_ Qi et al. (2020) proposed to enforce the convolution kernels to worse significant On ImageNet: be near isometric, but the gaps with ResNets are still While Oyedotun et al. (2020) was able to narrow the generalization gap between vanilla networks and ResNets, their experiments were limited to networks with only 30 layers, and their networks required many times method called Deep Kernel More recently, Martens et al. (2021) introduced more parameters. Shaping (DKS) for initializing and transforming networks based on an analysis of their initialization They time kernel properties. showed that their approach enabled vanilla networks to train faster than previous methods, even matching the of similarly sized ResNets when combined with speed stronger optimizers like K-FAC (Martens & Grosse, 2015) or Shampoo (Anil et al,, 2020). However; training their method isn 't fully compatible with ReLUs, and in their experiments (which focused on speed) their networks exhibited significantly more overfitting than ResNets\nusing Inspired by both DKS and the line of work mean-field theory, a new method we propose TAT inherits the main advantages of DKS, while called Tailored Activation Transformation (TAT) working particularly well with the \"Leaky ReLU\" activation function. TAT enables very deep vanilla neural networks to be trained on ImageNet without the use of any additional architectural elements, while only introducing negligible extra computational cost: Using TAT; we demonstrate for the first 50-layer vanilla deep network can nearly match the validation accuracy of its ResNet time that And unlike with the EOC method, validation accuracy we counterpart when trained on ImageNet: achieve does not decrease with depth (see Figure 1). Furthermore, TAT can also be applied to ResNets without normalization layers, allowing them to match O even exceed the validation accuracy of multi-framework open source implementation of DKS standard ResNets of the same widthldepth: Igithub _ and TAT is available at https deepmind dks_ com\nBACKGROUND 2\nOur main tool of analysis will be kernel functions for neural networks (Neal, 1996; Cho & Saul 2009; Daniely et al,, 2016) and the related QC maps Saxe et al,, 2013; Poole et al,,2016; Martens 2021). In this section, key introduce our notation and some concepts used throughout: et al: we\n2.1 KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS\nDynamical isometry is unavailable for ReLU (Pennington et al,, 2017), even with orthogonal weights_\n2.2 LOcAL Q/C MAPS\nBy equation 3, any diagonal entry 9+1 of 2l+1 only depends on the corresponding diagonal entry qi following recursion for these diagonal entries, which we call q values: of 2l. Hence, we obtain the\nWe note that q; is an approximation of rf (Ti, Ti). Analogously, one where Q is the local Q map can write the recursion for the normalized off-diagonal entries, which we call \u20ac values, as:\napproximation of the cosine where \u20ac is the local C map and Ti x2 /do. We note that is an similarity between f}(T1 (12 Because \u20ac is a three dimensional function, it is difficult to and f& analyze, as the associated q values can vary wildly for distinct inputs. However; by scaling the inputs to have norm Vd, and rescaling =1for all /. This allows uS 1,it follows that q; so that Q(1) satisfying C(1) Additionally, it to treat C as a one dimensional function from [~1,1] to [_1,1] =1. special structure as positive definite function (see Appendix A.4 for can be shown that \u20ac possesses details). Going forward, we will thus assume that 1, and that is scaled so that Q(1) = 1. =\n2.3 ExTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES\nAs argued in Martens et al. (2021), QC maps can also be defined for convolutional networks if one adopts a Delta initialization (Balduzzi et al,, 2017; Xiao et al,, 2018), in which all weights except those in the center of the filter are initialized to zero. Intuitively, this makes convolutional networks behave like a collection of fully-connected networks operating independently over feature map locations. As such, the QC map computations for a feed-forward convolutional network are the same as above_ Martens et al. (2021) also gives formulas to compute q and \u20ac values for weighted sum operations between the outputs of multiple layers (without nonlinearities), thus allowing more complex network In particular; the sum operation'$ output q value is given by q Ci_1 W?qi, and its topologies. = output \u20ac value is given by 4 Ci_1 w?giCi In order to maintain the property that all q values are 1 in operations are normalized in the sense that Zi_1 wz the network, we will assume that sum =1.\nFollowing Martens et al. (2021), we will extend the definition of Q/C maps to include global QC Global maps, denoted by Qf and C f for maps, which describe the behavior of entire networks given network f , can be computed by applying the above rules for each layer in f . For example, the global C map of a three-layer network f is simply C f (c) C(c): Like the local C map, global = CoC 0 maps are positive definite functions (see Appendix A4) In this work; we restrict our attention to the family of networks comprising of combined layers; and normalized sums between the output of multiple affine layers, for which we can compute global QC maps. And all of our formal results will implicitly assume this family of networks.\nQIC MAPS FOR RESCALED RESNETS 2.4\nsequence of residual blocks, each of which computes the sum of a residual ResNets consist of which consists of a small multi-layer convolutional network) and a shortcut branch branch which copies the block's input) In order to analyze ResNets we will consider the modified version used which removes the normalization layers found in in Shao et al. (2020) and Martens et al. (20213 the residual branches, and replaces the sum at the end of each block with normalized sum_ These following recursion: networks, which we will call rescaled ResNets; are defined by the\nwhere R is the residual branch; and w is the shortcut weight which must be in [~1,1]). Applying the previously discussed rules for computing QC maps, we get qi = 1for all / and\nExISTING SOLUTIONS AND THEIR LIMITATIONS 3\nGlobal QC maps can be intuitively understood as a way of characterizing signal propagation through magnitude of the activation the network f at initialization time. The q value approximates the squared\nQ; describe the contraction Or expansion of this magnitude through the action of f_ vector; So that approximates the cosine similarity of the function values for different On the other hand, the C value inputs, So that C f describes how well f preserves this cosine similarity from its input to its output:\nStandard initializations methods (LeCun et al,, 1998; Glorot & Bengio, 2010; He et al,, 2015) are motivated through an analysis of how the variance of the activations evolves throughout the network primitive form from This can be viewed as a of Q map analysis, and that perspective, these methods trying to ensure that 4 values remain stable throughout the network by controlling the local Q map are This is necessary for trainability, since very tiny large q values can cause numerical issues, saturated or activation functions (which have implications for C maps), and problems with scale-sensitive losses_ However; as was first observed by Schoenholz et al. (2017), a well-behaved \u20ac map is also necessary for trainability. When the global C map is close to a constant function (i.e. degenerate) on (_1,1), which easily happens in deep networks (as discussed in Appendix A.2), this means that the network '$ looking, and won't convey any useful information about output will appear either constant o random the input: Xiao et al. (2020) and Martens et al. (2021, give more formal arguments for why this leads optimization and/or pOOr generalization under gradient descent: to slow\nTo address these problems, Martens et al. (2021, introduced DKS, which enforces the conditions directly on the network's global \u20ac map (for some modest constant Cf(0) 0 and Cf (1) > 1) They_ show that these conditions, along with the positive definiteness of C maps, cause C f to be Cf - In addition to these C map conditions, DKS enforces close to the identity and thus well-behaved that Q(1) = 1 and Q (1) = 1, which lead to constant q values of in the network, and lower kernel approximation error (respectively). DKS enforces these QC map conditions by applying a model class-preserving transformation (x) = x(o(ax + 8) + 6). with non-trainable parameters &, B, The hyperparameter G is chosen to be sufficiently greater than 1 (e.g: and 0 1.5) in order t0 prevent the transformed activation functions from looking they nearly linear would be exactly linear if (as 1), which Martens et al. (2021) argue makes it hard for the network to achieve nonlinear behavior = Iduring Using training; training speed of ResNets on ImageNet with they DKS, were able to match the using K-FAC. However; DKS is not fully compatible with ReLUs, and the networks vanilla networks in their experiments fell substantially short of ResNets in terms of generalization performance.\nTAILORED ACTIVATION TRANSFORMATION (TAT) 4\nThe reason why DKS is not fully compatible with ReLUs is that positive homogeneous they are i.e. ad(x for & > 0. This makes the parameter of the transformed activation function (wx) redundant; thus reducing the degrees of freedom with which to enforce DKS'$ four QIC map attempt to circumvent this issue by dropping the condition Q (1) Martens et al. (20217 conditions_ using which leads to vanilla deep networks that are trainable, but slower to optimize compared to 1 significant drawback for DKS, as the best generalizing DKS with other activation functions This is a deep models often use ReLU-family activations We therefore set out to investigate other possible either in the form of different activation functions, new QC map conditions, or both: remedies To this end, we adopt a ReLU-family activation function with an extra degree of freedom (known as \"~Leaky ReLU\" ), and modify the QIC map conditions in order to preserve certain desirable properties\nTable I: Comparison of different methods applied to & network f_\nThe resulting method, which TAT) of this choice. we name Tailored Activation Transformation ( competitive generalization performance with ResNets in Our experiments. achieves\nTAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUs 4.1\nOne way of addressing the issue of DKS's partial incompatibility with ReLUs is to consider a slightly different activation function namely the Leaky ReLU (LReLU) Maas et al,, 2013):\nis actually stronger than DKS s Q map conditions (Q(1) = Note that the condition Q(q) = 1 and 1), and has the potential to reduce kernel approximation errors in finite width networks Q (1) compared to DKS, as it provides a better guarantee on the stability of Qf WIt random perturbations of the q values at each layer: Additionally, because the form of \u20ac does not depend 0n either of the layer's input q values, it won't be affected by such perturbations at all. (Notably, if one uses the negative slope parameter to transform LReLUs with DKS, these properties will not be achieved ) In support of these intuitions is the fact that better bounds on the kernel approximation error exist for ReLU networks than for general smooth ones as discussed in Appendix A.l).\nAnother consequence of using for our activation function is that we have \u20ac' (1) = 1 as in EOC. which is used to achieve Cf (0) If combined with the condition 0 in DKS) this would 0 identity function, which by equation imply by Theorem is only true when & 1 that \u20ac is the resulting in a linear network. In order to avoid this situation, and the closely related one where thus appears \"nearly linear\" we instead choose the value of & so that C ; (0) hyperparameter 1, for a following theorem; n controls how close Cf is to the identity, thus n < 13 As shown in the 0 allowing us to achieve a well-behaved global C map without making nearly linear:\nTheorem 1. For a network f with as its activation function with & 0), we have\nusing following proposition: as an activation function is given by the Another motivation for\nusing behaves similarly to ResNet; This result implies that at initialization, a vanilla network desirable given the success that ResNets have already demonstrated: property that is quite\nIn summary; we have the following three conditions:\nwe achieve by picking the negative slope parameter a SO that C;(0) = %. which We define the Tailored Rectifier (TReLU) to be with & chosen in this way. Note that the first two conditions are\nalso true when applying the EOC method to LReLUs, and its only the third which sets TReLU apart While this might seem like a minor difference, it actually matters a lot to the behavior of the global \u20ac Figure 2 where the \u20ac value quickly converges towards 1 with depth under map. This can be seen in EOC, resulting in a degenerate global \u20acC map. By contrast; the global C map of TReLU for a fixed a nice function, suggesting a very deep vanilla network with TReLU has the n converges rapidly to We prove this in Proposition 3 by showing same well-behaved global C map as a shallow network: the local \u20ac map in equation depth. For direct comparison converges to an ODE as we increase the of all QC map conditions, we refer the readers to Table 1.\nFor the hyperparameter 0 < 1 < 1, we note that a value very close to 0 will produce a network that while a value very close to 1 will give rise to a degenerate \u20acC map. In practice we is nearly linear settings. Once we decide On 7, we can solve 0.9 or 0.95, which seems to work well in most use n the value & using binary search by exploiting the closed-form form of \u20ac in equation 9 to efficiently layer compute Cf (0). For instance, if f is a 100 vanilla network, one can compute C f (0) as follows:\nwhich is a function of &. This approach can be generalized to more advanced architectures, such as rescaled ResNets, as discussed in Appendix B.\nTAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS 4.2\nUnlike LReLU, most activation functions don 't have closed-form formulas for their local \u20ac maps. As result; the computation of C ; (0) involves the numerical approximation of many two-dimensional integrals to high precision (as in equation 5), which can be expensive. One alternative way to quite control how close Cf is to the identity, while maintaining the condition C;(1) =1,is t0 modulate its second derivative C\" (1). The validity of this approach is established by the following theorem:\nTheorem 2.  Suppose f is a network with a smooth activation function If Cf (1) = 1, then we have\nstraightforward computation shows that C} (1) = LC\"(1) if f is an Given C(1) = 1 and C' (1) = 1, L-layer vanilla network. (See Appendix B for a discussion of how to do this computation for more From this we obtain the following four local QIC conditions: map general architectures:\nWe do not have a closed-form solution of this system. However; each expectation is a one dimensional integral, and so can be quickly evaluated to high precision using Gaussian quadrature. One can then use black-box nonlinear equation solvers, such as modified Powell's method (Powell, 1964), to obtain 6github _ dks for a complete implementation: solution: See https deepmind com_\n5 ExPERIMENTS\nOur main experimental evaluation of TAT and competing approaches is on training deep convolutional of these experiments is not to networks for ImageNet classification (Deng et al, 2009). The goal achieve state-of-the-art; but rather to compare TAT as fairly as possible with existing methods, and standard ResNets in particular: To this end, we use ResNet V2 (He et al,, 2016b) as the main reference\narchitecture, from which we obtain rescaled ResNets (by removing normalization layers and weighing the branches as per equation 6), and vanilla networks (by further removing shortcuts) For networks regularization, as was done without batch normalization; we add dropout to the penultimate layer for in Brock et al. (2021b). We train the models with 90 epochs and batch size of 1024, unless stated we obtain n by grid search in {0.9,0.95} The weight initialization used otherwise. For TReLU for all methods is the Orthogonal Delta initialization, with an extra multiplier given by Ow We initialize biases iid from N (0,o8). (1,0) in all experiments (unless explicitly (Ow, We use 0b) = (V2,0) in standard ResNets, as stated otherwise) , with the single exception that we use (ow; 0b = per standard practice He et al,, 2015). For all other details see Appendix D.\n5.1 TowARDS REMOVING BATCH NORMALIZATION\ntraining of very deep neural networks Two crucial components for the successful are shortcut BN) layers. As argued in De & Smith (2020) and Shao et al. connections and batch normalization (2020), BN implicitly biases the residual blocks toward the identity function, which makes the This suggests that one can network better behaved at initialization time, and thus easier t0 train. compensate for the removal of BN layers at least in terms of their effect on the behaviour of the network at initialization time, by down-scaling the residual branch of each residual block. Arguably; training deep networks without normalization layers (Zhang et al,, 2018; almost all recent work on Shao et al, 2020; Bachlechner et al, 2020; Brock et al, 2021a;b) has adopted this idea by introducing which may o may not be optimized during training). multipliers on the residual branches\nTable 2:  Top-1 validation accuracy of rescaled ResNet50 with 0.9 for TReLU_ varying shortcut weights_ We set n\n5.2 THE DIFFICULTY OF REMOVING SHORTCUT CONNECTIONS\nTable 3: ImageNet top-1 validation accuracies of shortcut-free networks on ImageNet_\nWhile the aforementioned works have shown that it anie >: Ilagenet [Op- 1 Vanudlon accurdCcs shortcut-free networks on ImageNet: is possible to achieve competitive results without nor- Optimizers malization layers, they all rely on the use of shortcut Depth vanilla BN LN connections to make the network look more linear at K-FAC 72.6 72.8 72 50 A natural question to ask is whether initialization: SGD 63.7 72.6 58. normalization layers could compensate for the re- K-FAC 71.8 67.6 72.0 moval of shortcut connections We address this ques - 101 SGD 41.6 43.4 28.6 tion by training shortcut-free networks with either BN or Layer Normalization (LN) layers. As shown in Table 3, these changes do not seem to make significant difference, especially with strong optimizers like K-FAC; These findings are in agreeme with the analyses of Yang et al. 2019) and Martens et al: (2021), who respectively showed that dee shortcut-free networks with BN layers still suffer from exploding gradients, and deep shortcut-fre networks with LN layers still have degenerate C maps.\nNETWORKS WITHOUT SHORTCUTS 5.3 TRAINING DEEP NEURAL\nvanilla networks achieve generalization deep The main motivation for developing TAT is to help performance similar to standard ResNets_ In our investigations we include rescaled ResNets with weight of either 0 (i.e. vanilla networks, a shortcut or 0.8_ In Table 4 we can see that with strong optimizer like K-FAC, we can reduce the gap on the 50 layer network to only 1.8% accuracy when training for 90 epochs, and further down to 0.69 when training for 180 epochs. For 101 layers, the gaps are 3.6% and 1.79 respectively, which we show can be further reduced with wider networks knowledge, this is the first time that a deep vanilla network has been trained see Table 9). To our high validation accuracy on ImageNet: In addition, our networks have fewer parameters to such and run faster than standard ResNets, and use less memory at inference time due to the removal of\n0.8), we do not include Table 4: ImageNet top-[ validation accuracy. For rescaled ResNets (w 0.0 or w any normalization layer: For standard ResNets, batch normalization is included. By default; ReLU activation is used for standard ResNet while we use TReLU for rescaled networks\nThe gaps when using SGD as the optimizer are noticeably larger; shortcut connections and BN layers which we further explore in Section 5.5. Lastly, using rescaled ResNets with a shortcut weight of 0.8 we can exactly match Or even surpass the performance of standard ResNets. and TReLU\n5.4 COMPARISONS WITH EXISTING APPROACHES\nComparison with EOC. Our first comparison Ta is between TAT and EOC on vanilla deep net- SOI For EOC with ReLUs we set works_ Ow; Ob_ (V2,0) to achieve Q(1) as in He et al. 1 = (2015), since ReLU networks always satisfy C' (1) For Tanh acti 0_ whenever 0b comprehensive comparison with EOC vations, a more difficult, infinitely many is as there are choices of that achieve C' 13 Ow; 0b_ (1.302, 0.02) Here we use Ow; 0b_ as sug- = gested in Hayou et al. (2019). In Table 5, we can settings, networks constructed see that in all the outperform EOC-initialized networks by a with TAT Another observation is that the accuracy of EOC-ini\nTable 5: ImageNet top-1 validation accuracy compari- son between EOC and TAT on deep vanilla networks_\nComparison with DKS. The closest approach to TAT in the existing literature is DKS, whose compare TAT to DKS on both LReLUs? similarity and drawbacks are discussed in Section 4. We and smooth functions like the SoftPlus and Tanh: For smooth activations, we perform grid search {0.2, 0.3, 0.5} for TAT, and {1.5,10.0, 100.0} for in in DKS, and report only the best over performing one. we observe that TAT, together with LReLU From the results shown in Table 7 (i.e. TReLU) performs the best in nearly all settings we tested, and that its advantage becomes larger dropout: One possible reason for the superior performance of TReLU networks is when we remove they. the stronger QIC map conditions that satisfy compared t0 other activations (i.e. Q for all 0(1) 1 and Q (1) 1, and invariance of \u20ac to the input q value), and the extra resilience to q VS = = kernel approximation error that these stronger conditions imply. In practice, We found that TReLU indeed has smaller kernel approximation error (compared to DKS with smooth activation functions, see Appendix E.l) and works equally well with Gaussian initialization (see Appendix E.7)\nTable 6: Comparison with PReLU.\ncomparISOI WIU rreLu. Tne Farametrc \"'Table 6: Comparison with PReLU: ReLU (PReLU) introduced in He et al. (2015) Depth Optimizer TReLU PReLUo.0 PReLUo.25 differs from LReLU by making the negative K-FAC 74.6 72.5 73.6 slope a trainable parameter: Note that this is 50 doing distinct from what we are with TReLU SGD 71.0 66.7 67.9 since there we compute the negative slope pa- 72.8 K-FAC 74.2 71.9 101 during training: rameter ahead of time and fix it SGD 70.0 54.3 66.3 comparisons with PReLU we consider In our two different initializations: 0 (which recovers the standard ReLU), and 0.25, which was used in We report the results on deep vanilla networks in Table 6 (see Appendix E.6 for He et al: (2015). For all settings, our method outperforms PReLU by a large margin results on rescaled ResNets) emphasizing the importance of the initial negative slope value. In principle, these two methods can be combined together (i.e. we could first initialize the negative slope- parameter with TAT; and then during optimize it training) , however we did not see any benefit from doing this in Our experiments\nWe experiments with (1.0,0.0) , and the scheme described in Pennington et al. (2017) also ran O w; 0b = and Xiao et al. (2018) for dynamical isometry: The results were worse than those reported in the table_\nFor DKS, we set the negative slope as a parameter and adopt the transformation y(da(x + 8) + 6).\nare results without dropout. Table 7: Comparisons between TAT and DKS_ The numbers on the right hand of The methods with are introduced in this paper:\nTable &: Batch size scaling:\n5.5 THE ROLE OF THE OPTIMIZER\nOne interesting phenomenon Batch size we observed Optimizer in our experiments, which echoes the find- 128 256 2048 4096 512 1024 (2021), is that a strong ings of Martens et al. K-FAC 74.5 74.4 745 74.6 74.2 72.0 optimizer such as K-FAC significantly out- SGD 72.7 72.6 72.7 71.0 69.3 62.0 performs SGD on vanilla deep networks in 70.2 LARS 72.4 72.3 72.6 71.8 71.3 training One plausible expla- terms of speed. nation is that K-FAC works better than SGD in the large-batch setting, and our default batch size of at which scaling efficiency begins to drop. Indeed 1024 is already beyond SGD's \"critical batch size\" it was shown by Zhang et al. (2019) that optimization algorithms that employ preconditioning, such as Adam and K-FAC, result in much larger critical batch sizes_\nTo investigate this further; we tried batch sizes between 128 and 4096 for training 50-layer vanilla TReLU 0.75 net - L K-FAC performs equally works_ As shown in Table 8 0.70 : well for all different batch sizes except 4096 (where we 9 KFAC with 90 epochs overfitting), while the performance of SGD see increased SGD with 90 epochs KFAC with 180 epochs starts to drop when we increase the batch size past 512. SGD with 180 epochs KFAC with 360 epochs Surprisingly, we observe a similar trend for the LARS SGD with 360 epochs optimizer You et al,, 2019), which was designed for large- Cases processed le8 batch training: Even at the smallest batch size we tested Figure 3: Training speed comparison be (128), K-FAC still outperforms SGD by gap of 1.8% tween K-FAC and SGD on 50 layer vanilla within our standard epoch budget  We conjecture the rea- TReLU network son behind this to be that vanilla networks without normalization and shortcuts give rise to loss landscapes with worse curvature properties compared to ResNets, and that this slows down simpler optimizers like SGD To investigate further; we also ran SGD (with a batch size of 512) and K-FAC for up to 360 epochs with ~one-cycle\" cosine learning rate schedule (Loshchilov & Hutter; 2016) learning rate to to 0 by the final epoch: As shown in Figure 3, SGD does indeed that decreases the eventually catch up with K-FAC (using cosine scheme), requiring just over double the number of epochs to achieve the same validation accuracy: While one may argue that K-FAC introduces additional computational overhead at each step, thus making head-to-head comparison versus SGD unfair; we note that this overhead can amortized by not updating K-FAC $ preconditioner matrix at every step. In OUT experiments we found that this strategy allowed K-FAC to achieve a similar per-step optimization advantage on vanilla networks_ runtime to SGD, while retaining its (See Appendix E.3.)\nCONCLUSIONS 6\nIn this work we considered the problem of training and generalization in vanilla deep neural networks (i.e. those without shortcut connections and normalization layers). To address this we developed a novel method that modifies the activation functions in a way tailored to the specific architecture, and which enables us to achieve generalization performance on par with standard ResNets of the same width/depth: Unlike the most closely related approach (DKS), our method is fully compatible with obviating ReLU-family activation functions, and in fact achieves its best performance with them. By deep' the need for shortcut connections, we believe our method could enable further research into models and their representations_ In addition; our method may enable new architectures to be trained for which existing techniques, such as shortcuts and normalization layers, are insufficient:\nREPRODUCIBILITY STATEMENT\nHere we discuss our efforts to facilitate the reproducibility of this paper: Firstly, we have made an open Python implementation of DKS and TAT; supporting multiple tensor programming frameworks, https : / /github _ deepmind/ dks. Secondly, we have given all important available at com details of our experiments in Appendix D.\nREFERENCES \n Now the paper ends. \nWhat are potential directions for future work based on the insights and findings of this research?", "output": "Some possible directions for future work are:\n\n1. Applying TAT to more complex architectures beyond convolutional networks. The paper showed that TAT can enable training of vanilla (shortcut-free) convolutional networks, but TAT may also be applicable to other architectures liketransformers  and GANs. Exploring these applications of TAT could discover new insights.\n\n2. Investigating the use of TAT as a tool for network interpretability. Since TAT allows training of networks without shortcuts and normalization layers, the hierarchical feature representations learned by such networks may be more interpretable. Studying this could provide a new perspective on network interpretability.  \n\n3. Combining TAT with trainable activation functions like PReLU. Although the paper found that initializing PReLU with TAT works better than random initialization, jointly optimizing the activation slopes during training may provide further benefits. Exploring this combination could yield even better performing networks.\n\n4. Exploring how TAT enables training of deep networks with alternative loss functions. The paper focused on classification tasks with cross-entropy loss, but TAT may enable training networks optimized for other losses like contrastive or ranking losses. This could uncover new capabilities of TAT.\n\n5. Developing fast approximations to TAT for larger networks. The current approach requires solving non-convex optimization problems for each network, which may become intractable for very deep networks. Engineering fast but approximate solutions to TAT could enable its application at larger scales."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nWith the ubiquitous use of online meeting plat- recognition forms and robust automatic speech systems, meeting transcripts have emerged as promising domain for natural language tasks Most recent works on meeting transcripts pri- marily focus on summarization and extraction of action items. However; meeting discussions useful question-answering (QA) also have understanding the dis component; crucial to course or meeting content, and can be used to long build interactive interfaces on top of tran Hence, in this work, we leverage this scripts_ inherent QA component of meeting discussions and introduce MEETINGQA, an extractive QA comprising of questions asked by meet- dataset ing participants and corresponding responses questions can be open-ended and As a result; actively seek discussions, while the answers multi-span and distributed across mul can be Our comprehensive empiri- tiple  speakers_ cal study of several robust baselines includ ing long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (Fl 57.3) and = behind human performance (Fl lag severely challenging new 84.6), thus presenting task for the community to improve upon_\nmeeting tran- from Figure 1: Representative example script segment in MEETINGQA. The question and anno- highlighted in red and blue respectively: tated answer are\ntranscripts have been released (Carletta et al,, 2005; Janin et al,,2003; Garofolo et al,,2004,inter alia).\nPrior NLP work on meeting transcripts mainly focuses 0 summarization (Oya et al,, 2014; Li et al,,2019; Zhu et al,,2020, inter alia). However; lack of annotated data impedes research on other important NLP tasks in this domain: To address this gap, we introduce a question-answering (QA) task based on conversations in meeting transcripts. Specifically, we consider questions asked by par ticipants during the meeting and aim to extract COI- responding answer spans from relevant discussions among meeting participants (refer to Figure 1). This task has several practical applications such as building an interactive meeting browserlinterface for navigating through transcripts and informing meeting summarization and handling tasks such as action items involving QA (Kathol and Tur pairs 2008; August et al,, 2022).\nIntroduction\nday Millions of meetings occur every worldwide amounts of meeting tran- which results in vast scripts. Meeting transcripts are typically long doc- often domain-specific depending o the uments subject matter; and contain a lot of information. catching up with a missed meet- Basic tasks such as ing, looking up a specific discussion O response to a query can be time-consuming: These tasks can be facilitated by NLP systems, including summariza- tion and question-answering: To this end, several publicly available small-scale corpora of meeting\nMEETINGQA data and code is available at https : archiki.github meetingqa html i0\nWhile standard QA datasets consist of human generated questions either based 0n short supplied contexts (Rajpurkar et al,, 2016,2018; Rogers et al. large 2021) or are ans collection of using a wered documents (Joshi et al,, 2017; Kwiatkowski et al. 2019; Zhu et al,, 2021b), our task setting is chal- lenging yet interesting in several ways. First, meet- long ing transcripts are documents and QA systems still struggle to understand long contexts Pang et al,, 2022; Soleimani et al,, 2021). Second, suc- cessfully answering questions asked within meet- ings requires robust understanding of the conver- sation and discourse that takes place both before and after a question. Third, the multi-party spoken text falls under a different domain when compared long to typical text documents. While standard documents rarely include any meaningful (non- rhetorical) questions, multi-party meetings often involve discussions asked by one participant and answered by the rest, allowing uS to use these ques QA dataset: Furthermore, the tions to create conversational nature of transcribed text differs from written documents and contain disflu- may encies and other artifacts. Finally, instead of using generated questions (like in Wu et al annotator- (2022)), questions asked by participants are more open-ended and discussion-seeking, with interest- ing answer types that can be multi-span andlor con- tributed by multiple speakers (e.g , Figure 1).\nmodels. Furthermore, we also investigate how well large language models fare recent instruction-tuned wering questions from MEETINGQA. Lastly, at ans silver-annotation pipeline using ME- we create containing Zhu et al,, 2021a), a corpus DIASUM transcripts, to provide ad 463.6K short interview ditional training data. We find that the best per- formance is achieved by finetuned short-context models (Fl = 57.3) . Overall, we show that models struggle to identify rhetorical questions and select- ing which utterances constitute the answer: Thus_ model performance significantly trails behind hu- performance on MEETINGQA (Fl 84.6), man leaving large potential for future improvements on this challenging task:\nOur Dataset: MEETINGQA 2\nWe first describe our data collection process in Section 2.1 and then provide an extensive analysis of MEETINGQA in Section 2.2.\n2.1 Data Collection\nWe leverage the punctuated Question Selection: identify possible questions (ending with 2 ) text to We also filter out questions containing 2 words manually find them to be either meaningless as we rhetorical. While questions are marked to facili- or tate annotators, we encourage them to find missed potential questions due to incorrect punctuation.\nTo this end, we first introduce our dataset MEET- INGQA, created by annotating meetings transcripts popular AMI (Augmented Multi-party from the Interaction) cOrpus, containing over 100 hours of meetings (Carletta et al,, 2005), via a robust anno- MEETINGQA comprises of 7,735 tation pipeline questions asked by participants across 166 meet- Unlike other datasets, questions in MEET ings. INGQA are less concise (12 words on average) and reflect queries asked in a conversational setting: The answers include realistic situations such as rhetorical questions, multiple discontiguous spans and/or contributions from multiple speakers.\nFor each possible question, Answer Annotation: we ask annotators to label the set of sentences (each identified by a unique number) from the meeting transcript that form the answer Additionally, we also collect meta-data about the question. First, we ask the annotators to label if the question was mean- ingful, used to filter out rhetorical, unanswered logistical questions and incorrect punctuations or Some speakers can ask consecutive O multiple questions in the same turn that are often related and answered together:  In such scenarios, al- we low annotators to combine questions and provide a common answer from the meeting transcript The using mark these questions the com- annotators bined question attribute. Finally, since our ques - they tions are conversation segments, may not be self-contained. Hence, we ask annotators to men- tion the question context sentences (if any) sepa- rately.  We refer readers to Appendix A for more details and examples from MEETINGQA.\nNext, on MEETINGQA dataset; we test diverse models designed for long input contexts such as Longformer (Beltagy et al,, 2020) , and BigBird (Za- heer et al., 2020) as well as RoBERTa (Liu et al. 2019), and DeBERTa-v3 (He et al,,2020) with as meeting context surrounding the question as much possible: To incorporate the multi-span nature of design and experiment answers in our dataset; we with multi-span  variants of the aforementioned\nFigure 2: Analysis of 200 randomly selected questions from MEETINGQA. On left, we show the distribution of question types. On right; we show additional properties of answerable questions: level of agreement in multi-speaker subjectivity of questions, question-framing; follow-ups and answer ambiguity in questions_ yes/no answers,\nAnnotation Process. All annotators were hired by a professional crowdsourcing company TELUS. 2 The company obtained consent from the crowd workers and conducted ethical reviews. To provide comprehensive in- train annotators, we structions for each type of annotation with several manually annotated examples from a small subset different   possible of   transcripts and scenarios curated by the first author The annotations were collected in multiple batches, starting with the first batch containing a small subset of 250 questions. iteratively provided extensive feedback to the We crowdworkers on their annotations and resolved existing issues till the annotations were satisfactory: assigned three independent annotators Next, we and calculated Krippendorff's to each question, 0.73  (Krippendorff;, 1980) using MASH a distance (Passonneau, 2006) , indicating substantial agreement  We then collected annotations for the remaining  questions  in additional batches two lusing question followed by one annotator per quality assurance stage to validate the outcome of the annotations. Overall, we spent $10,427 in the annotation process, amounting to $61 per meeting: For additional details refer to Appendix A\nTable 1: Dataset statistics of MEETINGQA\nunanswerable, multi-span, and multi-speaker (de- scribed below). Due to relatively small number of meetings in the AMI corpus and diversity in meet- larger fraction ing content, our test set contains a of questions from the dataset as opposed to the con- ventional 80:10:10 split across train/dev/test sets_\nQuestion Types: Unlike most QA datasets, ques- tions in MEETINGQA are extracted directly from meeting transcripts. Consequently, we find that the questions may not be concise, and may not begin wh' prefixes, making our dataset challenging with We perform a yet interesting for the community analysis of question types based on 200 manual randomly selected questions from the test set in Figure 2 (left) First; we observe that a majority of questions in MEETINGQA are framed in a `'yeslno what' and *how' questions manner; followed by that are typically information-seeking: We find that discussion-heavy setting such as OurS, yeslno in a questions elicit a detailed response that cannot be reduced to a direct ` yeshno' response in over 40% of the cases (see Figure 2 (right)). Further; manual analysis shows that nearly half the questions are subjective, i.e,, seeking opinions of meeting partic- ipants, and as high as 20% of answerable questions\nDataset Information and Analysis 2.2\nfiltering and quality control, After we were left meetings with a total of 7,735 questions from 166 meeting recordings). 100 hours of\nSize and Splits: We split our dataset into train; dev, and test sets such that questions in each split come from distinct meetings. Table 1 shows dataset statistics across  different answer types, namely\n'https telusinternational com WWW\nmeeting transcript; question and answer (for answerable questions) in Figure 3: Number of words (length) in transcript; question, and answer is 5.9K, The average length of MEETINGQA 12, and 35 words respectively, while the maximum length of a transcript, question, and answer is 19.4K, 155,and 305 words respectively:\nare framed rhetorically. Appendix A contains addi- tional tri-gram-based analysis of questions.\nHuman Performance. We estimate human per- using formance on MEETINGQA random sub- sample of 250 questions from the test split: Each question is assigned a different annotator who had previously annotated the meeting containing not that question. Scoring the provided answers rela- tive to the reference answers in our dataset, yields an Fl of 84.6. This breaks down to Fl of 80.7 and 86.3 for unanswerable and answerable questions respectively: The FL score for multi-span and multi- speaker answers is 88.1 and 87.7 respectively:\nFigure 3 shows the distribution of the Length: length of meeting transcripts, questions, and an- swers in MEETINGQA. On average, each meeting transcript comprises of 5.8K words which consti- long documents unlikely to fit entirely in the tute as input context of typical pretrained language mod- els (Devlin et al.,2019; Liu et al., 2019; He et al. 2020). Further; questions and their answers contain an average of 12,and 35 words respectively:\nMethods 3\nAnswer Types: Due to the nature of meeting conversations and questions asked by participants, follow-up dis- most answers are direct responses or cussions. However; some questions are rhetorical Or do not elicit any discussion. These questions are unanswerable (30% of MEETINGQA) Among an- swerable questions, we note two scenarios of inter- multi-span and multi-speaker answers_ Multi- est: span answers contain non-consecutive and discon tinuous utterances typically in the or sentences, form of relevant discussion interleaved with irrele- vant chit-chat (see examples in Appendix A) Addi- tionally, multi-speaker answers occur when multi- ple participants contribute to answering a question which is typical in a discussion   Note that multi- speaker and multi-span answer cases are not mu- tually exclusive (refer to Figure 1 for an example). We find that 40% of all answers (excluding unan- swerable questions) in our dataset are multi-span and 48% of answers are multi-speaker in nature Moreover; Figure 2 (right) shows from our manual disagree- analysis that a considerable amount of among speakers in multi-speaker an ment exists swers, with approximately 70% of cases displaying some form of disagreement Notably, 22% of an- swers involve additional follow-up Or action items, which are specific to the context of meetings.\nIn this section, we investigate the difficulty level of our new MEETINGQA for state-of-the-art QA systems and establish strong baseline results. We describe strategies for retrieving contexts from tran- scripts in Section 3.1, followed by different QA models in Section 3.2, and silver data annotation for data augmentation methods in Section 3.3.\nRetrieving Contexts from Transcripts 3.1\nGiven that meeting transcripts are very long docu- ments, it is infeasible to input the entire transcript typical QA models. Thus, we first as context to smaller transcript segment that fits the select model's input length limitations. We explore two strategies to retrieve contexts as described below.\nLocation-based Context Retrieval: We use the relative location of the question in the meeting tran script to retrieve a context by fitting as many (com possible under a fixed length plete) sentences as budget (measured in words). Further; split this we budget into two components: prefix and suffix refer- ting  to the sentences that precede and succeed the question respectively: We set the budget to prefix 50 words and the suffix budget to 250 words respec- tively, resulting in a total budget of 300 words_\n'Ensures context fits into QA models limited 512 tokens_\nwith extracted context from Section 3.1. Addition- ally, we explore Longformer and BigBird which support longer sequences of up to 4096 tokens by utilizing a combination of sliding window and global attention mechanisms.   Further; the Long former Encoder-Decoder (LED) model supports up to 16,384 input tokens. These models allow us to portions of the transcript needed most or all use answering the questions as the context: In case for of an overflow, as many utterances from we use the transcript around the question as possible and Note that these models output truncate the rest. Therefore, for single answer-span by default: multi-span answers, train models to predict a we span starting with first utterance and ending with the last utterance of the answer: gold\nTable 2: Upper-bound performance of different retrieval methods for answerable questions split: Answer-span overlap measures the relative number of sentences in the annotated answer span present in the context %\nNote that the suffix budget is significantly larger than the prefix budget since we expect to find an swers in sentences following the question: The sentences before the question only provide addi- tional context to the ongoing discussion:\nAlternatively, Score-based Context Retrieval: question as query and compare it to we use the other sentences from the entire transcript via two scoring methods consistent with Pang et al. (2021). using ROUGE-] score First, we retrieve sentences relative to the question. Second, use cosine we similarity based on sentence embeddings (Reimers and Gurevych; 2019). We concatenate sentences in appear in the transcript until reaching they the order the total length budget: Similar to location-based retrieval, set the total budget to 300 words we\nMulti-Span Models: In order to better model multi-span answers, we follow Segal et al. (2020) and pose multi-span QA as a sequence tagging task; predicting if each token in the context is part of the answer: For Simplicity, we restrict ourselves to their proposed IO tagging: Thus, the answer prediction is a concatenation of all token-spans contiguously Similar to single-span models, we tagged with I_ multi-span variants of RoBERTa, DeBERTa, train Longformer; and BigBird models.\nFurthermore, Instruction-Tuned Models: we (Chung et al,, 2022), publicly FLAN-T5 use available instruction-tuned model, to study zero- shot performance 0n our MEETINGQA. Given the relatively large size of contexts and distinct nature we rely on succinct instructions in- of our task;, Furthermore of few-shot demonstrations. stead due to the model's generative nature, we cannot directly use the predictions for Our extractive QA task. Therefore, we adapt instruction-tuned models for our setting by employing instructions that ask models to list sentences instead of directly generat- ing answers that may be less faithful to the context: Next; we filter out predicted sentences not present in the context. While this is a strict selection crite- possible hallucinations. rion; it removes any\nResults   of Context   Retrieval: Table 2 com pares both retrieval methods using the same total length budget on the answerable questions split. We observe that the sentence-level overlap be- tween extracted contexts and annotated answers for score-based retrieval is significantly lower than We use this overlap for location-based retrieval. to compute the maximum achievable performance of QA systems for each type of retrieval. Corre - spondingly, we find similar trends in upper-bound performance metrics (discussed in Section 4) with location-based contexts (near-perfect max Fl) con- siderably outperforming score-based contexts (max 40). Therefore, for short-context models, we Fl henceforth use location-based contexts\nModels for Meeting-based QA 3.2\nSilver Data Augmentation 33\nWe primarily focus o extractive models includ- ing both short and long-context models. Given segment from it (context) and the transcript or the question, models are tasked with extracting wer-span(s) from the context: We use two high- ans performing short-context models RoBERTa and DeBERTaV3, each supporting up to 512 tokens,\nlabels, and Due to high annotation costs of gold unavailability of similar QA datasets, we investi- gate automatic methods to annotate answers_ We match the salient features of MEETINGQA, such meaningful questions within the transcript and as\nAppendix E shows these choices improve overall scores_\nTable 3: Comparing performance of finetuned single-span models and human performance on across answer types (best numbers in bold) Intermediate Train Data denotes the intermediate training data used, lack of which indicates direct finetuning: AIl scores for unanswerable questions are equal as the reference string is empty:\noverlap between a random span and the target span large span lengths. To address this, Soleimani for et al. (2021) propose reporting Intersection-over- Union (IoU) defined as:\nusing the MEDIA SUM multi-speaker discussions Zhu et al,,2021a). This dataset contains dataset 463.6K short multi-party interview transcripts, de- tailed speaker information, and identifies a host O interviewer who steers the discussion via questions.\nWe begin by identifying the host speaker and fo- cusing 0n their questions Next; we predict which speaker(s) would answer the question by identify ing speaker entities mentioned in utterances or from previous dialogue turns. Finally, we search utter- ances from the identified speakers until a stopping criterion is met and label it as the answer: Due to the assumptions made in the above process, models trained directly on this data could overfit on spuri- Liang; 2017; Wang and ous correlations (Jia and apply various perturba- Thus, Bansal, 2018). we tions to the context such as separating the question and answer utterances, converting to unanswerable questions by removing relevant sentences, creat- ing more speaker transitions, and masking speaker names. Refer to Appendix F for additional details.\npredicted and target spans, re- where p and t are the spectively: Since our answer spans are much longer than those in SQuAD (refer to Figure 3) , we also re- port macro-averaged IoU to measure performance\nperformance of Training Settings: We measure various models in both finetuned and zero-shot set- tings. First; we directly finetune the base pretrained model on the model on MEETINGQA. Next; to supplement training data we explore intermediate- (Phang et al,,2018; Pruksachatkun et al. training 2020) with SQuAD v2.0 (Rajpurkar et al,, 2018)6 combination including silver data from Sec- or tion 3.3 prior to finetuning o MEETINGQA, in- creasing the training data by Sx and IOx respec- tively. Additional details on checkpoints, hyperpa rameters, and training are present in Appendix B.\nExperiments and Results 4\nFollowing Rajpurkar et al. Evaluation Metrics: (2016) report macro-averaged Fl on the entire we specific answer types (Sec- well test set as as on tion 2.2).5 However; FL treats sequences as bag non-significant of-words, and thus, there can be a\nWe devise a straightfor- Turn-based Baseline: ward algorithm called turn-based baseline that is inspired by the automatic silver data annotation\n6SQuADv2.0 is used for all models except BigBird, for existing model which we use TriviQA due to lack of reliable checkpoint on HuggingFace Wolf et al.,2019).\n5We also report exact match (EM) scores in Appendix C_\nComparing performance of finetuned multi-span models across evaluation metrics and answer types Table 4:\nalgorithm explained in Section 3.3. In the turn - speaker asks based baseline, when question; the predicted answer includes all the subsequent speakers until the same speaker utterances of other Note that, (stopping criterion). another turn gets turn-based baseline assumes all questions can be and always provides single-span answers, wered ans although the predictions may be multi-speaker:\npoints across all metrics). Furthermore, all fine- tuned models outperform the turn-based baseline (with the exception of LED-base), whereas the cor- responding zero-shot variants fail to outperform the turn-based baseline on overall metrics. This sug- challenging for current QA gests that our dataset is systems, leaving significant scope for improvement via interesting future work:\nImpact of Long-Context Models: We observe majority cases short-context models (espe- that in a cially RoBERTa) outperforms long-context models (Longformer and BigBird) by 1-2 points. Further- more, the LED model that completely fits 90% of transcripts has significantly lower overall score difference) due to poor performance (~30 FL point questions. on  answerable We believe that the ability to fit larger contexts is traded-off by well- optimized design of short-context models. This is consistent with the findings of Pang et al. (2022) and suggests better long-context models may be needed to outperform shorter extractive models.\nResults and Discussion 4.1\nWe report performance of various fine-tuned single- multi-span models in Tables 3, and 4 respec- span, tively on the test split of MEETINGQA. Further; performance in Table 5. we evaluate zero-shot We findings below and refer readers to summarize our Appendix C for additional results.\nMain Baselines and Comparison with Human Performance: Results   from Tables and 3 4 show that single-span models (narrowly) outper multi-span models, with the best overall form the performance achieved by single-span variant of DeBERTa-base (overall Fl 57.3). Other single- = span variants of Longformer and BigBird achieve higher performance on answerable questions (up 64.4) but have lesser overall performance to Fl questions. due to lower FI scores on unanswerable Comparing to the human performance (overall FL 84.6), we find at least a 25 point difference in = overall F] of all finetuned models. Across various types, the difference in Fl scores is still at answer least 20 points. Similar trends holds for EM and setting (refer to IoU metrics too.8 In the zero-shot Table 5), the difference in overall scores with re- spect to human performance is even greater 44\nImpact of Multi-Span Models: Table 5 shows that in the zero-shot setting, multi-span variants slightly outperform their single-span counterparts for long-context models and slightly underperform for DeBERTa. In Appendix C, we find that within performance drops for unan answer types zero-shot swerable questions while improving for multi-span and multi-speaker answers. For finetuned models (Tables 3 and 4), the overall performance of multi- span models is comparable if not slightly less than single-span variants.10 Notably, for short-context\n9Due to this, we do not experiment further with LED mod- multi-span, intermediate-training, and zero-shot settings. els in\n7Model predictions may be biased against (r towards) empty spans impacting score of unanswerable questions\nIONote that we do not intermediate train multi-span models on standard extractive QA tasks such SQuAD v2.0. Therefore, training data for multi-span models is always scarce. gold\n'Following the order EM IoU Fl for all models_\nTable 5: Comparing performance of zero-shot models on all questions. Single-span and multi-span models are denoted by SS and MS respectively. Identifying answer- able questions using FLAN-TS is denoted by self ans ext ans' denotes use of external supervised model and\nconfigu- Error analysis of different model Figure 4: questions. Top: rations on answerable percentage of errors where the model incorrectly predicts a question is unanswerable. Bottom: overlap with speakers in gold incorrect predictions on multi-speaker questions.\nmodels, there is significant gain in performance for all answerable questions. Further; we observe that multi-span models consistently underperform on questions (as high as 15 Fl points). unanswerable multi-span model on unanswerable Performance of questions can be negatively impacted by even one false positive I tag, changing the prediction from While prior work on unanswerable to answerable multi-span QA (Segal et al,, 2020; Li et al,,2022) have found tagging-based approaches to outper- they form single-span variants, only explore fac- toid questions 0n relatively shorter contexts_ Future work can focus 0n improving multi-span QA for open-ended questions like in MEETINGQA more\ntions and filtering (Section 3.2), the model under- performs on unanswerable questions Thus, we add an additional step to identify answerable questions and use model responses only for predicted answer- able questions. The question classification can be done zero-shot using the same FLAN-T5 model by training an external supervised model. 12 We or using observe that the FLAN-T5 model is more using effective (yields best performance) than supervised model (6 Fl point drop) as the predic- tions of the latter are biased towards the question being unanswerable. Future work can further focus accurately identifying answerable questions to on improve overall performance.\nImpact of Intermediate Training: Silver data settings augmentation effective is zero-shot in with 2l5 point improvement for single-span long context models (Table 5). For finetuned models, however; we do not observe significant improve- ments in overall scores from intermediate-training compared to directly finetuning: Interestingly, sil- augmentation improves performance on ver data unanswerable questions for single-span models (ex cept DeBERTA) and multi-span models.\nError Analysis: Next; we analyze some intrigu- ing patterns in the errors within model predictions identifying rhetorical O Firstly, we observe that questions asked in unanswerable meeting is a challenging sub-task:   Training a separate binary classification model that classifies whether a ques- tion is answerable based on the context from MEET- INGQA yields only an FL= 49.2 (see Appendix B). significant In Figure 4a, it becomes apparent that a portion of erTors in predictions for answerable ques- tions stem from the model incorrectly predicting that the question is rhetorical, particularly in the\nLastly, Instruction-Tuned Models. Table 5 performance of instruction-tuned shows zero-shot FLAN-TS model. We find the FLAN-T5 XL model (3B parameters) outperforms most zero-shot single- span models and narrowly underperforms zero-shot design multi-span models. Despite the of instruc -\n\"!Different instruction like based on the context, did anyone answer the given question? This elicits a `yes' /no' response.\n12Using the sequence classification head of RoBERTa-base model trained on questions from MEETINGQA (Appendix B)\nquestions based on their understanding of the con - versation resulting in straight-forward questions testing local information. Consequently, the answer spans of these datasets are significantly shorter single-span, restricted to one speaker and often cor- respond to Simple noun phrases (as high as 80% for QACONV) In contrast, questions asked by meet- ing participants are more open-ended, discussion- seeking, and correspond to longer answers Tx) with complex multi-span and multi-speaker scenar- ios. Note that our work is different from conver- sational QA datasets that consist of a sequence of questions and answers simulating conversation grounded in a short paragraph (Choi et al, 2018; Reddy et al,, 2019; Campos et al,, 2020).\nzero-shot setting:   Additionally, in case of multi- span answers, single-span models exhibit higher predictions include sen fraction of errors where tences not present in the answer; in contrast gold to their multi-span counterparts (for details refer to Appendix D). This follows from the construction of single-span models, as described in Section 3.2_ Lastly, for multi-speaker answers, we analyze the overlap in speakers (measured via IoU) of predicted answers in Figure 4b. and We find that even gold incorrect predictions of finetuned models contain roughly 55% speaker overlap with the gold answer; effectively predict which speak- i.e , models can question.  However; incorrect pre- ers answer the dictions in the zero-shot setting contain only 30% speaker overlap indicating that zero-shot models may struggle to predict which speakers answer the question. Future works can explore methods to effectively identify rhetorical questions and predict which speakers answer the question to improve performance. analysis of more detailed overall A errors can be found in Appendix D.\nLong-Context QA. Recent works show that QA models struggle to understand answer questions using (Pang long et al., 2022; correctly contexts 2021; Soleimani et al,, 2021; Dasigi Mou et al. et al.,2021). However; unlike our work; the source (long) documents for building these datasets are taken from written-text domains such as books film-scripts, research papers, or news articles:\nRelated Work 5\nConclusion 6\nOur work builds upon prior work on meeting tran- scripts and question answering: Rogers et al. (2021) comprehensive survey of several QA provide datasets and formats.\nIn this work, we present MEETINGQA, an extrac- meeting transcripts to tive QA dataset based on identify answers to questions asked during discus- meeting participants. Detailed analysis sion among of the data reveals it is a challenging real-world experiments with a wide variety of task. Baseline performance lags models show the current behind human performance by at least 25 and 44 over- all Fl points for finetuned and zeroshot models respectively: This demonstrates that current QA challenging, leaving tremen- systems find our task dous scope for improvement: We hope that future works will aim to bridge this gap and our work fosters research in NLP tasks (especially QA) on meeting transcripts. other text domains such as\nMeeting Transcripts. Several other small-scale meeting recordings O transcripts are corpora of publicly available (Janin et al,, 2003; Garofolo 2004; Chen et al., 2005; Mostefa et al. et al: 2007). popular We restrict ourselves the most and frequently used Other works AMI corpus_ study various aspects of summarizing meeting tran- scripts (Mehdad et al,, 2013; Wang and Cardie Shang 2013; et al,,2018; Li et al,, 2019; Zhu et al. 2020, inter alia) or extracting action-items (Mor- gan et al,, 2006; Purver et al,, 2007; Cohen et al. 2021). The work closest to ours uses Markov mod- els to classify dialogue-acts as questions, answers Kathol and Tur, 2008). or others\nAcknowledgements\nWe thank the reviewers and the area chairs for their helpful comments and feedback. We thank TELUS International for their help with data collection. We also thank Shiyue Zhang and Swarnadeep Saha This work for their helpful comments. was par- tially supported by NSF-CAREER Award 1846185, and NSF-AI Engage Institute DRL-2112635. The views contained in this article are those of the au- funding agency: thors and not of the\nPrior work com QA on Conversational Text: of QA datasets based on small chit-chat from prises Yang Sun et al,,2019; and Choi, 2019) TV shows domain-specific chat-rooms (Li et al,, 2020) or The QACONV (Wu et al,,2022) dataset builds on these works with conversations from multiple do- mains (including MEDIA SUM) these However; works employ human annotators for generating\nLimitations\ncal review; congressional proceedings, etc. which involve specific jargon and rules of engagement\nDue to the structure of MEETINGQA, the answers questions asked by participants (if any) to are present in the transcript itself; making it an ex Therefore, extensively tractive task: we do not explore the use of generative models since the pre- dictions do not stick t0 the sentences in the tran- script and could possibly include hallucinations_ we aim to mitigate hallucinations by uS- However instruction-tuned generative models with suit- ing ably designed instructions and enforce a strict exact filtering any possible hallucina- match criteria for Future work can explore how to adapt O tions evaluate non-instruction-tuned generative models on this task and better identify hallucinations with filtering to improve performance a more relaxed We also do not report zero-shot performance of InstructGPT (Ouyang et al,, 2022) as these models Additionally, are not freely accessible. we use simple multi-span QA adaptation technique from Segal et al. (2020), but predicting answer spans by classifying each token can be difficult to train leading to slightly lower performance (discussed in provides addi- hope Section 4.1). We our dataset tional motivation for future work on multi-span QA Finally, MEETINGQA only comprises of publicly available meeting transcripts in English, but our methodology of data collection and model training (using multilingual variants) should still be appli- cable for other languages in future work\nReferences \n Now the paper ends. \nQuestion: What are some challenges for future research in question answering on meeting transcripts?", "output": "Answer: The paper discusses several interesting challenges for future research in this area:\n\n1. Identifying rhetorical questions: The paper found that models struggled to identify questions that are rhetorical or do not have an actual answer in the transcript. Improving the ability to distinguish rhetorical from meaningful questions could improve performance.\n\n2. Selecting appropriate spans as answers: The paper notes that models often select sentences that are not part of the gold answer span. Better models are needed that can identify the most relevant spans as answers. \n\n3. Handling multi-span and multi-speaker answers: Many answers in meeting transcripts span multiple non-contiguous sentences and involve contributions from multiple speakers. Current models perform worse on these types of answers. Better methods for multi-span and multi-speaker QA could improve performance.\n\n4. Bridging the gap with human performance: Even the best models trail human performance by a large margin, over 20-40 F1 points. Narrowing this performance gap presents an important challenge for future research.\n\n5. Applying to other meeting datasets and languages: The current dataset only uses meetings in English from a single corpus. Applying this approach to other meeting datasets and languages could reveal additional challenges and opportunities."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n ABSTRACT\nTwo-party split learning is a popular technique for learning a model across feature- partitioned data. we explore whether it is possible for one party to In this work, steal the private label information from the other party during split training, and against such attacks whether there are methods that can protect Specifically, we first formulate a realistic threat model and propose a privacy loss metric t0 quan in split learning   We then show that there exist two simple yet leakage tify label effective methods within the threat model that can allow one party to accurately private ground-truth labels owned by the other party: To combat these at- recover tacks, we propose several random perturbation techniques, including Marvell approach that strategically finds the structure of the noise perturbation by min an imizing the amount of label ieakage (measured through our quantification metric) adversary: We empiricallyl of a demonstrate the effectiveness of our worst-case protection techniques against the identified attacks, and show that Marvell in particular has improved privacy-utility tradeoffs relative to baseline approaches_\nINTRODUCTION\nWith increasing concerns over data privacy in machine learning, federated learning (FL) (McMahan et al,, 2017) has become a promising direction of study Based on how sensitive data are distributed among parties, FL can be classified into different categories, notable among which are horizontal partitioned Yang FL and vertical FL et al,, 2019). In contrast t0 horizontal FL where the data are by examples, vertical FL considers data partitioned by features (including labels) canonical As example of vertical FL; consider an online media platform which displays advertisements from A clicking the ad and buying company B to its users, and charges B for each conversion (e.g , a user In this case, both parties have different features for each user: the product). A has features on the viewing records, while B has the user'$ conversion labels. user' s media B's labels are not available purchase behaviors happen entirely on B's websitelapp. A because each to user'\nIf both parties want t0 jointly learn model to predict conversion without data sharing; split learn- deep ing (Gupta & Raskar; 2018; Vepakomma et al,, 2018) can be used to split the execution of In vanilla split learning, before training begins, network between the parties on a layer-wise basis. both parties use Private Set Intersection (PSI) protocols Kolesnikov et al.,2016; Pinkas et al.,2018,, to find the intersection of their data records and achieve an example ID alignment: This alignment training phase. During training (Figure 1), the party without labels (non- paves the way for the split label party) sends the intermediate layer (cut layer) outputs rather than the raw data to the party with labels (label party), and the label party completes the rest of the forward computation to obtain the training loss. To compute the gradients with respect to model parameters, the label party initiates backpropagation from its training loss and computes its own parameters' gradients To allow the non-label party to also compute gradients of its parameters, the label party also computes the gradi- layer outputs and communicates this information back to the non-label ents with respect t0 the cut knowing the label party'$ raw label data, the result of the ID alignment, despite not As party: non-label party can identify the gradient value returned by the label party for each example.\nAt first glance, the process of split learning appears privacy-preserving because only the intermedi- ate computations of the cut layer_~rather than raw features or labels are communicated between \"gradient sharing\" the two parties. However; such schemes have been shown t0 be vulnerable to In vertical FL (and specifically privacy leakage in horizontal FL settings (e.g , Zhu et al,, 2019). split learning), it remains unclear whether the raw data can similarly be leaked during communi-\nFigure 1: Communication diagram of two-party split training for an example of online advertising: We study whether it is possible for the communicated gradient g to leak private label information_\nIn particular; as the raw labels often contain highly sensitive information (e.g. what a user cation_ has purchased (in online advertising) O disease or not (in disease predic- whether a user has developing a rigorous understanding of the threat of label leakage tion) Vepakomma et al. (2018)), following contributions: and its protection is particularly important: Towards this goal, we make the\nWe formalize a threat model for label leakage in two-party learning in the context of binary split  1 . classification (Section 3.1), and propose specific privacy quantification metrics to measure the severity of such threats (Section 3.2). We identify two simple and realistic methods within this threat model which can accurately 2_ recover the label party' $ private label information (Section 3.3). label-stealing ability of the We propose several random perturbation techniques to limit the 3_ non-label party (Section 4). Among them; OUT principled approach Marvell directly searches for the optimal random perturbation noise structure to minimize label leakage (as measured via quantification metric) against a worst-case adversarial non-label party. our techniques and MARVELL'$ We experimentally demonstrate the effectiveness of Our protection 4_ improved privacy-utility tradeoffs compared to other protection baselines (Section 5)_\nRELATED WORK 2\nPrivacy leakage in split learning: Although raw data is not shared in federated learning, sensitive information may still be leaked when gradients and/or model parameters are communicated between parties. Zhu et al. (2019) In horizontal FL showed that an honest-but-curious server can uncover device by knowing the model architecture, parameters, and com- the raw features and labels of techniques, Zhao et al. (2020) municated gradient of the loss 0 the device'$ data. Based on their showed that the ground truth label of an example can be extracted by exploiting the directions of gradients of the weights connected to the logits of different classes different the Here we study setting two-party split learning (in vertical FL) (Yang et al,, 2019), where no party has access to the In this setting, Vepakomma et al. (2019) model architecture or model parameters of the other party studied how the forward communication of feature representations can leak the non-label party' $ We instead study whether label information may be leaked from the raw data to the label party. during Despite the importance of label party to the non-label party the backward communication. maintaining the privacy of these labels, we are unaware of prior work that has studied this problem_\nPrivacy protection and quantification   Techniques to protect communication privacy in FL gen erally fall into three categories: 1) cryptographic methods such as secure multi-party computation (e:g;, Bonawitz et al, 2017); 2) system-based methods including trusted execution environments (Subramanyan et al, 2017); and 3) perturbation methods that shuffle O modify the communicated Abadi et al,, 2016; McMahan et al,, 2018; Erlingsson et al,, 2019; Cheu et al, 2019; (e.g: messages Our protection techniques belong to the third category, as we add random per- Zhu et al,, 2019). gradients to protect the labels Many randomness-based protection methods have turbations to the (Dwork; 2006; been proposed in the domain of horizontal FL In this case, differential privacy DP) ability to to measure the proposed random mechanisms 2014) is commonly used Dwork et al: anonymize the identity of any single participating example in the model iterates. However; in split learning, after PSI, both parties know exactly the identity of which example has participated in given gradient update. As we explain in Section 3.1, the object we aim to protect (the communi- cated cut layer gradients) , unlike the model iterates, is not an aggregate function of all the examples result; DP and its variants (e.g.    label DP but are instead example-specific. As Chaudhuri setting, and we instead Hsu, 2011; Ghazi et al,, 2021)) are not directly applicable metrics in our propose a different metric (discussed in Section 3.2).\n3 LABEL LEAKAGE IN SPLIT LEARNING\nlearning problem for binary classification, and then formally We first introduce the two-party split describe our threat model and privacy quantification metrics with two concrete attack examples.\n3.1 Two-PARTY SPLIT LEARNING IN BINARY CLASSIFICATION\nbinary' Problem setup. learning composition model h Consider two parties f jointly for classification problem over the domain X X {0, 1} (Figure 1) The non-label party owns the repre- Rd and each example's raw feature X \u20ac X while the label party owns sentation function f X _ Rd {0,1}2 . h(f(X)) be the logit function h R and each example's label y Let \u20ac 0 the logit of the positive class whose predicted probability is given through the sigmoid function: We measure the loss of such prediction through the cross entropy loss 1/(1 exp(-e)). 4 p1 y) e. During model inference, the non-label party computes f(X) log(1 exp( _ e) L and sends it to the label party who will then execute the rest of forward computation in Figure 1_\nModel  training (Figure 1: backward gradient computation). To train the model using gradient descent;, the label party starts by first computing the gradient of the loss L with respect to the logit Using the chain rule, the label party can then compute the gradient of L with y). p1 parameters and perform the gradient updates. To also allow the non-label respect to its function h party to learn its function f, the label party needs to additionally compute the gradient with respect denote this gradient by We to cut layer feature f(X) and communicate it to the non-label party. \")V_h(e)l-=fcx) \u20ac Rd (by chain rule) After receiving g, the non-label 'fcx)L = (P1 = 9 party continues the backpropagation towards f '$ parameters and also perform the gradient updates.\nWhy Not Differential Privacy? Note that for the non-label party randomly given iteration, Therefore, the identity of which examples are used is chooses B example IDs to form batch: known to the non-label party by default. In addition, the communicated features f ( X) and returned gradients g will both be matrices in with each row belonging to a specific example in the batch: RBxd The different gradients (rOws of the matrix) are not with respect to the same model parameters, but shuffling are instead with respect to different examples' cut-layer features; thus, no averaging Over Or of the rows of the gradient matrix can be done to communication to ensure correct computa- prior tion of f'$ parameters O the non-label party side. This example-aware and example-specific nature of the communicated gradient matrix makes differential privacy (which focuses on anonymizing an example ' $ participation in an aggregate function) inapplicable for this problem see also Section 2)\n3.2 THREAT MODEL AND PRIVACY QUANTIFICATION\nBelow we specify several key aspects of our threat model, including the adversary's objective and capabilities, our metric for quantifying privacy loss, and the possible inclusion of side information\nAdversary's objective: given moment in time during training At a with f and h fixed), since the communicated cut layer gradient g is a deterministic function of y (see Section 3.1), we consider an adversarial non-label party whose objective is to recover the label party's hidden label y based on training example. the information contained in g for every\nAdversary's capability. We consider an honest-but-curious non-label party which cannot tamper training by selecting which examples to include in a batch Or sending incorrect features f(X); with training procedure while trying instead, we assume that the adversary follows the agreed-upon split binary classification problem where the (input, output) to guess the label y This can be viewed as a We allow the adversary to use any binary classifier distribution is the induced distribution of (g,y). Rd {0,1} to guess the labels (scoring function This classifier can be represented by q 4 Rd gradient to threshold t) tuple, where r layer R maps an example $ cut a real-valued score cut-off s0 that q(g) = 1 ifr(g) > t and q(g) = 0 ifr(g) and the threshold t \u20ac R determines t we use this tuple representation to describe adversarial non-label party classifiers_ Moving forward,\nPrivacy loss quantification. binary classification, As we consider a natural metric to quantify the performance of an adverary' $ scoring function is the AUC of its ROC curve Denote the unperturbed class-conditional distributions of the cut-layer gradients by P(1) and P(0) for the\nPTo we assume no additional features in the label party to compute the logit: The data simplify notation leakage problem still holds true for other more complicated settings (see WDL experiment setting in Section 5).\nFigure 2: Distributions of quantities discussed in Observations [-4 after the first 100, 200,300, 400, 500 steps of stochastic gradient descent training of the WDL model on Criteo (see experiments)\nSide information.   Among all the scoring functions within our threat model, it is conceivable that Picking such effective ones would require only some would recover the hidden labels accurately: population-level side information specifically regarding the properties of the non-label party to have the positive and negative class'$ cut-layer gradient distributions. Since we (and distinction between allow the adversary to pick any specific (measurable) scoring function, we implicitly allow for such population-level side information for the adversary: However we assume the non-label party has no example-level side information that is different example by example. Thus we also don't use local DP for privacy quantification (detailed explanation in Appendix A.8). Next we provide two example scoring functions which use population-level side-information to effectively recover the label.\nPRACTICAL ATTACK METHODS 3.3\nAttack I: Norm-based scoring function. IVah(a)la-fex)llz: Note that Ilgllz p1 We y| following observations for |p1 'ah(a)la-f(x)llz; which hold true for yl and make the wide range of real-world learning problems.\nObservation I: Throughout training, the model tends to be less confident about a positive ex- ample being positive than a negative example being negative. In other words, the confidence gap positive example 1 1) is typically larger than the confidence gap yl (when y of a p1 Ip1 negative example 1 0) (see Figure 2(a)) of a when y This observation p1 p1 y po is particularly true for problems like advertising conversion prediction and disease prediction, where there is inherently more ambiguity for the positive class than the negative. For example, in advertising, uninterested users of a product will never click 0n its ad and convert; but those clicking, might make the purchase only depending interested, even after a fraction of the time time/money constraints. (See A.2 for such an ambiguity even for a class-balanced setting:) on Observation 2:  Throughout training, the norm of the gradient vector IV-h(e)l--f(x)llz is magnitude (has similar distribution) for both the positive and negative on the same order of examples (Figure 2(b)). This is natural because ah(a)la-f(x) is not a function of y\nand 2, the gradient norm Ilgllz of the positive instances are consequence of Observation As than that of the negative ones (Figure 2(c)). Thus, the scoring function Tn (9) larger generally strong predictor of the unseen label y: Ilgllz is We name the privacy loss (leak AUC) measured the attack Tn the norm leak AUC. In Figure 2(c) , the norm leak AUCs are consistently above lagainst 0.9, signaling a high level of label leakage throughout training:\nAttack 2: Direction-based scoring function   We now show that the direction of g (in addition to its magnitude) can also leak the label. For a of examples, ( Xa; Ya), ( Xb, Yb) , let their respective pair _ predicted positive class probability be p1,a, p1,b and their communicated gradients be ga, 9b. Let X Rd 9a gb / (Ilgall2llgbllz) . It is easy R denote the cosine similarity function cos(ga , 9b) R coS 57 Vh(2)l.-f(Xa), zh(2)l--f(Xb)) , where to see that cos(ga , 9b_ = sgn(p1,a Ya ) sgn(p1 , Yb ) cos( sgn(x) is the sign function which returns 1 if x > 0, and -1 if x We highlight two additional 0_ observations that can allow US to use cosine similarity to recover the label\nsgn(p1 , When the examples a,b are of different classes, the Observation 3: term Ya negative. b are of the same ~1 is On the other hand, when examples sgn(p1, Yb_ @, class (both positivelboth negative), this product will have a value of 1 and thus be positive. Observation 4: Throughout training, for any two examples &, b, their gradients of the function h h(2)l.-f(Xs)) always form an acute angle, i.e. > 0 (Figure 2(d)). For h(z)l2=f(Xa)= cOS neural networks that use monotonically increasing activation functions (such as ReLU, sigmoid, tanh), this is caused by the fact that the gradients of these activation functions with respect to its inputs are coordinatewise nonnegative and thus always lie in the first closed hyperorthant:\nis the product of the terms from Since cos(ga , 9b Observation 3 and 4 we see that for a given example, all the examples that are of the same class result in a positive cosine similarity, while all If the problem is class-imbalanced opposite class examples result in a negative cosine similarity are fewer positive examples than negative ones, it can thus and the non-label party knows there negative if more than half of the examples result determine the label of each example: the class is in positive cosine similarity; otherwise it is positive. many practical applications, the non-label For party may reasonably guess which class has more examples in the dataset without ever priori population having ~for example, in disease prediction, the percentage of the entire seeing any data advertising conversion prediction a certain disease is almost always much lower than 50%; in online the conversion rate (fraction of positive examples) is rarely higher than 30%. Note that the non-label party doesn 't need knowledge of the exact sample proportion of each class for this method to work.\nTo simplify this attack for evaluation, we consider an even worse oracle scenario where the non-label gradient of one positive example 9+- Unlike the aforementioned practical party knows the clean majority counting attack which needs t0 first figure out the direction of one positive gradient; this Thus, any protection oracle scenario assumes the non-label party is directly given this information method capable of defending this oracle attack would also protect the more practical one_ against With 9+ given, the direction-based scoring function Ta is simply ra(g) We name cos(g, 9+ ). the privacy loss (leak AUC) against this oracle attack Ta the cosine leak AUC. In practice, we randomly choose a positive class clean gradient from each batch as g+ for evaluation For iterations in Figure 2(e), the cosine leak AUC all have the highest value of 1 (complete label leakage).\nLABEL LEAKAGE PROTECTION METHODS 4\nheuristic random perturbation approach designed t0 prevent the In this section, we first introduce We then propose a theoretically justified method that aims practical attacks identified in Section 3.3_ to protect against the entire class of scoring functions considered in our threat model Section 3.2)\n4.1 A HEURISTIC PROTECTION APPROACH\nRandom perturbation and the isotropic Gaussian baseline: To protect against label leakage, the label party should ideally communicate essential information about the gradient without commu nicating its actual value. Random perturbation methods generally aim to achieve this goal. One obvious consideration for random perturbation is to the perturbed gradients unbiased. In other keep words, suppose g is the perturbed version of an example' $ true gradient g, then we want Elg 9] = 9 By chain rule and linearity of expectation, this ensures the computed gradients of the non-label party' $ parameters f will also be unbiased, a desirable property for stochastic optimization. Among perturbation methods, simple approach is to add iid isotropic Gaussian noise to every unbiased gradient t0 mix the positive and negative gradient distribution before sending to the non-label party:\nAlthough isotropic Gaussian noise is a valid option, it may not be optimal because 1) the gradients are vectors but not scalars, So the structure of the noise covariance matrix matters_ Isotropic noise might neglect the direction information; 2) due to the asymmetry of the positive and negative gradi- gradients ent distribution, the label party could add noise with different distributions t0 each class\nWe now introduce an improved heuristic approach of adding zero Norm-alignment heuristic: mean Gaussian noise with non-isotropic and example-dependent covariance. [Magnitude choice] As we have seen that Ilgllz can be different for positive and negative examples and thus leak label information, this heuristic first aims to make the norm of each perturbed gradient indistinguishable from one another:  Specifically, we want to match the expected squared 2-norm of every perturbed gradient in a mini-batch to the largest squared 2-norm in this batch (denote by IlgmaxIlZ) [Direction choice] In addition, as we have seen empirically from Figure 2(e), the positive and negative gradients a one-dimensional line in Rd with positive examples pointing in lie close to one direction and negative examples in the other: adding noise (roughly speaking) along \"this Thus we consider only gradient 9; in the batch, we add a zero-mean Gaussian noise vector line\" More concretely, for a 1j supported only o the one-dimensional space along the line of 9j In other words, the noise' $ 0}9;9] _ To calculate & j, we aim to match Elllg; +n;ll?] covariance is the rank-1 matrix Cov[nj] = IlgmaxI? simple calculation gives 0j 719;IE align to the maximum Since V Ilgmax 1. we norm, we name this heuristic protection method max_norm: The advantage of max_norm is that it has no parameter to tune. Unfortunately, it does not have strong theoretical motivation, cannot flexibly trade-off between model utility and privacy, and may be broken by some unknown attacks_\n4.2 OPTIMIZED PERTURBATION METHOD: MARVEL L\nMotivated by the above issues of max_norm; next study how to achieve principled we a more trade-off between model performance (utility) and label protection (privacy). To do $o, we directly scoring function s leak AUC under minimize the worst-case adversarial utility constraint: We name this protection method Marvell (optiMized perturbAtion to pReVEnt Label Leakage).\nNoise perturbation structure: Due to the distribution difference between the positive and nega layer gradients, we consider having the label party additively perturb the randomly tive class' $ cut sampled positive 9(1) and negative 9(0) gradients with independent zero-mean random noise vectors possibly different distributions (denoted by D() and D() ). We use P(1) and and n(0) with P(o) 1 (1) to denote the induced perturbed positive and negative gradient distributions. Our is to find the goal optimal noise distributions D() and D() by optimizing our privacy objective described below:\nPrivacy protection optimization objective. As the adversarial non-label party in our threat model against all is allowed t0 use any measurable scoring function for label recovery, we aim to protect such scoring functions by minimizing the privacy loss of the worst case scoring function measured Formally, our optimization objective is min D(1)_ through our leak AUC metric. AUC(r) . D() maxr using Here to compute AUC(r), the FPR,(t) and TPR,(t) needs to be computed the perturbed distributions P(1) P(o) instead of the unperturbed P(1) and P(0) (Section 3.2). Since AUC is and difficult to directly optimize, we consider optimizing an upper bound through the following theorem:\nFor 0 < \u20ac < 4 and any perturbed gradient distributions P() and P() that are abso- Theorem 1 lutely continuous with respect to each other;\nUtility constraint: In an extreme case, we could add infinite noise to both the negative and positive gradients. optimally to 0 and make the worst case leak AUC 0.5, which is This would minimize gradient descent cannot converge under infinitely equivalent to a random guess. However; stochastic large noise, so it is necessary to control the variance of the added noise We thus introduce the noise tr(Cov[n()1) + (1 tr(Cov[n(0)]) where p is the fraction of positive power constraint: p P, p) examples (already known to the label party); tr(Cov[n(i)1) denotes the trace of the covariance matrix of the random noise n(); tunable hyperparameter to control the level and the upper bound P is of noise: larger P would achieve a lower sumKL and thus lower worst-case leak AUC and better leading t0 slower optimization privacy; however; it would also add more noise to the gradients,\nWe weight each class's noise level tr(Cov[n convergence and possibly worse model utility. by its p) since, from an optimization perspective, we want to equally control example proportion (p O 1 every training example's gradient noise. The constrained optimization problem becomes_\nOptimizing the objective in practice: To solve the optimization problem we first introduce some modelling assumptions We assume that the unperturbed gradient of each class follows a Gaussian being distribution: g(1) N(g(1) , vIdxd) and g(0) N (g() , ulaxd) approximation, Despite this an as we see later in Section 5,it can achieve strong protection quality our identified attacks. In against  addition, it makes the optimization easier (see below) and provides US with insight 0n the optimal noise structure. We also search for perturbation distributions that are Gaussian: D(1) N (0, E1 and N (0,Zo) with commuting covariance matrices: D(0) The commutative require - ZiEo ZoZl_ slightly restricts our search space but also makes the optimization problem more tractable Our ment goal is to solve for the optimal noise structure, i.e. the positive semidefinite covariance matrices Zo= 9(0) denote the difference between the positive and negative gradient'$ mean Zl: Let 4g : 9(1) following theorem (proof and interpretation in Appendix A.4): We now have the vectors\nThe optimal Ei and Xo to (2) with the above assumptions have the form: Theorem 2.\noptimization problem positive By Theorem 2, Additional details of Marvell our over two much simpler 4-variable optimization problem: semidefinite matrices is reduced to We include detailed description of how the constants in the problem are estimated in practice and what solver algorithm in Appendix A5. we use in a full description of the Marvell Beyond optimization de- tails, it is worth noting how to set the power constraint hyperparameter P in Equation 2 in practice_ As directly choosing P requires knowledge of the scale of the gradients in the specific application I4gll; and and the scale could also shrink as the optimization converges, we instead express P = This alleviates the need t0 know the scale of the gradients tune for a fixed hyperparameter 0. in advance, and the resulting value of P can also dynamically change throughout training as the distance between the two gradient distributions Agll; changes. mean\n5 ExPERIMENTS\nwe first describe our experiment setup and then demonstrate the label protection In this section, quality of Marvell as well as its privacy-utility trade-off relative to baseline approaches.\nEmpirical Setup: We use three real-world binary classification datasets for evaluation: Criteo and Avazu; two online advertising prediction datasets with millions of examples; and ISIC, healthcare All three datasets exhibit severe label leakage problem image dataset for skin cancer prediction. on dataset and preprocessing details). without protection: We defer similar (see Appendix A.6.1 results on Avazu to Appendix A.7 and focus o Criteo and ISIC in this section. For Criteo, We train embedding layers for Wide&Deep model (Cheng et al,, 2016) where the non-label party owns the input features and the first three 128-unit ReLU activated MLP layers (first half of the deep part) remaining layers of the deep part and the entire wide part of the while the label party owns the layers each with 64 channels followed by For ISIC, we train a model with 6 convolutional mmodel3_ layer; and the cut layer is after the fourth convolutional layer: a 64-unit ReLU MLP In this case, 5x5x64 layer feature f(X) and gradient g are both in an example's cut We treat such tensors as vectors in R16O to fit into our analysis framework (for additional model architecture and training Appendix A.6.2, A.6.3 details see\n3In this setting; the label party will also process input features (through the wide part) just like the non-label party, further relaxing our formal split learning setup in Section 3_\nFigure 3: Norm and cosine leak AUC (computed every batch; at the cut layer and at the first layer under no with different scale hyperparameter $ throughout the ISIC training_ protection VS . Marvell\nAND FLEXIBLE PROTECTION 5.1 LABEL LEAKAGE S STRONG AND MARVELL\nWe first evaluate the protection quality of Marvell against the and cosine attacks dis- norm against the leakage metrics when no protection is applied cussed in Section 3.3. We also compare As the results across the three datasets are highly similar; we use ISIC as an example (no_noise): We see in Figure 3(a)(b) that unlike no_noise where the label (other datasets see Appendix A.7.1). information is completely leaked (leak AUC ~ 1) throughout training; Marvell achieves a flex ible degree of protection (by varying s) against both the norm 2(a) and direction attacks 2(b) on the cut layer gradients and has strong protection (leak AUC ~ 0.5) at $ 4.0. Additionally, = gradients of layers before the cut layer (on the non-label party side it is natural t0 ask whether the can also leak the labels as the non-label party keeps back propagating towards the first layer: In Fig- ure 3(c)(d), we compute the leak AUC values when using the non-label party'$ first layer activation gradient as inputs t0 the scoring functions to predict y gradient layer Without protection, the first still leaks the label very consistently. In constrast, Marvell still achieves strong privacy protection being 4.0) despite the protection analyzed at the cut layer: at the first layer (s\nPRIVACY-UTILITY TRADE-OFF COMPARISON 5.2\nprovide strong privacy protection against our identified attacks, After showing Marvell can we against other prO- now see how well it can preserve utility by comparing its privacy-utility tradeoff no_noise, isotropic Gaussian (is0), and our proposed heuristic max_norm tection baselines: Similar to how we allow Marvell power constraint to depend on the current iteration $ to use gradient distribution through P = sllAgll2, we also allow iso to have such type of dependence Iguax|Z Iaxa) to every gradient in specifically, we add N(0, (t/d) batch with t tunable To trace out the complete tradeoff curve privacy hyperparameter to be fixed throughout training: training is0, we conduct more than 20 runs for each protection method with for Marvell and different value of privacy hyperparameter s for Marvell t for is0) in each run on every dataset (Note that no_noise and max_norm do not have privacy hyperparameters._\nWe present the tradeoffs between privacy (measured through norm and cosine leak AUC at cut using layer/first layer) and utility (measured To summarize the test loss and test AUC) in Figure 4_ training run, we pick the 95% quantile over the batch-computed leak AUCs leak AUC over a given throughout all training iterations This quantile is chosen instead of the mean because we want to measure the most-leaked iteration'$ privacy leakage (highest leak AUC across iterations) to ensure points during training:  95% quantile is chosen instead of the max the labels are not leaked at any against randomness of the training process_ (100%, as we want this privacy leak estimate to be robust\nPrivacy-Utility Tradeoff comparison results  In measuring the privacy-utility tradeoff, we aim t0 find a method that consistently achieves a lower leak AUC (better privacy) for the same utility value.\nAs shown in Figure 4, Marvell almost always achieves [Marvell vs iso] a better trade - off than iso against both of our proposed attacks at both the cut layer and the first layer on important to note that although the utility constraint is both the ISIC and Criteo datasets It is in terms of training loss optimization; Marvell's better tradeoff still translates to the gener- alization performance when the utility is measured through test loss O test AUC. Additionally, despite achieving reasonable (though still worse than Marvell) privacy-utility tradeoff against is0 performs much worse against the direction-based attack: the norm-based attack; on ISIC, significant amount of isotropic noise (with t even after applying a is0's cosine leak 20), layer (Figure 4(b,f)). AUC is still higher than 0.9 at the cut In contrast; Marvell is effective against this direction-based attack with a much lower cosine leak AUC 0.6.\nFigure 4: Privacy (norm cosine leak AUC) vs Utility (test loss & test AUC) trade-off of protection methods (Marvell, iso, no_noise max_norm) at the cut and first layer on ISIC and Criteo_\n[max_norm heuristic] Beyond Marvell approach max_norm can we see that our heuristic (Figure 4(a,f,)) leak AUC value than Marvell at the match and sometimes achieve even lower We believe this specifically results from our norm and direction consideration utility level_ same when designing this heuristic. However; without a tunable hyperparameter; max_norm cannot tradeoff between privacy and utility: Additionally, unlike Marvell which is designed to protect scoring functions, max_norm might still fail to protect the entire class of adversarial against against other future attack methods beyond those considered here.\nIn summary, Our principled method Marvell significantly outperforms the isotropic Gaussian work particularly well proposed max_norm heuristic can also baseline, and the against our norm- and direction-based attacks which we identified in Section 3.3.\nCONCLUSION 6\nlabel leakage threat model in the two-party split learning binary clas- In this paper; we formulate sification problem through a novel privacy loss quantification metric (leak AUC) Within this threat provide two simple yet effective attack methods that can accurately uncover the private model, we labels of the label party: To counter such attacks, we propose a heuristic random perturbation method optimal theoretically principled method Marvell max_norm aS well as which searches for the noise distributions to protect against the worst-case adversaries in the threat model. We have con- ducted extensive experiments to demonstrate the effectiveness of Marvell and max_norm over the isotropic Gaussian perturbation baseline iso_\nOpen questions and future identify, rigorously Our work is the first we are aware of to work: quantify; and protect against the threat of label leakage in split-learning, and opens up a number of worthy directions of future study: In particular; as the model parameters are updated every batch in problem setup, the true gradient of an example and the gradient distribution would both change. our interesting question is whether the adversarial non-label party can remember the stale gradient An of the same example from past updates (possibly separated by hundreds of updates steps) in order to recover the label information in the current iteration in a more complex threat model. It would also be interesting to build on our results to study whether there exist attack methods when the classification binary; problem is multiclass instead of and when the split learning scenario involves more than two parties with possibly more complicated training communication protocols (e.g ; Vepakomma et al. 2018)\nleakage in the two Ethics Statement: we have identified a realistic threat of label In our paper; party split learning binary classification problem. We aim to raise awareness about potential privacy applications have been deployed. issues in this problem domain; where many industrial Beyond making such threats clear; we have taken the first steps towards protection proposed both we have hope heuristic and principled methods that can preserve label privacy: We our work will pave the way for future analyses that make the two-party split learning framework more effective and secure_\nTo make Our paper reproducible, we provide: Reproducibility Statement:\nand Theorem 2 in Appendix A.3,A.4; Proofs of our Theorem Detailed experiment description including 1) data preprocessing; 2) model architecture, 3) training algorithm and hyperparameters in Appendix A.6. running instructions (in README Source code with md) at https:I githubcom/OscarcarLi/label-protection.\nREFERENCES \n Now the paper ends. \nWhat are the limitations of the proposed Marvell approach for protecting against label leakage and how could it be improved?", "output": "Some potential limitations:\n\n1. Marvell only considers simple binary classification problems. It would be interesting to see how it performs for multi-class classification problems.\n\n2. Marvell only considers the two-party setting. It's performance is unknown in settings with more than two parties. \n\n3. Marvell requires some knowledge of the scale of the gradients to set the power constraint hyperparameter P. Improving the way this hyperparameter is chosen could make the approach more practical. \n\n4. Marvell focuses on protecting against label leakage during backward gradient communication. Future work could look at protecting the forward pass communication as well.\n\n5. Marvell only considers specific norm-based and direction-based attacks. The approach could be further improved by also considering more complex attack strategies.\n\nTo improve Marvell, future work could:\n\n- Extend it to multi-class settings \n- Generalize it to settings with more than two parties\n- Develop a more adaptive way of choosing the power constraint hyperparameter\n- Investigate how to protect both forward and backward communication \n- Explore defenses against more complex and unlabeled attack strategies"}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nhas been an active area throughout the history of speech research Hemphill et al,, 1990; Calhoun et al, 2010; Busso et al,, 2008; Zadeh et al,, 2018; Chen et al., 2020a; Cohn et al., 2019; Yadav et al. 2020; Martinez-Lucas et al,, 2020). However; com pared to \"lower-level tasks like automatic speech recognition (ASR) and speaker identification, SLU has received much less attention and resources, and specifically there are much fewer benchmarks with freely available data:\nSpoken language understanding (SLU) tasks have been  studied for many decades in the speech research community, but have not re - ceived as much attention as lower-level tasks like speech and speaker recognition. In this we introduce several new annotated SLU work; benchmark tasks based freely   available on speech data, which complement existing bench- marks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve infer- longer speech sequences; named en ence over tity localization addresses the speech-specific locating the targeted content in the sig task of nal; dialog act classification identifies the func given speech utterance. tion of In order t0 facilitate the development of SLU models that leverage the success of pre-trained speech rep resentations, we will release a new benchmark suite, including for each task (i) curated an notations for a relatively small fine-tuning set (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model per formance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance We of the baseline models. also analyze the sensitivity of pipeline models' performance to the speech recognition accuracy, using more than 20 publicly available speech recognition models_\nSLU tasks can in principle be addressed via a using speech pipeline approach map ASR to to text and an NLP (text) model to map text to the desired output: The alternative is an end-to- end (EZE) model, which maps directly from the input speech to the target output: While pipeline approaches can take advantage of existing strong ASR and NLP models, EZE models can be more efficient at inference time, can avoid ASR error propagation, and can directly use aspects of the speech signal beyond the text that are useful for the prosody) (Arora et al,, 2022a; Chen et al. (e.g , task 2020b; Jurafsky et al,, 1998; Tran et al,, 2018). In addition, for tasks whose output includes speech segments or time spans, there is no direct combi- nation of an ASR model and an NLP model that produces precisely the desired type of output: For some SLU tasks, the current state f the art is Shon et al,,2022a; Lai et al,,2020), pipeline model whereas for others EZE models are better (Pasad et al,, 2021; Sharma et al,,2022; Wu et al,, 2022b PPeng et al,,2022; Arora et al,,2022b; Shon et al. 2022b). In order to better understand the pros and cons of pipeline and EZE approaches, more public benchmarks are sorely needed.\nIntroduction 1\nSpoken language understanding (SLU) tasks in- inferring the linguistic structure O semantic volve meaning of a speech signal beyond its text tran- script: We use this term broadly to include any language processing (NLP) task applied natural speech, and tasks that involve linguistic under- to standing but also localization in the signal of rele- vant segments Or producing speech as output. SLU\nWhile   collecting   large of labeled amounts speech  data   for many   SLU may be  pro tasks hibitively costly, recent advances in pre-trained Baevski et al,,2020; Hsu et al,,2021; Chen models et al,,2021; Wu et al,,2022a; Baevski et al,, 2022; Lin et al , 2022b; Mohamed et al., 2022) make\nCore contributors in alphabetical order\nRelated work 2\nfine-tuning sets it feasible to use relatively small for each task. There have been several recent ef- forts to introduce new benchmark SLU tasks (Yang et al,, 2021; Bastianelli et al,, 2020; Feng et al. 2021; Evain et al,, 2021; Arora et al., 2022b; Lu- et al,,2021a; Shon et al,, 2022a; Tomasello pgosch et al,, 2022), most (but not all) using fairly small ttraining sets of several hours to several dozens of Among hhours of speech. them, the Spoken Lan - guage Understanding Evaluation (SLUE)l Shon et al,, 2022a) motivated us since it pursues a natural speech, rather than a short command type of speech populated in other benchmarks. that is However; there are only two SLUE tasks (sentiment analy sis and named entity recognition), thus more tasks with different complexities are needed to cover the diverse application of SLU.\nYang SUPERB et al,, 2021) aggregates several existing speech tasks mainly to evaluate frozen pre-trained speech  models It focuses low on level tasks but also contains two SLU tasks intent classification (from Fluent  Speech Com- mands (Lugosch et al,, 2019)) and slot filling (from SNIPS (Coucke et al,, 2018)). However; the for- many models have is easy task where mer an close to 100% accuracy, and the latter uses syn- thesized rather than natural speech: SLURP Bas - spoken version of a text tianelli et al,, 2020) is a Liu et al,, 2019) where the authors hired dataset workers to dictate the written conversations be- tween humans and personal robot assistants: It scenario prediction, includes three SLU tasks entity prediction. action prediction, and These generalized as the nature of the tasks cannot be (Feng speech command. ASR-GLUE short et al. 2021) is based on the well-known GLUE bench- mark (Wang et al,, 2018, where the authors hired people to speak the GLUE text It includes five However GLUE tasks and one additional task: ASR-GLUE contains only a test set; researchers must rely 0 other datasets for training: Timers and Such (Lugosch et al,, 2021b) is a dataset of speech commands that involve numbers, designed filling for intent classification and slot that has lim- Spoken SQuAD (Lee et al,, 2018) ited use case and Spoken CoQA 2022) (You et al., are syn- thesized speech versions of the text SQuAD (Ra- jpurkar et al, 2016) and CoQA (Reddy et al, 2019) datasets   NMSQA (Lin et al., 2022a) is a multi- speaker QA dataset whose test set contains spoken natural speech but the train and validation sets are synthesized. Other well-known SLU datasets in- clude ATIS (Hemphill et al,, 1990) and Switch- board NXT (Calhoun et al., 2010), which have been used for tasks like intent and DAC, but the data is available under license constraints Wu et al (2020) published an open-sourced speech dataset; however; its dialog act annotation is not manually annotated but predicted using commercial API.\nWe introduce   SLUE Phase-2, set of SLU tasks that complement the existing SLU datasets OI dialog benchmarks. The new tasks include act clas - sification (DAC), question answering (QA), sum- marization (SUMM), and named entity localiza- tion (NEL) , applied to English speech data. SLUE Phase-2 has several advantages compared to other recent work introduced in section 2:\nMore diverse tasks: SLUE phase-2 not only in- clude utterance or word-level classification task but also includes QA and SUMM task:\nMore challenging tasks: The complexity of the task is influenced by the type of input and the type of output   SLUE phase-2 uses conversational or longer discourse speech as input The type of out- put is not limited to labels or text, but also includes the speech span time stamp.\nNew human annotation: new annotation A was collected by human annotator: Human annotator validated an automatically-collected data if needed. Natural speech: We do not use synthesized speech. considerably We only include conversational or long discourse speech rather than short speech com- mands_\nCC license: Creative Common licensed dataset to give the best freedom of use_\nprovide publicly available? For each task; we datasets, annotations, models, and code. We pro- vide both pipeline and EZE baseline models and for pipeline models, we use multiple ASR systems to analyze the effect of the ASR error rate on the final task performance.\nSpeech summarization has gained interest over the past  few with such abstrac - tasks as years tive summarization of instructional How-2 videos (Sanabria et al., 2018) and TED Talks (Kano et al. 2021), but the raw audio for these tasks is not pub- licly available. Other corpora, such as the ICSI (Janin et al,, 2003) and AMI (McCowan et al. 2005) meeting summarization corpora, contain rel-\nIWe refer to the original SLUE as \"SLUE Phase-].\n2To be released.\nNamed entity local- atively less annotated data: ization (NEL) is a fairly new task: A similar task; audio de-identification (audio de-ID); has been in- for troduced with annotations conversational data from Switchboard and Fisher (Cohn et al., 2019; Baril et al,, 2022), but these datasets are not free_ special case of NEL where the en- Audio de-ID is a tities of interest are related to personal identifiers.\n2020) to evaluate the overlap between the predicted and the ground-truth answer spans.\nSpeech summarization (SUMM) 3.13\nthe task of generating SUMM refers to text given  speech input: summary   from SUMM requires is challenging as it model to assimi late information across very long input contexts to identify essential  information in order and paraphrase to obtain the abstractive summary of using ROUGE (Lin; We evaluate SUMM speech: 2004), METEOR (Denkowski and Lavie, 2014) and BERTScore (Zhang* et al,, 2020).\nfocus English speech-related work (most We on comparable with our work), but there are also ongo- ing efforts for other languages (Tomashenko et al. 2019; Evain et al,, 2021).\nSLUE Phase-2: Tasks and data 3\nNamed Entity Localization (NEL) 3.1.4\nThis section introduces the tasks and metrics in The SLUE phase-1 introduced the SLUE Phase-2. summary of model per- \"SLUE score a numerical However; as formance across tasks. we consider a more diverse set of tasks, using the same pre- trained model for all tasks is difficult; and evalua- tion via a single SLUE score may discourage build ing systems for individual tasks. In SLUE Phase-2, therefore, we do not adopt the single SLUE score, and evaluate each task individually:\nof NEL is to predict the start and end The goal spoken utterance_ times of any named entities in a NEL is related to named entity recognition (NER), but NER involves identifying entity phrases while NEL involves locating them in the audio. We eval- performance via two FL scores based on the uate overlap between the predicted and ground-truth defined similarly to the time ranges: frame-Fl, QA frame-Fl measure; and word-Fl, defined simi- larly to the de-identification metric of Cohn et al. (2019). The word-Fl score has a hyperparameter p \u20ac [0, 1], which is the fraction of overlap between ground-truth word segment and the predicted re- a gion needed to count the word as detected; p = perfect match is required. means\nTasks 3.1\nWe explore more diverse and complex tasks com pared to SLUE phase-1. As an extension of NER task in SLUE, we describe the NEL task to predict the audio time-stamps of named entities. DAC is an utterance classification task within conversation in- dialog teractions to predict acts given input speech: We address two longer-range context tasks: QA and long sequence and SUMM where the model takes a utilizes context across the entire scope to answer questions O summarize speech respectively.\n3.2 Datasets and annotation\nSLUE-HVB for DAC 3.2.1\nFor the DAC task we adapt the Harper Valley Bank (HVB) spoken dialog (Wu et al,, 2020) corpus3 of scripted consumer banking dialogs, simulated The data contains about 23 hours by 59 speakers. transcrip- of audio from 1,446 conversations with dialog tions and metadata, as well as act annotation However; the original DAC annotation is automatic, without manual validation, and the set of dialog acts is simple and tailored to this corpus We define a new set of acts and collect human annotations by professional annotators listening to the audio. Our dialog acts (See Table 9 in Appendix set of new for detail) is based on the well-known Switchboard NXT (Calhoun et al,, 2010) dialog act set. Based pilot annotation we remove several unneeded on labels and merge others unnecessarily granular: Fi split the HVB data into fine-tune, dev; and nally; we test sets (Table 1). The intent of conversation is\nDialog Act Classification (DAC) 3.1.1\nDAC is the task of identifying the function of a dialog;  question, given speech utterance in a such as statement or backchannel: It is an utterance-level multi-label multi-class classification task; that is an utterance can have more than one class (func- using We evaluate DAC macro-averaged tion). (unweighted) FL score.\nQuestion Answering (QA 3.1.2\nThe goal of QA is t0 find the answer span in a spoken document given a spoken question: The span is denoted by the start and end frames answer of a short phrase in the document: We use the Chuang et al. frame-level FI (frame-Fl) score\n'cC-BY-4.0 license\n(2017) to search for relevant documents to the ques - tions with their transcripts automatically: The de- tailed search criteria and the final number of SLUE- SQA-5 questions from each source text QA dataset are in Section B.2 and Table 11 in the Appendix.\nbalanced along the splits. We exclude short audio clips (<2]0ms) and audio that contains no speech:\nTable 1: SLUE-HVB data statistics\nTo ensure the evaluation quality, we also asked human annotators to pick 408 question-document which the document provides enough pairs, in clues to answer the question, from test data as the verified-test set: The data statistics of SLUE-SQA- 5 are in Table 2.\nSLUE-SQA-5 for QA 3.2.2\nPrevious open-source English QA datasets, spoken including Spoken SQuAD (Lee et al, 2018), NM- SQA (Lin et al,, 2022a) , Spoken-CoQA (You et al. training set consisting of 2022) , do not have a large realistic human speech; so we propose a new spo- ken QA dataset; SLUE-SQA-5, whose fine-tune, dev, and test sets all consist of real speech data:\nTable 2: SLUE-SQA-5 data statistics\nThe text transcriptions of question-answer pairs in SLUE-SQA-5 are collected from five differ- SQuAD' (Rajpurkar et al. ent text QA datasets: 2016), Natural QuestionsS (NQ) (Kwiatkowski et al,, 2019), TriviaQA6 (Joshi et al,, 2017), We- bQuestions (WQ) (Berant et al,, 2013), and Curat- Baudis and Sedivy, 2015). We edTREC 8 (TREC) questions from the training set of the the text Igather five text QA datasets aS our fine-tune set. For our dev and test sets, we first collect the questions from the dev set of SQuAD, NQ, TriviaQA, WQ and the test set of TREC, and then randomly split these questions into two subsets as our dev and test sets_ To get the version of the collected ques- spoken tions, we used Amazon Mechanical Turk (MTurk) crowdsourcing platform with anonymous, non expert workers, to collect spoken questions read by human speakers. The collection details are shown in Section B.l in the Appendix\n3.23 SLUE-TED for SUMM\nOf the existing corpora for abstractive speech sum- marization; How-2 has been used in recent work However; raw audio is not (Sharma et al., 2022) publicly available for the entire corpus, and the task of summarization is relatively easy due to shorter videos and simple reference summaries. Therefore, challenging task of generat- we consider the more ing abstracts and titles for TED Talks, whose audio The TEDSummary dataset is publicly available: was introduced by (Kano et al,, 2021) and accom panied by a tool to crawl and download TED talk that may be used to recre- videos from the web ate the TEDSummary cOrpus However; the lack of information about the exact talks used in the reproduce their data corpus makes it difficult to selection. Based on their crawler; and more recent 10 talks released on the TED website we introduce SLUE-TED, a re-designed corpus of summaries for TED Talks spanning the years until 2022.\nFor the documents, to avoid the enormous cost collecting spoken versions of long text docu- of we search for off-the-shelf docu- ments, spoken paired docu- question ments relevant to each as ments from the Spoken Wikipedia dataset (Kohn et al,, 2016), which includes 1.2k spoken Wikipedia articles from about 400 different real speakers. We split the articles in Spoken Wikipedia into about 37k spoken documents with duration of 40 seconds_\nWe find that, on average, nearly 669 of words in the title and 57.4% of words in the abstract are present in the transcript of a given audio, suggest- ing that ASR pre-training can be useful to improve speech summarization performance. For bench- mark evaluation, we randomly split this corpus into 80% finetune, 10% validation, and 10% test set as detailed description of the shown in Table 3. A dataset is available in the Appendix C.2.\nWe adopt a similar procedure with Joshi et al:\nCC BY-SA 4.0 license\nCC BY-SA 3.0 license\nApache License 2.0\n'CC-BY 4.0 license\nhttps:Ilgithub com/ntteslab-sp-admin/TEDSummary\nPublic Domain\nCC BY-NC-ND 4.0 license\nTable 3: SLUE-TED data split\nWhisper (Radford et al,, 2022). Additionally, we quantify the performance gain on WER and SLU tasks achieved by fine-tuning custom ASR models using compared to off-the-shelf ASR models.\nIn all experiments, we use the fine-tune set of the corresponding task to fine-tune pre-trained models, the dev set to pick the best model, and the test set to evaluate both EZE and pipeline baselines. In ad- performance of an \"oracle\" dition, we measure the pipeline system that uses ground-truth transcripts instead of ASR output   Below, use the base we sized model when there are multiple variants of the pre-trained model:\nSLUE-VoxPopuli for NEL 3.2.4\nSLUE-VoxPopuli was published with NER anno- tations in SLUE Shon et al., 2022a). We extend SLUE-VoxPopuli to NEL by adding word-level time stamps in the dev and test sets_ We use the Montreal Forced Aligner (MFA) (McAuliffe using et al,, 2017) to obtain word-level time stamps, public English acoustic model (McAuliffe MFA s and Sonderegger; 2022). MFA is a standard tool that is commonly used by the community to obtain ground-truth forced alignments. We manually ver- ify the MFA produced entity alignments for 188 utterances (20% of the utterances with entity tags) in dev set and conclude that the MFA output prO- ground-truth: We share more details vides a reliable for the data annotation and verification procedure in Appendix D.l. Data statistics for the SLUE NEL data are shown in Table 4 Note that we do publish NEL annotations for the finetune set as not we focus on re-purposing NER models for NEL which we believe is a more realistic use-case; aS is also common for the speech-to-text forced align- ment models, such as MFA, to be trained without ground-truth alignments.\nDAC 4.1\nWe follow similar setup to Baseline models: the sentiment analysis baseline models in SLUE with some differences due to the multi- Phase-1 label nature of DAC. For the EZE baseline, we pre-trained speech model, specifically start with a Baevski et al,, 2020), and add a self- wav?vec2 fully attention pooling layer and two connected lay (including the output layer) , with a Sigmoid out- ers put activation for each of the 18 dialog act classes. Outputs that is higher/lower than threshold of positivelnegative for the corre- 0.5 are classified as sponding class. For the pipeline baselines, we use either the off-the-shelf ASR models or an ASR us - ing DAC data fine-tuned wavZvec2, and fine-tune a DeBERTa (He et al., 2020) model for the text classification.\nResults: Table 5 shows the baseline results, and Figure la shows the relationship between WER variety of score of pipeline models for and Fl ASR models (the ones used in Table 5 and all other NeMo models). We observe a strong correlation between the WER and DAC Macro FL score (Pear- son coorelation coefficient = -0.9). As the off-the- shelf ASR models perform well on conversational speech; fine-tuning the ASR model does not give a large improvement over the best NeMo model.\nTable 4: SLUE-NEL data statistics\nExperiments and results 4\n4.2 QA\nPipeline Approach: The pipeline QA is system composed of an ASR model and a text QA model predicting the start and end words of the answer span on the ASR output transcript:\nIn the SLUE Phase-1 baseline experiments, larger pre-trained models and LM shallow fusion consis- tently gave better performance compared to smaller pre-trained models and without LM shallow fu- sion. Thus, in this paper; analyze how the ASR we pipeline models is corre- word error rate (WER) in lated with SLU task performance, by using multi- ple off-the-shelf open-source ASR models, specif- ically NeMo models (Kuchaiev et al,, 2019} and\nWe fine-tuned DeBERTa with the ground-truth transcripts of the SLUE-SQA-5 fine-tune set to get the text QA model of all pipeline systems. Note that the DeBERTa text QA models in pipeline systems for searching and the text QA models paired used\nDAC task baseline performance on test set Table 5: *the best NeMo model based on DAC FI score is \"stt- en-conformer-transducer-xxlarge\nEZE Approach: We DUAL (Lin et al. used 2022a) as the QA EZE approach (denoted as EZE DUAL). DUAL is composed of a wav?vec2-large model encoding speech  waveforms, k-means model converting wavZvec2 representations into Longformer model taking cluster cluster IDs, IDs as input and predicting the start and end in- training dex of answer spans_ We followed the procedure in the DUAL paper except we used the k-means model of 500 clusters and fine-tuned its Longformer model for 45 epochs on the SLUE- SQA-5 fine-tune set:\ndocuments (please refer to Section B.2) were fine- tuned on different datasets: the former were tuned on the SLUE-SQA-5 fine-tune set while the latter were tuned on the external SQuAD dataset:\nResults: Table 6 shows   the baseline results on the test and verified-test sets, and Figure lb shows the relationship between document WER and frame-FL on the test set of QA pipeline mod- strong correlation (Pearson cor- We observe a els_ relation coefficient--0.89, p-value<O.01) between document WER and frame-FI. Pipeline-oracle sig- nificantly outperforms all the baseline models, and performance gap is larger in the verified-test the suggesting that there is room for improvement: set; Besides, the pipeline-w2v2 does not outperform the pipeline-nemo model, indicating that the fine-\nWhen evaluating pipeline systems on the SLUE SQA-5 dev and test sets, MFA t0 align used we ground-truth transcripts and ASR output transcripts speech: The ground-truth answer words and the to predicted by the text QA model are answer words converted to the time interval of the ground-truth and predicted answer span, which were then used to calculate the frame-Fl score\nby using strong ASR model like Whisper: Fur- pipeline system performs ther; we observe that the slightly better when the text summarization model is fine-tuned on ASR transcripts The pipeline mod- outperform the EZE els on ROUGE and system METEOR, showing that the pipeline model aids in producing more accurate words. However; the end-to-end model does have a higher BERTScore, demonstrating the ability of the EZE model to prO- duce semantically relevant summaries. All the base- line models perform worse than the pipeline-oracle model suggesting room for improvement\ntuned ASR model does not lead to better QA per- formance_\nTable 6: QA task baseline performance. *the best Nemo model based on frame-Fl score is \"stt-en-contextnet- 1024'\nTo analyze the correlation between WER and the performance of the speech summarization task, plot ROUGE-L Figure lc for vari- scores in we pipeline systems and a ground-truth transcript- ouS based text summarization model. We observe strong correlation (Pearson correlation coefficient= 0.9, p-value<O.01) between WER and ROUGE-L scores, suggesting that we can boost SUMM per- using a stronger ASR model: formance\n4.3 SUMM\nPipeline Approach: The oracle pipeline is con- structed by using the ground truth transcript to train a text summarization model, and infer the most likely summary from the truth transcript: ground we use different combinations of speech rec- Then_ ognizers and text summarization models to build different pipeline models for speech summarization. For the pipeline baseline, train ASR models on we the TEDLIUM-3 Hernandez et al., 2018, corpus using the ESPNet Watanabe et al , 2018) toolkit The ASR models consist of a conformer encoder- decoder architecture with pre-trained SSL repre- sentations as features (see Appendix C.1 for more experiment details about our models). We also with state-of-the-art off-the-shelf speech recogniz- including Whisper  (Radford et al,, 2022) and ers, The resulting talk transcripts are NeMo models. often exceeding 2048 tokens, requiring long; very our text summarization models to be able to handle such long input sequences. Therefore, we use the Longformer Encoder-Decoder (LED-large) model (Beltagy et al, 2020), initialized using BART-large model (Lewis et al,, 2019). We investigate training summarisation model on both ground truth our text and ASR transcripts.\nunderstanding of the To facilitate a better perfor- analyze the mance of our EZE SUMM model, we percentage of exact matches in reference summary and predicted summaries for each POS tag: We observe that the majority of summarization errors occur because the model is not able to correctly generate the proper nouns in summary: A similar analysis on the percentage of exact matches for named entities shows that only 6.6% of entities in the reference summary were found in the predicted summary. Based on this analysis, we infer that the speech summarization models struggle to current correctly extract entities for the summary: (Full Ap-T match available in Table 15 in the POS tags pendix)\n4.44 NEL\nBaseline models: For NEL inference, we use the baseline NER models from Shon et al. (2022a) Both the EZE and ASR (within pipeline) models as the backbone and are trained wavZvec2 use with character-level connectionist temporal clas- sification (CTC) (Graves et al,, 2006). The text within pipeline) model uses the DeBERTa as NER the backbone and is trained on ground-truth tran- scripts. Note that no dedicated model is trained for NEL. This is intentional: NER and NEL are related tasks and a realistic use case would require a single model that performs both tasks\nEZE Approach: EZE speech summarization model is trained using the ESPNet (Watanabe et al,, 2018) toolkit by first pre-training for speech recognition task on the TEDLIUM-3 corpus (Her- nandez et al,, 2018) and then fine-tuning 0n OUI SLUE-TED data for speech summarization task as described in Sharma et al., 2022).\nResults: Table 7 shows the performance for all baseline models on the test set (see Appendix C.3 performance). We observe that the per - for dev set formance of the pipeline system can be improved\nA CTC model produces a posterior [nference:\nTable 7: SUMM task baseline performance. The ASR models are trained on the TEDLIUM-3 corpus. *the best NeMo model based on SUMM ROUGE-L score is \"conformer-transducer-xxlarge' For pipeline models, we also experiment with training NLU model on ASR Transcripts (ASR) instead of ground truth transcript\nNEL task baseline performance Table & on test set_ The W2V2-B models are fine-tuned on slue-voxpopuli data *the best nemo model based on NEL frame-fl score on dev is \"stt_en_conformer_ctc_small\"\nFigure 2: Example inference for an EZE NEL model using a CTC recognizer: The transcript is \"the eu funds entity: # and are the start and end labels of an ORG ]\nRTxv probability matrix, \u20ac consisting of the \u20ac posterior of each character in the vocabulary of size V for each of the T frames in the input audio. For ASR, the character vocabulary consists of the English alphabet; word separator token * and a For the EZE model, the vocabulary blank token\" \u20ac\" also includes special characters for the start and end phrase. of an entity We obtain a frame-level char acter sequence output via greedy decoding on \u20ac The time stamps corresponding to \"\" tokens in the output character sequence provide word-level start and end boundaries. As CTC is not trained with explicit alignment signal, the word boundary an tokens may not be a reliable indicator of the true time stamps, and we introduce two hyperparame- possible mis-alignments: ters aS a heuristic fix for offset is a fixed duration by which we shift the time stamp predictions, and incl_blank \u20ac {0,1} denotes whether any trailing tokens are considered a part of the predicted entity segment:\nrecall values (see Table 18, Appendix D.3). We also find that the off-the-shelf NeMo ASR model (pipeline-nemo) outperforms the dataset-specific ASR model (pipeline-w2v2).\nscatter plot of NEL and Figure   Id shows WER scores for a variety f pipeline models. Al- have though models with the lowest WER do the best frame-Fl, the overall correlation is not high. The NeMo models have different training objec- tives and model architectures, and we note that within each model class, the ASR and NEL met- rics are much better correlated (see Figure 12, Ap- pendix D.3) This suggests that model architecture and/or training objective play a significant role in 12 alignment quality:\nDiscussion 5\nAmong the baseline models, our pipeline models generally outperform their end-to-end counterparts However; as shown in prior work (e.g;, (Arora et al. 2022a; Pasad et al,, 2021)), end-to-end models of ten have more room for improvement with careful and creative modeling ideas, and we hope that this new testbed helps spur such research\nIn the pipeline approach, the predicted text from ASR is passed to a text NER model, and the time stamps for detected entities are extracted from the For the EZE model, the time stamps ASR s &. corresponding to the entity start and end special characters are extracted directly from its \u20ac . An Fig; example is presented in 2\nIn addition, the WER sensitivity analysis in Fig- suggests different strategies are needed for the ure\nResults:   Table 8 presents the baseline results_ The pipeline and EZE baselines have fairly simi- lar frame-FI, but these approaches have comple- mentary strengths as seen from their precision and\nMore word-Fl results in Tab. 19 in Appendix D4_\n12The details of hyperparameter tuning and timestamp ex traction from NeMo models are in Appendix D.2_\npipeline system depending on the SLU task For example, fine-tuned ASR (pipeline-w2v2) plays a significant role in the DAC task while the QA task is not; and ASR model architecture is critical for the NEL task while WER is more matter for DAC and SUMM tasks\npotential biases in the SLUE Phase-2 benchmark. Specifically, for SLUE-SQA, implemented an we automatic check using the Google Speech-to-Text service. If the Word Error Rate (WER) exceeded 30%, workers were recommended t0 re-record the We chose a 30% WER threshold to iden- utterance tify and exclude empty o prematurely cut utter- Our analysis showed that such violations ances were less than &% of questions. Additionally, we personally listened to each recording and only dis- carded those where a significant portion of the con- tent was missing: Recordings were accepted even if the WER exceeded 30%, ensuring that our dataset does not include any potential bias inherent in the automated speech-to-text service.\nConclusion 6\nSLUE Phase-2, with four additional SLU tasks and high-quality annotation, enables a more compre- hensive analysis of diverse SLU tasks than previ- ously possible: Besides the task definitions and annotations, this work contributes multiple base- using lines and performance analysis modern off- the-shelf ASR and text models. The baseline per- perfect, and the formance on all tasks is far from relative performance of different models differs indicating that these tasks are ripe for across tasks, additional work and analysis to push the boundary of SLU research:\nThe DAC annotation in SLUE-HVB and verified- test set in SLUE-SQA data were done by ASAPP internal data labeling team. Everyone who partici- pated in the annotation was an employee of ASAPP and conducted the work within the scope of their usual employment. Specifically most of them have year of experience in speech and language- over related data labeling and their education level is degree. above a Master s\nLimitations\nOne limitation of this work is the lack of human performance scores o the new tasks. Although the baseline performance is far from perfect; and it likely that human performance is much quite seems better; this should be measured in future work: An- other limitation is that it is unknown how much each task should benefit from access to the audio in addition to text; this could be measured in principle for humans, but we leave this t0 future work again\nAcknowledgements\nMolly We would like to thank Kyle Hager; and Ruhl for their helpful comments and discussion from a linguistic perspective, and the whole AS- APP MLDL team members for high quality anno- Part of this work used PSC Bridges2 and tation_ NCSA Delta through allocations CIS210014 and IRII20015 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (AC CESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\nBroader Impact and Ethics\nSpoken language understanding benchmarks, like we propose in this work, facilitate the the ones development of technologies that may be particu- larly useful for speakers who are unable to read or write text and ultimately also for unwritten lan- guages, where speech is the only form of commu We hope that this work also spurs more nication. collaboration across the fields of speech and natural language processing, both of which are needed to make progress in this area:\nReferences \n Now the paper ends. \nWhat are some potential limitations and risks of the benchmarks and tasks introduced in this paper?", "output": "Some potential answers:\n\n1. Lack of human performance scores. The paper mentions that baseline performances on the new tasks are not perfect but it is likely that human performance would be much higher. However, human performance scores are not measured, which limits the ability to evaluate how close machine performance is to human-level. \n\n2. Unknown benefit of audio vs. text input. The paper does not measure how much each task benefits from having access to the audio input in addition to the text transcripts. This limits the understanding of how important the audio components are for these spoken language understanding tasks.\n\n3. Potential biases in data. The paper describes some measures taken to reduce biases in the data collection but does not conclusively show that the data is free of biases. Any biases in the data can limit the generalizability of models trained on the data.\n\n4. Risks of misuse. Like any machine learning technology, the benchmarks and tasks can enable the development of systems that could potentially be misused for illegal or unethical purposes if not used responsibly. Appropriate controls and oversight would need to be in place to mitigate these risks."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nclasses in previous tasks. Even if there is no CF; the classifi- We call this problem, cross- cation results will still be poor: task class discrimination (CTCD) Those approaches that regularization-based do not save any previous data; e.g , not deal with CTCD. or orthogonal projection-based, do Replay-based methods implicitly deal with CTCD to some a memory buffer M to extent because such a method uses amount of data (replay data) from old a small tasks save small batch of current task data arrives, the Xnew When system jointly trains Xnew and sampled replay data some Xbuf from M. enables the system to partially learn the Xbuf decision boundaries between the new classes and the old classes because the amount of the saved data is very small_\nExisting continual learning CL) research regards catas- trophic forgetting (CF) as almost the only challenge This challenge in class-incremental paper argues for another learning (CIL), we call cross-task class discrimi- which how to establish decision boundaries nation CTCD), i.e., between the classes of the new task and old tasks with no (or limited) access to the old task data. CTCD is implicitly and partially dealt with by replay-based methods. A replay replay data) from method saves a small amount of data batch of current task data arrives; previous tasks: When the system jointly trains the new data and some sampled re - The replay data enables the system to partially play data: learn the decision boundaries between the new classes and the old classes as the amount of the saved data is small. However; this paper argues that the replay approach also training bias issue which reduces the effec- has a dynamic tiveness of the replay data in solving the CTCD problem: novel optimization objective with a gradient-based adaptive method is proposed to dynamically deal with the problem in the online CL process: Experimental results show that the new method achieves much better results in online CL.\nDue to the limited replay data, the training is biased, ability to solve the CTCD problem: which reduces its To training bias also changes as more make matters worse, the This paper first shows that the problem is tasks are learned. gradient imbalance (GI) on logits, i.e , higher reflected as positive gradients than negative gradients on the logits and vice versa. It further shows that GIis caused by two main is- sues. The first is data imbalance. Since the memory buffer sampled size, the batch size of the new data Xnew and the data Xbuf from the memory buffer are all fixed, if the system many tasks, the average number of samples in has learned each previous class in Xbuf will be much smaller than that of This results in higher positive gradients each class in Xnew negative gradients logits of the previous classes than on the leading to training bias and pOOr decision boundaries (or weak CTCD capability) between the classes of the new and CL training fo- The second is CL imbalance, ie., old tasks_ samples (which are harder to train as cuses more on the new they are new) than the replayed samples (which have been seen and trained many times before). This causes further GI. This imbalance is involved (see Sec. 4.2 for details) \n1. Introduction\nContinual learning (CL) learns sequence of tasks in- This work focuses crementally: on the class incremental learning (CIL) setting [32] in online CL. In CIL, each task consists of a set of unique classes, the sets of classes of any two different tasks are disjoint and the system has no access to the task information in testing: In online CL; the data gradually from a data stream: Whenever the small comes Thus; batch of data arrives, it is trained in one iteration the data for each task is effectively trained in one epoch:\nSome existing works [2,42] have tried to deal with data imbalance in offline CL. For example, SSIL [2] separately calculates the cross-entropy loss of the new data and the replay data to mitigate data imbalance they But are not the gradient angle. Irom The second issue of GI is more complex and has not been attempted before_\nExisting CL papers almost regard catastrophic forgetting (CF) as the only issue for CL. In fact; CIL also has another major challenge. When the system learns task; if no a new data from previous tasks is available, it has no way to es- tablish decision boundaries between classes and old new\nLUCIR [19] uses the cosine normal update the model_ ization to calculate the predicted probability and margin ranking loss to separate ground-truth old classes from new 8] adds an additional fine-tuning stage with a small classes. learning rate and balanced subset of BiC [46] samples. bias correction layer: [53] uses weight alignment to adds correct the biased weights. CCIL [33] applies a loss to bal- learning:   However; many of ance intra-task and inter-task these algorithms [8,53] need the full data of the current task upfront andlor multiple training epochs to to be available address the data imbalance issue, which are not suitable for online CL as online CL does not have the full training data of a task available when the task arrives in the data stream_\nThis   paper method, called GSA novel proposes (Gradient Self-Adaptation), to deal with GI (and CTCD) training objective and in online CL. GSA includes a new gradient-based self-adaptive loss to compensate for the GI The loss is dynamically controlled by two gradient rates which automatically measure and adapt to the dynamic GI situation. The main contributions of this paper are:\n(1) It deals with the CTCD problem in online CL and optimization framework that decomposes proposes new the problem into cross-task classification and within-task classification (see Section 5). In [22], CTCD is called inter class   separation, task but it out-of-distribution uses an based approach to dealing with the problem in offline CL The paper uses a replay-based approach for online CL. (2) It analyzes the CTCD problem from the gradient im- balance (GI) angle and finds two kinds of gradient imbal- ance (data imbalance and CL imbalance) (see Section 4_ gradient-based self- on the analysis, it proposes Based adaptive loss to compensate for the GI: (3) Experiments in both the disjoint and long-tail online CL settings show that GSA outperforms strong baselines by large margin (see Section\nGradient-based CL methods: GEM [29] and A-GEM [10] rotate the current gradient when the between the angle gradient and the gradient computed on the refer- current MEGA [17] uses the loss of Xnew ence memory is obtuse adjust the relative importance of learning and Xbuf to new task and maintaining the past knowledge. OWM [51] OGD [14], and AOP [15] avoid CF by projecting the gra- subspace of old dients on the new task onto an orthogonal gradient imbalance These methods do not deal with tasks_ but represent a different family of CL approaches_\n2. Related Work\n3. Preliminary\nThere are many existing CL techniques. Regularization- based approaches penalize changes to important param eters of old tasks [23, 50, 52]. Replay-based approaches some past data and replay them in task train- save new ing [8, 12, 13, 24,37,44,47, 54]. Generative replay builds to generate pseudo old data for replay data   generators ing 18, 21,34,38,41,45]. Dynamic architectures based approaches [1, 14, 20, 28, 36,39,43,49] overcomes CF by expanding O isolating parameters. Data augmentation has also been used to learn better features for CL recently [55].\nFollowing [3], Problem   description: we learn se setting: quence of tasks, 1, 2, in the online CIL t, In online CL, each incrementally arriving data batch is seen So the system effectively learns only once by the system. epoch: For a task t, we denote its dataset as each task in one ()}\"l where nt is the number of training sam- {(w} Yk: denotes the set of classes of task t. L(t) ples in task t.\nModel architecture and training setting: Our model F consists of a feature extractor he with the parameter set 0 classifier fv with the parameter set & and It uses a replay memory buffer M. method with Whenever a small batch of new data Xnew from the data stream is accumulated, it is trained jointly with a small batch of data Xbuf sampled from M to update the model in one training iteration. The model produces the logits F(x; 0 ,0) = fo(ho (x) ) , which are used to calculate the loss or to predict in testing: Reservoir sam- pling is used for memory update.\nExisting online CL Online CL and replay methods: ER randomly use the replay approach: methods  mainly samples the buffer data MIR chooses buffer data [11]: whose loss increases the most [3]. Shapley value theory is applied in ASER for memory updatelretrieval [40]. Knowl- edge distillation is employed in DER++ [6]. NCCL deals with CF by calibrating the network 48]. Contrastive learn ing is used in SCR [30]. GDumb samples and stores class balanced data in memory [35]. However; it does not deal with CL imbalance identified in this paper: GSS diversi- fies the gradients of the samples the buffer [4]. ER in AML [7] avoids CF by shielding the learned representations from drastic adaptation to accommodate new classes. OCM is based on mutual information 16]: Our work identifies training two gradient imbalance issues and designed a new strategy and a novel loss function to deal with them\nGradient imbalance (GI) on logits. We now introduce the proposed gradient-based analysis on logits and the con- cept of GI In CL; each task is usually learned by minimiz- ing the softmax cross-entropy loss, Lce\nData imbalance in CL: Several researchers have dealt with data imbalance in offline CL. SSIL [2] isolates the computation of softmax on  previous and classes to new\n4.1. Gradient Imbalance due to Data Imbalance\nfor input %_ Given training sample % of class Ci, the gradi- ents on logits (cj = C are given by\nIn a replay-based method, the memory buffer size is USU - ally fixed and small and the batch size Nbuf for the buffer batch and the batch size Nnew for the new data batch Xbuf As the number of previous classes Xnew are also fixed. grows with more tasks learned, the number of sampled data Nbuf for each previous class gets   smaller; Xbuf in UFETTLG)[ neh but the number of samples for each class in new TLG Xnew of the new task remains unchanged. Then we have buf N U,4077 and the samples from previous classes uiew TzG new data classes can become highly imbalanced. and the positive gradients for the previous classes can surpass The negative gradients (in absolute values) We can empir- their ically verify this by calculating the positive-negative (PN) gradient rate with t' t:\nthat logit Oci From Eq: 2 x gives its we see true negative gradient and the other logits positive 0cj gradi- gradient update rule for parameter ents As the w iS learning rate, the negative lr * Vw, where lr is the W W gradient (pCi 1) results in an increase in Oci for the true class Ci and the positive gradient (pC; results in decrease Thus, the negative gradient Ocj in for each wrong class Cj- encourages the model to output a larger probability for the true class, and positive gradients help output lower proba- bilities for the wrong classes_\nHowever; in CL; as the model has no access t0 the train- ing data of previous tasks when it learns task in- new crementally, all gradients o previous classes are positive during the new task training (there are no negative gradi- ents) (imbalance of positive and negative gradients). Then the model tends to output smaller probabilities on previous classes, biasing the classification towards the new classes.\nEmpirical Verification: We introduce the setup first_\nDatasets and Tasks: We conduct our experiments using two dataset settings: (1) Split CIFARIOO, where we divide the CIFARIOO dataset into 10 tasks with 10 unique classes per task, and (2) Split TinyImagenet, where we divide the TinyImageNet dataset into 20 tasks with 10 different classes per task: For online CL; we run each task in one epoch:\n4. Gradient Imbalance (GI) in Replaying\nreplay-based method actually makes the (positive and negative) gradients more balanced There are adjustments of both ups and downs for logits for previous classes How We ever; this is insufficient. discuss tWo reasons: data im- balance and CL imbalance We propose some metrics first.\nModel, Optimizer and Batch Size: We use the full size ResNet-18 to perform our experiments. To give a more gen- optimizers eral analysis, we run with both SGD and Adam learning rate, we follow [3] and set it as 0.1 for the SGD For optimizer: To ensure a good performance, we search and set the learning rate as 0.001 for the Adam optimizer: For batch size, we also follow [3] and set it to 10 for Xnew and 10 for which is randomly sampled from the the buffer batch Xbuf memory buffer: Since ER (Experience Replay) is a basic using experiments replay method we conduct our ER 1113\nlearning the new task Metrics: When the model is (including the current for a seen class Ci in any seen task task t), we define the mean of positive gradients (pk that the logits of all other classes than ci have and the mean of negative gradients (pk logit has over all 1) that class Ci training samples (including both the data samples seen nl;r So far from the current task t and the corresponding replay data used) for this task as\nWe show the PN results of the classes of a few tasks Fig: subsequent tasks are of the two datasets in more as following observations from Fig: learned. We can make the See plots based on SGD in Appendix 1 (see footnote 2). 1\nSub-figures (A show that when (1) and (B in Fig: the model is trained with the new task t, the PN rates for the classes of old tasks get smaller than ~1 and tend t0 de- This indicates that positive gradients surpass nega- crease. tive gradients (in absolute values) for the old classes The reason is that the new data classes dominate the training, positive gradients to classes of and the new data give high the old tasks to ensure the new task data are not classified As more tasks are learned, the data get to those old classes resulting in lower negative gradients even more imbalanced, on the old classes from the replay data of the old task t' and thus the downward trend of the curves in (A) and (B).\nwhere yk: is the label of the kth sample of task t and I is the indicator function\nAt the task level for classes in any task t' (t' < t), we define the mean of positive and negative gradients that the logits of the class set L(t' ) receives in training task t respec- tively as\n(2). Sub-figure in Fig: 1 shows that the tendency\nFigure 1_ PN rate (rate in the figures) of the CIFARIOO or TinyImageNet experiments with Adam optimizers_ The buffer size is 1000 We choose four different tasks in each experiment and plot their PN rates as subsequent tasks are learned. In (C) we report their accuracy: The plots based on the SGD optimizer are given in Appendix 1, which show the same trend.\nof the test accuracy performance of the previous classes is similar t0 the tendency of their PN rates_\ndataset as single task (the blue curve and its moving av Fig: erage yellow curve in We also observe bal 2 (A)): anced gradients from detailed results (not shown here) for all classes when we learn the first task (no replay data)\nDealing with Data Imbalance. We propose a sampling strategy that guarantees the same number of training sam- ples for each class in each training iteration Specifically; we fix Nnew + Nbuf where Nnew and 20, are the Nbuf number of samples in Xnew and Xbuf , respectively: We use E;z1 ILO)| L() the ratio between and to decide the num ber of samples from Xnew (the rest are not used here), i.e_ Ii(t) 2ETE7), 1) where int(:) returns Nnew (int(20 max integer of a given number and Nbuf 20 _ Nnew the nearest We use Nbuf to sample previous tasks data in the memory In this way, the number of training data for every buffer: class is approximately equal:\n(2). We ignore the accumulative part in Eq: 6 for the time being gradient imbalance in the classes of and focus on the each task alone. Based on the detailed results (not given here) , we observe that as more tasks are learned, the GI for being learned the classes of the current task worse and gets why worse. This explains the classes of the last task in sub and (B) have the highest GI: Note, the last task figures (A Let us explain why: has no accumulation as t' =t\nAs example, we consider class Ci from the current an As more and more tasks are learned, the replay data task t. from previous tasks in Xmix will contribute less and less positive gradients to Ci because the replay data may have been trained many times in the past and are well overfitted to their own classes in the past and their probability of being they classified to the new class Ci is very small and hence contribute very low positive gradients t0 C. In OUT experi ments, we observed this and also decreased negative gradi ent On Ci. We plot the average cross-entropy loss of the new data batch and the replay data batch when the model learns the second task of CIFARIOO in Appendix 2. The figure shows that the average croSs-entropy loss of the new data batch is much higher than that of the replay data batch in training process. This is because the system has the whole learned good features from previous tasks that makes the negative learning of the new class Ci easier resulting in less gradients. But positive gradients drop more significantly:\n4.2. Gradient Imbalance due to CL Imbalance\nGI still occurs even after the number of samples used in each class is balanced using the technique above Let us call the balanced data for training Xmix , which includes both the replay data from previous tasks and new data from the We now introduce the second cause of GI, CL current task. training in CL. To imbalance, which is due t0 incremental explore CL imbalance, we define the rate of accumulated positive and negative gradients (A-PN rate) for a class Ci L(t from task t to the current task t (t/ t),\nA-PN(t' , t, Ci) gives the accumulated gradient rate of each class over the learning process to the current task t_\nEmpirical Verification: We plot the A-PN rate of each class in Fig: 2(A) for the CIFARIOO dataset and in Fig: 2(B) for the TinylmageNet dataset after the last task is learned. We make the following observations from Fig: 2 (A), (B), (C) and some more details in the experiments:\nWe now consider the accumulation part the summations in Eq: The next section will show that A-PN is used 6). to dynamically adjust the loss for each class in learning to balance the gradients. By right; when we learn the current we only need to consider the current GI situation to task However, considering only oe case (the adjust the loss: current situation) is risky due t0 random fluctuations. That\nThe A-NP rate accumulated is close to - 1 or bal- (1). jointly train all classes in the CIFARIOO anced when we\nFigure 2. A-PN rate (rate) for each class (T-axis) after the last task is trained The classes are ordered by their sequence of appearances in The buffer size is 1000 and the optimizer is Adam (C) shows the test performances (see the formula in the legion) of all the original data The results based on the SGD optimizer are given in Appendix 3. learned classes_\nis why we consider the impact on class Ci from all subse- gradient rate for quent tasks, which gives us a more robust adjusting the loss in dealing with GI in learning:\nThe second part distinguishes Cn and the previous classes The negative interference from L cn` with respect to new classes to previous classes is limited in the second part: For Ldecom the relationship of loss and loss Lce, we have the following proposition:\n(3). Fig: 2(C) shows that higher A-PN rates (imbalanced gradients) result in lower test accuracy: The fact that the ac- cumulated negative gradient being greater than the accumu lated positive gradients in absolute values (i.e-, A-PN -1) makes the model biased towards the new classes_\nProposition 1. For from the new class Cn sample Tcn the following holds Xnew in\n53 The Proposed GSA Method\nThe is in Appendix 4. Similarly, for a sampled data proof point n) from the buffer data of class cj(j Tej Xbuf we define its new loss as:\nOur method GSA consists of two parts: (1) a new op timization framework that separately optimizes cross-task classification and within-task classification. (2) A gradient- based self-adaptive loss to alleviate the gradient imbalance (CL imbalance , in our framework.\nput the previous classes (similar gradient In this loss, we rates) in the first term t0 maintain the established decision boundaries between previous classes and put the logit of the true class and the logits of the classes in the current task in the second term t0 establish the decision boundaries be- negative transfer tween class Cj and the new classes The irom the new classes t0 previous classes is limited to the (Tcj is the upper bound second term_ We prove that Ldecom in Appendix 4. Lcel (Tci of\n5.1. Optimizing Within-class Classification and Cross-task Classification\nprevious classes Assume that the system has seen n (C1, Cn-1) and there m (Cn; n + 1 new classes Cm) class, say Cn, in the batch in the current batch Xnew for decompose its learning into two parts: (1) of new data, we learning the decision boundaries between Cn and the other classes in the current batch Xnew and (2) learning the deci- sion boundaries between Cn and all previous n 1 classes of old tasks in Xbuf. of new For example, for sample Ten original cross-entropy loss is: class Cn from Xnew , its\nConsidering data balance. To estimate the first term in Eq: 8 for class we use Xnew as the data of new classes Cn; as Xnew is usually class-balanced: For a similar reason, we use Xbuf to estimate the first term in Eq: 10 for the previous class Cj. However; for the second term in Eq: 8, using X new prediction bias towards new classes as Xnew lacks causes Also, using Xbuf to estimate samples of previous classes. the second term in Eq: 10 is not appropriate as the number of new classes samples in the buffer increases gradually and long for a time, the new classes are the minor classes in the To solve both problems, we use the sampling strat- buffer: egy proposed in the last section under \"Dealing with Data\nwhere is the logit value of for class Cs- We decom Ocs T cn Ldecom' pose the loss (denoted by into two parts: ~ cn\nLdeeom The first part  of distinguishes Cn and other new Their gradient rates are similar: classes with respect t0 \u20ac cn\n'If this is not the case, we can sample some data of the same class from the memory buffer or directly duplicate the samples to guarantee it:\nImbalance. Then we obtain a mixed set of new data and old data sampled from the memory buffer; called Xmix. Xmix regarded as samples from the uniform distribu- joint . can be tion for all classes, including both the new and old classes_ Then our final optimization is: goal\n(3). A-PN and PN rates are class-based metrics that re- flect the discriminative  power of the model for all seen Our loss then gives class-based weights for sam- classes_ ples from different classes The new loss thus automatically adjust the loss weight Wyk to balance the accumulated nega- tive gradient and the accumulated positive gradient for each class_ We justify the new loss in Appendix 5\nConsidering the gradient imbalance, we replace the second LGSA-( term and the third term in Eq: 11 with our CE losses EEq: 14 and Eq: 12 in the expectation form respectively: The final optimization objective in Eq: 11 is turned into:\n5.2. Self-Adaptive Loss for CL Imbalance\nBased on the analysis in Section 4 we know that even though the data balance is guaranteed, GI still exists as gradient rate is different for different classes, the which is caused by CL imbalance: We note that CL imbalance (measured by the gradient rate of each class) not oly OC- and previous curs   between classes but  also new occurs among previous classes. To mitigate it, we propose GSA- CE (Gradient-based Self-Adaptive CE) loss LGSA-CE: The loss function is dynamically adjusted based 0n the gradient rates, which change as more tasks and batches are learned. N\"I be the samples in their LGSA-CE is Xmix_ Let Tk, Yk\n6. Experiments\nEvaluation data. Four image classification datasets are 1) MNIST [27] has 10 classes with 60,000/10,000 used. training/test examples. We 5 disjoint tasks created with 2) CIFARIO [25] has 10 classes with 2 classes per task: 50,000/10,000 training/test We created 5 disjoint samples. 3) CIFARIOO [25] has 100 tasks with 2 classes per task: classes with 50,000/10,000 training/test 10 dis- samples. joint tasks are created with 10 classes per task: TinyIm- 4 ageNet [26] has 200 classes. We created 100 disjoint tasks Each class has 500 training exam- with 2 classes per task: ples and 50 test examples.\nGSA? is compared with Compared Baselines: 16 re- 9 online CL baselines, AGEM, ER, MIR, cent baselines: GSS, ASER, RainBow-rt, ER-AML, GDumb, and SCR offline CL baselines, DER++, ILZA, Co?L, LUCIR, and they CCIL; BiC and SSIL; as deal with datalclass imbal- ance and can be run in the online CL mode, i.e-, training in epoch without requiring the full data of each task to be one available when the task arrives. The citations of these SyS- given in Table 1 associated with their results. Note tems are that Rainbow [S] is not a standard online CL method as it re- replay-data with data augmentation for 256 epochs trains all after each task; which is not suitable for online CL because\nwhich where is the task in class When t' appears Cs training task t, the model is we incrementally update the by adding the new gradients A-PN(t' , t,Yk: and PNt, Yk into the current sums of all previous gradients. LGSA-CE has three advantages:\n(1). When the new task arrives, we do not need t0 assume we have the entire training set for calculating some that statistics or an exemplar set like CCIL [33]. Our method is thus suitable for online CL.\n(2). The model sets the values of Wyk and based on Ucs automatically as they - A-PN rates and PN rates vary dynam ically with the current gradient imbalance situation:\nCode and Appendix: https Ilgithubcom/gydpku/GSA\nTable 1. Accuracy on the four experiment datasets with different memory buffer sizes M_ All values are averages of 15 runs_\nTable 2. Average forgetting rate. All numbers are the averages of 15 runs_\naverage of 5 runs. Memory size M is Table 3. Ablation accuracy Ik for CIFARI00 and 2k for TinyImageNet_\nall baselines.   Denoting Cnum aS the number of all classes, employ a linear layer of size [dimh, Cnum] as the classi- we fier fo For an input \u20ac, we use F(x) to compute Lce:\nData Augmentation: To learn better features, we ap ply two data augmentations each image in Xnew and to random-resized-crop and random-gray-scale. mix _ For X fair comparison, the same data augmentations are to applied and Xbuf) to improve their performance, all baselines ( X new which results in an average of about 2% of improvement for all methods with no drop in performance for any baseline.\nany-time inference. it cannot be used for We thus removed this retraining operation and denote it as Rainbow-rt\n6.1. Architecture, Data Augmentation; Training De- tails and Evaluation Protocol\nTraining and hyperparameter settings   Like ER and many other online CL systems, GSA uses reservoir sam- pling for memory update. We follow [16] and train GSA We set the learning rate as 0.001 with the Adam optimizer: and fix the weight decay as 0.0001 for all settings. We fol- 16] and set the batch size of Xnew as 10 for all methods low and Xmix as 64 for our GSA and base- also as 64 for Xbuf lines. We list other hyper-parameters in Appendix 6.\nArchitecture_ For MNIST; GSA and baselines employ a fully-connected network with two hidden layers as the fea- ture extractor he, each comprising of 400 ReLU units. A linear layer of size [400, 10] is used as the classifier fo. For CIFAR1O, CIFARIOO, and TinyImageNet; we follow [6] and use ResNetl8 (not pre-trained) as the feature extractor ho with around 11 million trainable parameters for GSA and\nWe use the official codes of baselines. Their source links\nand default hyper-parameters are listed in Appendix 6. We run all methods with one epoch for each task:\nEvaluation protocol: We first learn all tasks from the data stream for each dataset, and then test the final model using the test data of all tasks We report the average accu - racy of 15 random runs_\n6.2. Results Analysis and Ablation Experiments\nThe results in Table show that the best online CL base- line is SCR and the best class-imbalance baseline is BiC_ Although BiC and SSIL were not originally designed for they perform well for online CL; better than all online CL; online CL baselines, which do not deal with data imbalance_ This indicates that the data-balanced approaches in BiC and SSIL help. Our method GSA consistently outperforms them by very large margins as we also deal with the proposed CL imbalance. Note that OCM is not compared in the tables competitor of GSA GSA is actually comple- as it is not mentary to OCM. For example, with the largest memory size for each dataset, OCM+GSA gives 96.5% on MNIST, 77.5% on CIFARIO, 53.7% on CIFARIOO, and 35.7% 0n TinylmageNet and also outperforms OCM 161.\nFigure 3_ The gradient rate after the model has learned all classes gradient rate. of CIFARIOO. The memory buffer size A-PN is the is 1000. Joint training means the model learns all classes together as one task in one epoch:\nloss_ Table 3 shows that all these incomplete GSA systems are poorer than the full GSA in Table 1. The poorer per- formance of \"no balanced-sampling\" is mainly due to data imbalance between new classes and previous classes in the buffer: Specifically, in the initial training process of the new task; the stored training data of the new task classes are uSu- ally fewer than that of the previous classes in the buffer and the cross-task class boundaries are not established well. The poorer performance of \"no CL imbalance\" is because the gradient rates across dif- method does not consider different results in poorer performances ferent classes, which again\nForgetting rate: Table shows that our GSA has 2 substantially lower forgetting rates than baselines except GDumb, Rainbow-rt and SCR datasets) , (in but two GDumb, Rainbow-rt, and SCR 's accuracy values are sub- stantially lower than that of GSA (see Table 1) The calcu- lation method of the average forgetting rate and Training times are given in Appendix 7_\nFig; GSA-CE loss mitigates the gradient imbalance: 3 shows that the GSA-CE loss clearly reduces the A-PN gra- dient rate imbalance than the method without it by dropping the A-NP rate that is bigger than -1 and improving the A NP rate that is smaller than -1. And it makes the curve of A-PN rate more similar t0 the joint training curve_\nAblation: We conducted ablation experiments on GSA using two datasets, CIFARIOO and TinyImageNet   The re- sults are given in Table 3_\n1). Ablation study of training loss in GSA In the exper- ~no 'no previous loss\" iments new loss and \"no cross (establishing the loss we do not consider the first term boundaries between new classes), the second term (main- Itaining the decision boundaries between previous classes) and the last term (establishing the cross-task boundaries) in Eq: 15 respectively   Table 3 shows that their performances are all poorer than GSA (Table 1) This is because in the first forgets the knowledge of previous classes case, the model which causes a drastic drop in performance. The second ex periment shows that our method benefits from considering The third experiment shows that all the new data in Xnew establishing cross-task boundaries is an important problem for CIL and can improve the overall performance further:\nLong-tail online CL experiments See  Appendix 8 we will show that Our method also outperforms the where baselines_\n7. Conclusion\nThis paper discussed the challenge of cross-task class discrimination (CTCD) and showed that the  replay  ap proach partially deals with the problem. However; the re major bias in training, which is mani play approach has fested by gradient imbalance on the logits and significantly limits the online CL performance and the ability to solve The paper then analyzed gradient im- the CTCD problem: perspectives: data imbalance introduced from balance two After that_ by replay and CL imbalance due to CL itself: learning strategy and a new self-adaptive it proposed a new loss function to deal with the problems. Empirical evalua- tion demonstrated that the new approach GSA improves the baselines by large margins.\n(2). Ablation study of data imbalance and CL imbalance _ experiments \"no balanced sampling;' In the we replace the sampling strategy for X with random sampling from the Mu memory buffer Xbuf . In the experiment no CL imbalance;' proposed in Eq: 12 and LGSA-CE we do not use the losses Eq: 14 and introduced into the final optimization objective but replace them with the simple cross-entropy in Eq. 15,\nReferences \n Now the paper ends. \nHow could the sampling strategy and training loss techniques proposed in this paper be extended and applied to broader deep learning tasks beyond continual learning?", "output": "\n\nPotential answer:\n\nThe sampling strategy and training loss techniques proposed in this paper aim to address the issue of data and gradient imbalance that arises in continual learning. These same techniques could potentially be applied to other deep learning tasks that suffer from class imbalance and long-tailed data distributions.  \n\nFor the sampling strategy, oversampling minority classes and undersampling majority classes in an balanced way at each training iteration could help address data imbalance in tasks like object detection, segmentation and text classification. This could potentially improve model performance on minority and tail classes.\n\nThe gradient self-adaptive loss function could also be extended beyond continual learning by dynamically adjusting the loss weights for each class based on their positive and negative gradient rates. This could compensate for gradient imbalance caused by class imbalance and long-tailed distributions in the data. Dynamically tuning the loss weights during training could help make the model more robust to rarer and underrepresented classes.\n\nIn summary, the key ideas of balanced sampling and gradient-aware adaptive losses show promise for improving deep learning models trained on imbalanced and long-tailed data in tasks beyond just continual learning. With appropriate extensions and modifications, these techniques could help address the challenges of data and gradient imbalance that appear in many real-world deep learning applications."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\ndeep Interpreting and explaining the behavior of neural Explainable AI pro- networks is critical for many tasks: way to address this challenge, mostly by provid- vides per-pixel relevance to the decision. Yet, interpreting ing such explanations may require expert knowledge. Some re - cent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some This paper proposes Bot- concepts and model decisions: tleneck Concept Learner (BotCL), which represents an im- age solely by the presencelabsence of concepts learned through training over the target task without explicit super- It uses self-supervision and tai- vision over the concepts: lored regularizers s0 that learned concepts can be human- Using understandable. image classification tasks as some our testbed, we demonstrate BotCL' $ potential to rebuild neural networks for better interpretability\nExamples of concepts discovered by BotCL in Ima- Figure geNet [10] and concepts in the input image. BotCL automatically discovers a set of concepts optimized for the target task and repre- sents an image solely with the presencelabsence of concepts_\nrelationships between the image and decision mediated by limited number of concepts. That is, the decision is ex- plained by giving a set of concepts found in the image. The straightforward of the decision is thus interpretation once the interpretation of each concept is established:\n1. Introduction\nUnderstanding the of deep neural behavior networks major challenge in the explainable AI (XAI) (DNNs) is community, especially for medical applications [19,38], for identifying biases in DNNs [2,18,42], etc. Tremendous re- search efforts have been devoted to the post-hoc paradigm posteriori explanation [29,33]. for This paradigm pro- duces a relevance map to spot regions in the input image that interact with the model's decision. Yet the relevance map only tells low-level (Or per-pixel) relationships and does not explicitly convey any semantics behind the decision Inter- pretation of relevance maps may require expert knowledge:\nSome works use concepts for the post-hoc paradigm for better interpretation of the decision [14,50], while the link between the decision and concepts in the image is not ob- vious. The concept bottleneck structure [23] uses the pres encelabsence of concepts as image representation (referred The classifier has access only to to aS concept activation)_ the concept activation, so the decision is strongly tied to the This bottleneck structure has become the main- concepts. stream of the concept-based framework [5,20,28,31].\nA major difficulty in this framework is designing a of set promising approach is concepts that suits the target task: A handcrafting them [4,21,48], which inherently offers bet- ter interpretability at the cost of extra annotations 0n the Recent attempts automatically discover concepts concepts. not always be consis- [1,13,14,46]. Such concepts may 47] tent with how humans (or models) see the world [25, require some effort t0 interpret them; but concept and may significant advantage. discovery without supervision is a\nThe concept-based framework [22,37,50] is inspired by the human capacity to learn concept by (subcon- new sciously) finding finer-grained concepts and reuse them in recognition [24]. Instead of giv- different ways for better ing per-pixel relevance, this framework offers higher-level\nCorresponding author:\nCode is avaliable at https:Ilgithubcom/wbwS2O/BotCL and a simple demo is available at https://botcl liangzhili.com/.\nInspired by these works, we propose bottleneck concept learner (BotCL) for simultaneously discovering concepts learning the classifier: BotCL optimizes concepts for and the given target image classification task without supervi- An image is represented solely by sion for the concepts_ the existence of concepts and is classified using them: We 27] adopt a slot attention-based mechanism [26, to spot the region in which each concept is found This gives an extra signal for interpreting the decision since one can easily see what each learned concept represents by collectively show- ing training images with the detected concepts Figure shows examples from ImageNet [10]. BotCL discovers predefined number of concepts in the dataset, which are ex image emplified by several images with attention maps. An Shark is represented by the right part of of Great White fins mouth (Cpt single 1) and BotCL uses (Cpt . 3). classifier; which is simple fully-connected (FC) layer as but enough to encode the co-occurrence of each concept and each class_\nity as they suit human perception; however; the annotation Moreover; cost is non-negligible. such handcrafted con- cepts may not always be useful for DNNs [47]. These prob- lems have motivated automatic concept discovery: Super- finding low-level semantics, and pixels are a handy unit for concepts are defined by clustering them [13,14,30]. An- other interesting approach is designing a set of concepts to be sufficient statistics of original DNN features [46]. These methods are designed purely for interpretation, and concept discovery is made aside from training on the target task:\nZing concept bottleneck  structure   allows optimiz The ProtoPNet [8] adopts set of concepts for the target task: and identifies   concepts this based the dis - structure on tance between features and concepts.  SENN [1] uses self- supervision by reconstruction loss for concept discovery.\nSENN inspired us to use self-supervision, but instead of adopt reconstruction loss, we contrastive loss tailored_ For image classification task, this contrastive loss is a natural essential for concept discovery:\nContribution. For better concept discovery, we propose self-supervision over concepts, inspired by the re- to use cent success in representation learning [9,16]. Our ablation study demonstrates that self-supervision by contrastive loss key: We also try several constraints on concepts them- is the selves, i.e , individual consistency to make concept more selective and mutual distinctiveness for better coverage of various visual elements. These additional constraints regu training process and help the model learn concepts lar the of higher quality:\n3. Model\n{(xi, yi)li N}, where Given dataset D 1,2, = Ei is an image and Yi is the target class label in the set $2 associated with \u20aci. BotCL learns a set of k concepts while learning the original classification task. Figure Za shows an overview of BotCL's training scheme, consisting of a con- cept extractor; regularizers, and classifier; as well as self- supervision (contrastive and reconstruction losses\nimage \u20ac, we extract feature map F = @(x) \u20ac For a new Raxhx using a backbone convolutional neural network $ F is then fed into the concept extractor gC , where C is a ma- trix, each of whose K-th column vector Cr is a concept pro The concept extractor produces con- totype to be learned: cept bottleneck activations t \u20ac [0,1]k , indicating the pres ence of each concept; as well as concept features V Rdxk \u20ac from regions where each concept exists. The concept acti- input to the classifier to compute vations in t are used as \u20ac [0, 1]/0  . We use self-supervision and regular- score taking training, izers for input to constrain the t and V as concept prototypes_\n2. Related Works\n2.1. Explainable AI\nXAI focuses on uncovering black-box deep neural net- A major approach works [3,6,12,32,35,36,41,43,46]. is generating relevance map that important regions spots for the model's decision_ Various methods have been de- signed for specific architectures, e.g-, CAM [49], and Grad- CAM [33] for convolutional neural networks; [7] for Trans- formers [40]. However; the interpretation of the relevance maps may not always be obvious, which spurs different ap proaches [34,45], including context-based ones.\n3.1. Concept Extractor\nConcept slot attention [26, 27]-based extractor uses mechanism to discover visual concepts in D_ We first add position embedding P to feature map F to retain the spatial = F+P. The spatial dimension of Fr information, i.e., F' is flattened, so its shape is X d, where / hw\n2.2. Concept-based framework for interpretability\nstraightforward way to define a set of concepts for target task is to utilize human knowledge [22,48]. Such con - cepts allow quantifying their importance for a decision [21]. corpus of concepts [4,39] is beneficial for delving large into hidden semantics in DNNs [50]. These methods are of the post-hoc XAI paradigm, but handcrafted set of con- cepts can also be used as additional supervision for models with the concept bottleneck structure [15,22,31].\nThe slot-attention computes attention over the di- spatial e Rd Let Q(ck, mension for concept k from Cr and F' and K (F' ) Rdxi denote nonlinear transformations for Cr \u20ac respectively, given as multi-layer perceptrons with and F three FC layers and a ReLU nonlinearity between them: At- \u20ac [0,1]l is given using a normalization function tention @r\nHandcrafting a set of concepts offers better interpretabil-\nFigure 2. The model pipeline. (b) Self-supervision and regularizers.\nReconstruction loss: SENN [1] uses an autoencoder-like learning better  representation. structure for We assume this structure works well when visual elements are strongly tied with the position? since even discrete concepts should have sufficient information to reconstruct the original im- age. Based on this assumption, design a reconstruction we loss for self-supervision: As shown in Figure 2b, decoder D only takes t as input and reconstructs the original image. We define our reconstruction loss as\nThis attention indicates where concept h presents in the image as shown in Figure 1. If concept k is absent, corre- sponding entries of Gr are all close to 0 We summarize the presence of each concept into concept activation tr by re- Iducing  the spatial dimension of ar as tk tanh C @m m where @rm is the m-th element of ar\n3.2. Feature Aggregation\nFor training, we also aggregate features in F correspond- ing to concept h into concept feature Vr by\ncomposition of natural images is Contrastive loss: The arbitrary, so information in t should be insufficient rather to reconstruct the original image. we thus design a simple borrowing the idea from the recent loss for an alternative, learning for self-supervision [9,16]. success of contrastive\nwhich gives the average of image features over the spatial dimension weighted by attention:\n3.3. Classifier\nWe leverage the image-level labels of the target classifi- lk, where lk is the k-dimensional cation task Let t 2t = vector with all elements being 1. (t,+') of concept If a pair activations belong to the same class (i.e  y for y and y y' corresponding t0 t and \u20ac' ), they should be similar t0 each other since similar set of concepts should be in the cor- responding images, and otherwise dissimilar: The number [S2/ of classes can be smaller than the number |1| of images in a mini-batch so that a mini-batch can have multiple im- Therefore, we use sigmoid instead ages of the same class_ of softmax, leading to\nlayer single FC We use a without a bias term as the clas - sifier; and concept activation t (t1, tk)T is the only input; serving as the concept bottleneck [22]. Formally, let- W be a learnable matrix, prediction y RlQ| is given ting by\nbe roughly interpreted as learning the This classifier can correlation between the class and concepts. Let Ww be the raw vector of W corresponding to class W 92, and is Wwh A positive value of its K-th element: means that con- Wuh cept k co-occurs with class W in the dataset, so its presence image positively supports class W_ Meanwhile, a in a new negative value means the concept rarely co-occurs_\nweight to mitigate the class imbalance where & is the prob- lem (see supp. material) and\n4. Training\n4.1. Self-supervision for Concept Discovery\nThe absence of concept labels motivates us to incorpo- rate self-supervision for concept discovery. We employ two losses for different types of target tasks\n~For example, images of \"7 in MNIST almost always have the acute angle in the top-right part:\n5. Results\n4.2. Concept Regularizers\nWe also employ concept regularizers to facilitate train- They constrain concept prototypes ing: Cn} through {vr _\n5.1. Experimental Settings\nWe evaluate BotCL on MNIST [11], CUB2OO [44], and ImageNet [10]. For evaluating discovered concepts, we re- shape generated a synthetic dataset (Synthetic) [46].\nIndividual  consistency: For better interpretability, each large variations learned concept should not have That is, the concept features Ur and vk of different images should be similar t0 each other if tr is close to 1_ Let Hr denote the set of all concept features of different images in a mini- batch, whose activation is larger than the empirical thresh- old \u20ac, which is dynamically calculated as the mean of tk in a mini-batch: Using the cosine similarity sim(  .) , we define the consistency loss as:\nFor MNIST; we applied the same networks as [1 ] for the backbone and the concept decoder: For CUB2OO same data we used pre-trained ResNet split as [22]) and ImageNet; layer [17] as the backbone with convolutional to X 1 reduce the channel number (512 for ResNet-18 and 2048 for ResNet-101) to 128. We chose a concept number k 20 for MNIST and k 50 for the other natural image datasets_ To setting generate Synthetic, we followed the of [46], where were generated for training and 2,000 for 18,000 images We used k evaluation_ 15 with ResNet-18 backbone.\nImages were resized to 256 X 256 and cropped to 224 X 224 (images in Synthetic were directly resized to 224x224). Only random horizontal flip was as data augmenta- applied tion during training: The weight of each loss was defaulted 0.01, Adis 0.1, Acon 0.05, and AR 0.1. to Aqua\nwhere the second summation is computed over all combi- This loss penalizes a nations of concept features Un and smaller similarity between Ur and\nMutual distinctiveness: To capture different aspects of images, different concepts should cover different visual el- ements. This means that the average image features of con- mini-batch,  given by Vr within cept k etlk Uk, Ut should be different from any other Vr' _ We can encode this into a loss term as\n5.2. Classification Performance\nWe compare the performance of BotCL with correspond- ing baselines (LeNet for MNIST and ResNet-18 for others classifier), our reimplementation of k-means with a linear and PCA in [46],3 and state-of-the-art concept-based mod- els. Table 1 summarized the results. BotCL with contrastive BotCLCont) achieves the best accuracy on CUB2OO, loss ImageNet, and  Synthetic, outperforming the baseline lin- It is also comparable to the state-of-the-art ear classifiers. on MNIST and Synthetic. BotCL with reconstruction loss (BotCLRec performance drop over CUB2OO, Im- shows outperforms BotCLCont on ageNet; and Synthetic, while it MNIST This behavior supports Our assumption that the re- construction loss is useful only when concepts are strongly tied to their spatial position. Otherwise, t is insufficient to reconstruct the original image, and BotCL fails. Contrastive self-supervision is the key t0 facilitating concept discovery:\nwhere the summation is computed over all combinations of Note that concept h is excluded from this loss if concepts image in a mini-batch has concept 6. no\n4.3. Quantization Loss\nsufficiently represented by Concept activation t can be binary instead use value, but continuous value for we training  We design a quantization loss to guarantee values are close to 0 or 1, given by\nWe also explore the relationship between the number of classes and BotCL s accuracy over CUB2OO and ImageNet: large variants of ResNet as the back- We used small and bone. We extracted subsets of the datasets consisting of the along first n classes with the class IDs. Figure 3 shows that competitive performance when the number of BotCL has We conclude that BotCL hardly de- classes is less than 200. the classification performance on small- or middle- (grades sized datasets.   However; this is not the case for n 200 (refer to supp. material for larger n and different k' s).\ngives the element-wise absolute value and where abs gives the Euclidean norm\n4.4. Total Loss\nWe use softmax cross-entropy for the target classification The overall loss of BotCL is task's loss, donated by lels defined by combining the losses above as\n'Implementation details are in supp. material:\nFigure 3_ Classification accuracy vS . the number of classes_ We used subsets of CUB2OO and ImageNet with k 50 and ResNet-1 8 and ResNet-101 backbones_\nTable 1. Performance comparison in classification accuracy: The best concept-based method is highlighted in bold. BotCLRec and BotCLCont are both BotCL but with reconstruction and contrastive loss, respectively: For ImageNet; we used the first 200 classes\n53.2 Consistency and distinctiveness of each concept\nBotCL is designed to discover individually consistent and We qualitatively verify this mutually distinctive concepts_ by showing each concept with its top-5 activated imagess attention maps in Figure 4b. For MNIST, different with concepts cover different patterns, and each concept covers the same patterns in different samples (even the samples of different classes). Figure Sb for CUB2OO shows that BotCL similar behavior on the CUB2OO dataset; that is, renders the top-5 concepts are responsible for different patterns, and each of them is consistent:\n5.3 Contribution of each concept in inference\nWe can qualitatively see the contribution of each concept by seeing removing the concept and changes in the corre- the Fig- sponding self-supervision task'$ output: As shown in 4c, (responsible when we set the activation of Cpt ure for the vertical stroke of digit 9) to zero, the reconstructed image looks like digit 0 _ 1, representing the ab- When Cpt sence of the circle in digit 7, is deactivated (i.e-, t1 is set to 0), circle emerges in the upper part of the reconstructed image. The resulting image looks more like digit 9_\n5.3. Interpretability\nQualitative validation of discovered concepts 5.3.1\nFigure 4a visualizes showing concept image k in the ar over MNIST: We selected 5 concepts out of 20 that are most 0.5) in the training set 4 Tak- frequently activated (i.e, tk they' ing digits 0 and 9 as an example, we can observe that 3-5 and the only difference is Cpt share Cpts 2 that lo- cates in the lower edge of the vertical stroke of 9_ This specific to digit 9_ We used BotCLRec; SO we can stroke is before reconstruction, which generates remove Cpt an image like 0 (refer to Section 5.3.3). Some concepts are in- compatible with human intuition; interpret such yet we can missing stroke that 1 may attend to the concepts (e.g., Cpt completes circle).\nFor CUB2OO shown in Figure Sc, we show images most input image in Figure Sa among the dataset similar t0 the ablating each concept: in terms of tTt' with When Cpt (responsible for the yellow head) is deactivated, more black- head bird images appear in the top-8 images. Cpt _ 5 covers birds feet and is common among most bird classes. De- activating this concept does not change the top-8 images These observations suggest that although some concepts do not contribute to classification performance, images are SUC - cessfully represented by combinations of concepts_\nFor CUB2OO, we train BotCLConst   with 50 and Figure Sa shows the attention maps of an image 20. k of yellow We can observe headed bird: that black 1-5 cover different body parts, in- the attentions for Cpts cluding the head, wing, back; and feet; which proves that the natural image as from BotCL can learn valid concepts Supp:  material exemplifies all concepts discovered well. from MNIST and CUB2OO.\n5.4. Quantitative Evaluation on Synthetic\nOne problem of the concept-based approach is the ab- quantitative evaluations of concepts sence of established because the choice of concepts may be arbitrary and the same level of representability may be achieved with differ- A single predefined set of concepts ent sets of concepts.\n'Cpts 1-5 the   frequency counted are   ordered based in the on dataset:\n'For each concept K, five images whose tr is highest among D_\nFigure 4 Concepts for MNIST Attention maps for different input images. (b) Top-5 activated images (images in the dataset whose tk is largest) for each concept: Images reconstructed by our concept decoder with detected concepts (original) and with a certain all concept deactivated.\nFigure 5. Concepts learned for CUB2OO. Visualization of 5 most important concepts for yellow headed black bird: (b) Top-5 activated concepts. (c) Image retrieval when all detected concepts were used (original) and when a certain concept was deactivated\ntest set with concept k activated: The concepts and the 5 combinatorial optimization shapes are associated as prob- lem so that the sum of Coverage maximized. sk over s are\nis not enough to evaluate the goodness of discovered con- Literature has evaluated concepts qualitatively cepts. as Section 5.3) or by user study (as Section 5.5).\nWe decided to use Synthetic [46] for quantitatively eval- uating concepts. The task is a multi-label classification that involves 15 shapes shapes (shown Combinations of the 5 in Figure 6a , S .1 to S . 5) form 15 classes, and the other 10 shape shapes are noises6 is covered by concept We deem k when the shape's area and concept k'$ area (the area with predefined threshold) for BotCL, where 0.9is a @n: overlap. Let hsk: overlaps with concept shape 1 denote The coverage of s by concept k K, and hsk otherwise. is given by\nWe use k 15 to train BotCL Figure 6a   visualizes the concept associated with each shape? A concept is lo- for images with the highest concept activa- cated by 6 @n The concepts cover the associated shapes with tions t concept  usually relatively  small regions, but one covers multiple shapes. This can be further evident in Figure 6b Coveragesk that shows only covers S whereas 3 Cpt 1 and Cpt 13 covers multiple shapes_ Cpt\nWe use three metrics other than accuracy t0 evaluate the performance of concept discovery8: Completeness mea- concept covers its associated shape in the sures how well dataset. (ii) Purity shows the ability to discover concepts shape. (iii) Distinctiveness quanti- that only cover a single\n'Images generated with random shapes, there be multiple are SO can classes (combinations of shapes) in a single image, which forms multi- label classification task:\nNote that in this experiment; only shapes matter but not colors.\n'Further details are in supp. material_\nFigure 6 Experiment on Synthetic. 5 are the five shapes of which combinations form classes Attention maps next t0 each S.1-S shape are of the concept that covers the shape. (b) Coveragesk (the concept associated with each of the five shapes is marked).\nTable 2. Quantitative evaluation on Synthetic Note that ACE uses concepts for post-hoc explanation and does not use them for clas- sification. Comp. Dist , and Acc mean completeness, distinctive- ness, and accuracy; respectively:\nfies the difference among concepts based on the coverage_\ncompared9 BotCL with contrastive loss is with ACE [14], and two baselines PCA and k-means in [46]. We ap- or PCA to F of all images in the dataset af- ply k-means flattening the dimensions_ The cluster centers or ter spatial Atten- the principal components are deemed as concepts tion maps can be computed by Euclidean distance Or cosine similarity: Once the attention maps are obtained, we follow BotCL' s process for classification.\nAs shown in Table 2, BotCL shows better completeness, distinctiveness, and accuracy scores than comparative meth- they' Although k-means is able to discover concepts, ods. optimized for the target classification task; and the are not performance is low. As we discussed, the concepts learned by BotCL tend to cover more than one target shape, causing comparatively low purity. The cluster center of k-means is able to capture only one kind of shape at the cost of com - pleteness. We can also observe that all methods are affected larger k ensure bet- by concept number k, and generally ter performance on all metrics. This result is not surprising, larger k is preferable for better in- but we confirmed that We detail the generation of the dataset, im- terpretability: plementation of PCA and k-means, and formal definitions of metrics in supp: material\nTable 3 Results of our user study:\nusing of MNIST and 30 participants for CUB2OO Amazon Mechanical Turk\n5.5. User Study\nWe defined three metrics t0 summarize the participants\" Concept discovery rate (CDR): The ratio of responses. the responses that are not None of them to all responses. higher CDR means participants can find some consistent vi- sual elements for many concepts. (ii) Concept consistency of partic- (CC): The ratio of exact matches out of all pairs ipants A high value means many participants responses. use the same phrases to describe a concept. (iii) Mutual in- formation between concepts (MIC): The similarity of the response distribution, computed over all possible of pairs This value is high when multiple concepts cover concepts.\nOur user study is designed to evaluate BotCL with re- alistic datasets for the challenge of human understanding Participants are asked to observe the test images with the attention map for concept k (refer t0 Section 5.3.2) and se- lect some phrases in the predefined vocabulary that best de- scribes the concept (i.e,, attended regions). They can also they choose None of them if cannot find any consistent vi- sual elements. We recruited 20 participants for each concept\n9 SENN [1] and ProtoPNet [8] are not comparable. SENN s concepts globally cover a whole image. ProtoPNet requires way more concepts_\nFigure 7_ Results of ablation study. Hyperparameter values vs. classification accuracy on ImageNet and CUB2OO. (b) Hyperparameter a) values vs. classification accuracy and other metrics on Synthetic_\nhighly arbitrary, and different sets of concepts may achieve similar classification performance. This arbitrariness may allow the designing of dedicated concept regularizers for the target task However; training failures happen when they are set to be large: A small value benefits training:\nthe same visual elements; therefore, lower is better\nFor  comparison; we also evaluated manual annota- and random  scribbling for the tionl0 same images: Ta- ble 3 shows that BotCL yields scores for all metrics good on both datasets (close to the manual annotation), show ing the learned concepts are interpretable for humans (from CDR), consistent (from CC), and mutually distinct (from MIC) More details are in supp: material\nImpact of AR: Due to the lower performance of the re - construction loss, we studied the impact of the contrastive loss only: The contrastive loss almost always improves the The performance boost is signif- classification accuracy: icant in CUB2OO and Synthetic. As ImageNet has more training data, this may imply that self-supervision greatly contributes to the learning of concepts when training sam- ples are limited. These results demonstrate the importance loss This is interesting since this of the contrastive loss. uses the same labels as the classification loss.\n5.6. Ablation Study\nhyper- We conducted ablation studies using the default parameters except for the one to be explored. As there is no ground truth concept for CUB2OO and ImageNet; only ac- curacy is evaluated (Figure 7a). For Synthetic, accuracy and employed the three metrics in Section 5.4 are Figure 7b).\nImpact of k A small k decreases the accuracy and other metrics, which means the necessity of searching the mini- training tends to fail for all mum number of concepts. Also, datasets when k is large (detailed in the supp. materials) . The number of concepts should be tuned for each dataset_ This sensitivity is one of BotCL s limitations.\n6. Conclusion\nThis paper presents BotCL for learning bottleneck con- Our qualitative and quantitative evaluation showed cepts. BtCLs ability to learn concepts without explicit supervi- sion O them but through training for a target classification We also demonstrated that BotCL could provide inter- task. pretability on its decision and learned concepts themselves_ One limitation of BotCL is that it requires Limitations: tuning the number k of concepts for each dataset: It might interesting research direction to estimate k, e.g- based be an on the number of classes in a given classification task We will investigate the phenomenon to mitigate this problem:\nImpact   of Aqua\" This   hyperparameter controls how binary: close t should be to The accuracy and the other gradually increases. BotCL en- Aqua metrics worsen when codes some information into t (such as the area that a con- cept occupies), which is lost for larger Aqua: An extreme value may also cause vanishing gradients.\nImpact of Acon and The individual consistency Adis and mutual distinctiveness losses hardly affect the perfor- mance on CUB2OO and ImageNet, although we can see Syn- slight drop when the values are zero for CUB2OO. For thetic, the performance metrics vary they designed as are Meanwhile, the  accuracy is relatively insensitive to be to these hyperparameters. The choice of concepts may be\nThis work is partly supported by JST Acknowledgement CREST Grant No. JPMJCRZOD3, JST FOREST Grant No_ JPMJFR2160,JSPS KAKENHI Grant-in-Aid for Scientific Research (A) This work is also supported by JSPS KAK- ENHI Grant  Number 19K10662, 20K23343, 21K17764, and 22H03353.\n1OThe authors annotated.\nReferences \n Now the paper ends. \nHow effective is the contrastive loss in helping BotCL discover more accurate concepts compared to the reconstruction loss?", "output": "Based on the results in the paper, it seems the contrastive loss is much more effective at discovering useful concepts compared to the reconstruction loss. Several key findings support this:\n\n1) When using the contrastive loss (BotCLCont), the model achieves higher classification accuracy on most datasets compared to using the reconstruction loss (BotCLRec) (see Table 1). \n\n2) On CUB200, ImageNet, and Synthetic, BotCLCont outperforms BotCLRec, indicating the contrastive loss leads to better concepts for these datasets (Table 1).\n\n3) The ablation study shows that when using only the contrastive loss (i.e. setting AR to 0), the classification accuracy improves significantly, especially on datasets with limited training data like CUB200 (Figure 7a).\n\n4) The user study finds that the concepts discovered with the contrastive loss are more interpretable, consistent, and distinct, as measured by the CDR, CC, and MIC metrics (Table 3).\n\nIn summary, the results suggest that the contrastive loss, which focuses on contrasting between positive and negative samples within a minibatch, is better at discovering concepts that are useful for both classification performance and interpretability compared to the reconstruction loss."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nwe formally address universal object de- In this paper; which aims to detect every scene and predict tection, ev- The dependence on human annotations, the ery category limited visual information, and the novel categories in the open world severely restrict the universality of traditional We propose UniDetector, detectors a universal object de- recognize enormous categories tector that has the ability to world The critical points for the universal- in the open 1) it leverages images of multi- ity of UniDetector are : heterogeneous label spaces for training ple sources and through the alignment of image and text spaces, which guar antees sufficient information for universal representations_ 2) it generalizes to the open easily while keeping the world balance between seen and unseen classes, thanks to abun- dant information from both vision and language modali- 3) it further promotes the generalization ability to ties: proposed decoupling train- novel categories through our ing manner and probability calibration. These contribu- tions allow UniDetector to detect over 7k categories, the size largest measurable category sO far; with only about 500 classes participating in training: Our UniDetector be - haves the strong zero-shot generalization ability on large- vocabulary datasets it surpasses the traditional supervised baselines by more than 4% on average without seeing any On 13 public detection datasets with corresponding images: UniDetector also achieves state-of-the-art various scenes; performance with only a 39 amount of training data.\nFigure 1. Illustration for the universal object detector: It aims to detect every category in every scene and should have the ability to utilize images of multiple sources with heterogeneous label spaces for training and generalize to the open world for inference\nthey 43] have made remarkable progress, heavily rely on large-scale benchmark datasets [12, 32]. However;  object varies in categories and scenes (i.e , domains). detection In the open world, where significant difference exists com- pared to existing images and unseen classes appear; one has guarantee the success of to reconstruct the dataset to again object detectors, which severely restricts their open-world generalization ability: In comparison, even child can gen - eralize well rapidly in new environments. As a result, uni versality becomes the main gap between AI and humans universal object detector can directly work Once trained, re-training, thus in unknown situations without any further significantly approaching the of making object detec - goal tion systems as intelligent as humans\n1. Introduction\nUniversal object detection aims to detect everything in every scene. Although existing object detectors 18,31,42,\nuniversal object detector should have the following First, two abilities_ it should utilize images of multiple\n'corresponding author\nCodes are available at https:Ilgithubcom/zhenyuw l6/UniDetector:\nsources and heterogeneous label spaces for training: Large- scale collaborative training in classification and localization sufficient required t0 guarantee that the detector can is gain information for generalization: Ideal large-scale learning needs to contain diversified types of images as many as pOS- sible with high-quality bounding box annotations and large category vocabularies. However; restricted by human anno- tators, this cannot be achieved. In practice, unlike small vO- cabulary datasets [12,32], large vocabulary datasets [17,23] tend to be noisily annotated, sometimes even with the incon- sistency problem: In contrast, specialized datasets [8,55,70] particular categories only focus To cover ade- on some quate categories and scenes, the detector needs to learn from all the above images, from multiple sources with heteroge- comprehensive and neous label spaces, so that it can learn complete knowledge for universality. Second, it should gen- eralize to the open world well. Especially for novel classes during - training; the detector can still that are not annotated predict the category tags without performance degradation. However; pure visual information cannot achieve the pur- pose since complete visual learning demands human anno- tations for fully-supervised learning:\nUnder the decoupling manner; we further present tector: class-agnostic localization network (CLN) for producing generalized region proposals. Finally, we propose probabil- ity calibration t0 de-bias the predictions. We estimate the prior probability of all categories, then adjust the predicted category distribution according to the prior probability The calibration well improves the performance of novel classes.\nOur main contributions can be summarized as follows:\nWe propose UniDetector; a universal detection frame- work that empowers us to utilize images of heteroge- generalize to the open world neous label spaces and To the best of our knowledge, this is the first work to formally address universal object detection_ Considering the difference of generalization ability in recognizing novel classes, we propose to decouple the training of proposal generation and Rol classification to fully explore the category-sensitive characteristics. the   produced   probability, calibrate We to propose which balances the predicted category distribution and raises the self-confidence of novel categories.\nExtensive experiments demonstrate the strong universal recognizes the most measurable cat ity of UniDetector: It Without seeing any image from the training set, egories our UniDetector achieves a 4% higher AP on existing large- vocabulary datasets than fully-supervised methods. Besides the open-world task; our UniDetector achieves state-of-the- art results in the closed world 49.3% AP on COCO with a pure CNN model, ResNet5o, and the 1x schedule:\nIn this paper; we formally address the task of univer- To realize the above two abilities of sal object detection_ the universal object detector; two corresponding challenges training with multi- The first one is about should be solved images. Images collected from different sources are source associated with heterogeneous label spaces. Existing detec - tors are only able to predict classes from one label space and the dataset-specific taxonomy and annotation inconsis- tency among datasets make it hard to unify multiple het- erogeneous label spaces: The second one is about novel category discrimination. Motivated by the recent success of image-text pre-training [20, 39, 58], leverage their we pre-trained models with language embeddings for recogniz- However; fully-supervised training ing unseen categories. during makes the detector focus on categories that appear training: At the inference time, the model will be biased to wards base classes and produce under-confident predictions Although language embeddings make it for novel classes_ possible to predict novel classes, the performance of them is still far less than that of base categories.\n2. Related Work\nObject detection aims to predict category tags and bound- ing box coordinates of each object within an image. Exist- ing methods can be generally divided into two-stage and one-stage methods. Two-stage   detectors  mainly include They RCNN [15] and its variants [4, 14,18,43]. usually extract a series of region proposals first, then perform clas- sification and regression. comparison, one-stage detec- In tors [31,33,42] directly generate classification results for Different from these methods, models such the anchors_ [26,50,61,67] are anchor-free for object detection. Re- as cently, transformer-based methods [5,10,27,60,71]also de- velop rapidly: most of these methods can only However; in the closed world work\nWe propose UniDetector; universal object detection framework; to address the above two problems. With the help of the language space, investigate  possible we first to train the detector with heterogeneous label structures spaces and discover that the partitioned structure promotes feature   sharing and avoids label conflict simultaneously. the generalization ability to novel classes Next, to exploit region proposal stage, we decouple the proposal gen- of the eration stage and Rol classification stage instead of training them jointly: Such a training paradigm well leverages their characteristics and thus benefits the universality f the de-\nOpen-vocabulary object detection. Traditional object de- tection can only detect categories that appear at the train- universal  object  detection, categories ing time In that be detected need cannot be known in advance. For to this purpose, zero-shot object detection [1, 40, 68, 69] is proposed and aims to generalize from seen t0 unseen cat- egories. their  performance still   far  behind However; is fully-supervised methods. Based on these researches, open - vocabulary object detection [57] forwards the task By in-\nOverview of UniDetector: It consists of three steps With the image-text pre-training parameters, UniDetector is trained with Figure 2_ In this way, it can directly detect in the open world for inference. images of different sources with multiple label spaces V= denotes The first stage conducts image-text pre-training to align the module to process visual information, and L' denotes the language encoder: the two spaces, the second stage trains with images of heterogeneous label spaces in the decoupling manner; and the third stage applies probability calibration t0 maintain the balance_\nfrom either Dtrain Or Dtest belongs to the same predefined label space (i.e. class vocabulary) L.\nvolving image-text aligned training, the unbounded vocabu- laries from the texts benefit the generalization ability of the detecting novel categories. With the development model for of large-scale image-text pre-training works [20,39,58], re- cent methods [11,13, 16,37,63] have considered adopting such pre-trained parameters in open-vocabulary detection and boosting the performance and category vocabulary to existing methods still large extent: Despite their success, transferring within a single dataset  Besides, their target at categories are usually categories. more than seen unseen Their generalization ability is thus restricted:\nIn this brand new object detection work, we propose which focuses on the universality of detectors. At the task training time, we utilize images from multiple sources_ That is, images of heterogeneous label spaces L1, L2, Lns At the inference time, the detector predicts class labels from which is provided by the users_ Ltest; new label space\nThe advances in traditional object detection cannot be trivially adapted to our universal detection task: The main reason is that there exist novel categories at the inference Ui_1 Li- Lnovel Ltest Techniques in traditional time: Lbase object detection benefit base categories Uiz_1 Li but may hurt novel categories. issue The core of our work how images of heterogeneous label is therefore to utilize categories. spaces, and how to generalize to novel\nMulti-dataset object detection training:  Previous object detection methods focus on only one single dataset: Since one dataset is involved in training, both the dataset only vocabulary are limited. Recently, training on mul scale and tiple datasets [3,46] has been used to boost the model's ro bustness and expand the detector'$ vocabulary size. The difficulty of multi-dataset training for object detection is to utilize multiple heterogeneous label spaces. For this pur- pose, [62] leverages pseudo labels to unify different label partitioned structure, and [36] uti- adopt spaces, [53,66] language embeddings. However; these methods still lizes detecting in the closed world. Different from them focus on we target generalizing in the open world.\n4. The UniDetector Framework\nWe propose the UniDetector framework to address the which is illustrated in Fig: universal object detection task; The basic process consists of three steps_ 2\nStepl: Large-scale image-text aligned pre-training: Tra- ditional fully-supervised learning with only visual informa- tion relies on human annotations, which restricts the univer- Considering the generalization ability of language sality. language embeddings to assist de- features, we introduce Inspired by the recent success of language-image tection_ embeddings from pre-training; we involve the pre-trained We adopt RegionCLIP image-! text model [20,39,58,63]. [63] pre-trained parameters for Our experiments_\n3. Preliminary\nGiven an image I, object detection aims to predict its {(bi, Ci)}{1 which consists of bounding box label y TL We are usually given a coordinates bi and category label Ci Dtrain {(I1,91),- single dataset (In, Yn) } and the goal = Dtest. is t0 inference on the test dataset\nStep2: Heterogeneous label space training: Unlike tra- ditional object detection, which single concentrates on from dataset with the same label space, we collect images different sources with heterogeneous label spaces to train\nTraditional object detection can only work in the closed single dataset. world, where images are restricted to The Each object category Ci dataset has its own label space L\nFigure 3_ Possible structures to utilize images from heterogeneous label spaces for training: The above boxes denote the structure for training, and the below boxes denote the inference process. All the classification head here adopts the similarity between the region features language embeddings The separate structure trains individual networks and ensembles them for inference, the unified structure unifies and the multiple datasets into one dataset; and the partitioned structure shares the same backbone but different classification heads.\nThe various training images are necessary for the detector: universality: Meanwhile, adopt the de- the detector s we during training process instead of the coupling manner the previous joint training\nplers like the class-aware sampler (CAS) [38] and the repeat factor sampler (RFS) [17] are helpful strategies for multi- dataset detection in the closed world [66]. However; the open-world performance is unaffected: The reason is that With language the core issue here is about novel classes embeddings, the adverse effect of the long-tailed problem becomes negligible. We thus adopt the random sampler\nStep3: Open-world inference: With the trained object de- tector and the language embeddings from the test vocabu- perform detection in the open world directly lary; we can for inference without any finetuning: However; since novel categories do not appear during training, the detector is easy to generate under-confident predictions. We propose prob- ability calibration to keep the inference balanced between categories and novel categories in this step. base\nLikewise, loss functions like equalized loss [48,491 and seesaw loss [52] influence universal object detection little. Instead, the sigmoid-based loss is more suitable since the classification of base and novel categories will not interfere sigmoid function with each other under the To avoid an sigmoid-based classification loss excessive value of when the number of categories increases, randomly sample a we certain number of categories as negative ones_\nHeterogeneous Label Space Training 4.1.\nExisting object  detectors can  only learn from  images with one label space because of their single classification layer: To train with heterogeneous label spaces and ob- tain sufficiently diversified information for universality, we present three possible model structures, as shown in Fig: 3.\nDecoupling proposal generation and Rol classification:\ntwo-stage object detector consists of a visual backbone encoder; RPN and Rol classification module. Given an image from the dataset D with the label space L, the net- work can be summarized as: {2ij}jL| @ Rol d RPN j=1 exp( _2T,e;/v)) j \u20ac dbackbone; 1/(1 + L, where pij Pij is the probability of the i-th region for the category j {2ij}}L4, denotes the logit outputs from the Rol head, and language embedding of the category j. ej is the\nOne possible structure is train with   separate  label to Fig: As in 3a, we train multiple models on every spaces. With new language embed single dataset (i.e., label space) dings at the inference time, each single model can perform inference on the test data: These individual test results can be combined t0 obtain the ultimate detection boxes. An- other structure is to unify multiple label spaces into one label space, as in Fig: 3b. Then we can treat these data they the same as before_ Since images are treated as if are they single dataset; can be processed with tech- from one niques like Mosaic [2] or Mixup [59] to boost information integration among different label spaces. With the help of language embeddings for classification, we can also use the partitioned structure in Fig: where images of multiple 3c, sources share the same feature extractor but have their own classification layer: At the inference time, we can directly embedding of test labels to avoid label conflict. use the class\nThe region proposal generation stage and the Rol clas- sification stage act differently when it comes to universal The proposal generation stage maintains satisfy detection. ing universality ability since its class-agnostic classification can be easily extended to novel classes_ In contrast, the class-specific Rol classification stage cannot even work for novel categories Even with language embeddings, it is still The distinct properties affect their biased to base classes joint training since the sensitivity of the classification stage to novel classes hampers the universality ability of the pro posal generation stage. Consequently, decouple these we two stages and train them separately to avoid such conflict.\nWe then need to consider the data sampler and the loss function. When the data turn large-scale, an unavoidable problem is their long-tailed distribution [23,25,45]. Sam-\nSpecifically, the region proposal generation stage is ini- tialized with traditional ImageNet pre-trained parameters\nibration is to decrease the probability of base categories and increase that of novel categories, thus balancing the ulti- probability prediction. The probability calibration is mate illustrated as follows:\nFigure 4 Illustration of class-agnostic localization network The localization confidence and class-agnostic classification con- tribute together to generating proposals for universal detection.\nOur probability calibration is mainly about dividing the original probability with probability Tj of the cat- prior The probability Tj records the bias of the prior egory j. pre-defined hyperparameter_ network t0 category j. is larger Tj indicates that the model is more biased toward the category. After calibration, its probability turns smaller which contributes to the probability balance. We can con- duct inference on the test data first and use the number of categories within the results to obtain If the number of Tj' prob- test images is tOo small to estimate the accurate prior training images to calculate Tj ability; we can also use the\nand trained in class-agnostic way: After training, it pro a series of region proposals. With the generated pro duces posals, the Rol classification stage is trained in the Fast This stage is initialized with image RCNN [14] manner: text pre-trained parameters for predicting unseen categories_ These two kinds of pre-trained parameters also contain complementary features, which provide more comprehen- sive information for universal object detection:\nfrom reflects the class-specific prediction for Eq: Pij the i-th region proposal. Considering the open world gen- eralization ability of the class-agnostic task; multiply we with its objectness score ni from CLN for the detec pij introducing hyper-parameter B tion score. After further 1-8) the final detection score is Sij\nClass-agnostic localization network To produce gener alized proposals in the open world, present the class we agnostic localization network (CLN), as illustrated in Fig: Instead of a single RPN module, our CLN contains both 4_ the RPN and the Rol head to generate proposals for uni Such kind of network promotes versal object detection. during proposal generation box  refinement We mainly adopt localization-based  objectness   for  object discovery since localization-related metric tends t0 be robust t0 novel objects in the open world [21]. In the Rol head, based on the localization confidence, we also keep the binary class agnostic classification because it provides strong supervised signals to the network training: For the i-th proposal, denot- ing its localization confidence from the RPN as $i from the Rol head as s7 and its classification confidence as si , the ultimate confidence from our CLN can be obtained (s%1s,2)(1-a through geometric weighting: (si) ni is a pre-defined hyperparameter: where\n5. Experiments\nTo demonstrate the universality of our UniDetector; we conduct experiments and evaluate our UniDetector in the open world, in the traditional closed world and in the wild_ Its superior performance under various conditions well il- lustrate its universality.\nDatasets.  To simulate images of multiple sources and het erogeneous label spaces; we adopt three popular object de training the detector: COCO [32], Ob- tection datasets for jects365 [45], and OpenImages [25]. COCO contains dense and high-quality annotations from human labor on 80 com Objects365 is larger-scale and contains 365 mon classes_ classes. OpenImages consists of more images and 500 cat- egories, and many annotations are sparse and dirty: Due to large scale of these datasets, we randomly 35k, the sample and 78k images from them respectively for training: 60k, Without specification; we all use the selected subset.\n4.2. Open-world Inference\nWith the language embeddings of the vocabulary test our trained detector can directly perform inference in Ltest; the open world. However; since only base categories appear during training, the trained detector will be biased toward base categories. As result; boxes of base categories in de larger confidence score than tection results tend to have novel categories, thus predominating in the inference prO- Considering the huge number of novel categories, the cess. easily make the detec- over-confidence of base classes will tor ignore novel category instances of a larger number and hurt the detector' $ performance in the open world\nWe mainly perform inference on the LVIS [17], Ima geNetBoxes [24], and VisualGenome [23] datasets to eval- Considering uate the detector's open-world performance. their large category numbers, these datasets can simulate the open-world  environment to LVIS v0.5 some extent: 1,230 categories, and LVIS v] consists of ,203 contains categories, with 5,000 images and 19,809 images for the validation set, respectively   ImageNetBoxes contains over 3,000 categories_ We random sample 20,000 images from To compare with the supervised the dataset for evaluation.\nTo avoid the bias issue, we propose probability calibra- tion for post-processing the predictions. The purpose of cal\nTable 1. The performance of UniDetector in the open world. We evaluate it on LVIS, ImageNetBoxes and VisualGenome_ S, U,P denote treating heterogeneous label spaces as separate spaces, a unified one O a partitioned one The Faster RCNN (closed world) row is from the traditional supervised Faster RCNN C4 baseline trained on the corresponding dataset with the same random data sampler: We select 35k, 60k, 78k images from COCO, Objects365 and OpenImages respectively for training\nclosed world baseline, we  sample 90,000 images as the training set: The most recent version of the VisualGenome dataset contains 7,605 categories large However; since number of its annotations come from machines, the annota- tions are pretty noisy: We select 5,000 images that do not training images for inference. appear in the\nachieve comparable or even better performance compared to the corresponding closed-world detector; while the re- quired training budget is less. Another noticeable result is that the traditional closed-world detector suffers from the long-tailed problem APr is only 1.9% compared to the In comparison, the APr 25.49 AP f - and AP f from our de- tector are significantly more balanced. This illustrates that UniDetector also greatly alleviates the long-tailed effect:\nEvaluation metrics. We mainly adopt the standard box AP for evaluating the performance. For the LVIS dataset; we also evaluate the performance on its rare, common and fre- APc quent categories separately, denoted as APr, and AP f For the ImageNetBoxes dataset, since most of the images APso within it are object-centric, besides the AP and met- rics, we also adopt the top-1 localization accuracy (denoted challenge [44] to eval- Acc.) as Loc. from the ImageNet uate the object-centric classification ability of the detector: For the VisualGenome dataset; considering the noise and in- adopt - consistency of its annotations, we the Average Recall metric for evaluation_ (AR;\nWe then analyze the effects of different structures on the COCO and Objects365 datasets_ We use WBF [47] t0 en- semble two detectors for the separate label spaces. Images training under during of different sources cannot interact this structure, which restricts the feature extraction ability: the inconsistency labels of different For the unified space, missing annotation problem: datasets lead t0 a serious Al- though we adopt pseudo labels according to [62] and boost the image fusion through mosaic, the open-world AP is still In contrast; with the partitioned structure, all not improved kinds of images train the backbone together; thus promoting In the classification time, the partitioned feature extraction_ label space mitigates the label conflict:  Therefore, the par- titioned structure performs the best among them.\nImplementation details: We implement our method with Without otherwise specified, we choose mmdetection [7]. ResNet50-C4 [19] based Faster RCNN [43] as our detec- tOr, initialized with RegionCLIP [63] pre-trained parame- All the models are trained in the 1x schedule, which ters . is 12 epochs. For hyperparameters, T is set t0 0.01, is set to 0.6, and &, B are set to 0.3 both:\nWith the partitioned structure, COCO and Objects365 joint training achieves the 22.29 AP; higher than the sin - gle results of 16.4% and 20.29. We also notice that Open- Images single training obtains the 16.8% LVIS AP only slightly higher than COCO and even lower than Objects365. Considering the more images and categories within it; the limited performance can be attributed to its noisy annota- tions_ However; if we further add OpenImages images to COCO and Objects365, the LVIS v0.5 AP can be improved At this time, COCO and Objects365 images to 23.5%_ have high-quality annotations, while Openlmages provides categories but noisy annotations. Images from mul- more tiple sources cooperate and bring various information, thus contributing to better open-world performance. This is the significant superiority of training with heterogeneous most\n5.1. Object Detection in the Open World\nWe list the open-world results of UniDetector in Tab: we conduct supervised closed-world ex For comparison, periments with the same Faster RCNN structure and the C4 On the LVIS v0.5 dataset, the tradi- random data sampler: tional supervised detector obtains the 17.79 AP In com parison, our UniDetector with only 35k COCO images ob- With only 60k Objects365 images, it tains the 16.49 AP obtains 20.2% AP With significantly fewer images and an- notated categories, the detection AP is even higher: The effectiveness of our UniDetector is demonstrated: it can\nZero-shot performance on 13 ODinW datasets: Table 3.\nThe performance of UniDetector in the closed world_ Table 2 The models are trained on COCO train2017 set with the Lx sched- epochs), ResNet5o backbone and evaluated on COCO (12 ule val2017. %: the method uses extra images and more epochs_\nComparison with existing open-vocabulary detection Table 4 methods on the COCO dataset:\nWe surpass the Dyhead [9], the state-of-the-art structure. CNN detector by 6.3% AP Compared to Softteacher [54], semi-supervised model that utilizes additional images and more epochs, our UniDetector also achieves trains with 4.8% higher AP Compared to recent transformer-based de- tectors, the performance superiority is also obvious. The results show that our UniDetector not only generalizes well in the open world, but also holds effectiveness in the closed The superiority on both the open world and closed world. world strongly confirms the universality of our UniDetector:\nlabel spaces to universal object detection. A similar trend of results is also observed for LVIS vl.\nWe further evaluate our UniDetector on ImageNetBoxes and  VisualGenome datasets_ two datasets contain These categories, thus better simulating the open-world en- more keeps vironment. Our UniDetector an excellent open-world generalization ability. On the ImageNetBoxes dataset; it ob- AP; from surpassing the 3.99 AP tains the 8.2% traditional detectors with comparable training images. It is also worth mentioning that the  domain  gap between the ImageNet- Boxes dataset and COCO-style datasets is relatively large, since ImageNetBoxes images are mainly object-centric. In this situation, our UniDetector still generalizes well, which validates the universality of our UniDetector: On the Visu- alGenome dataset,  where category numbers are more than higher detection re- 7,000, our UniDetector also obtains sult compared to the traditional Faster RCNN. The most sig- AR1oo from nificant improvement comes metric, which the Through this experiment, the category is more than 4%. recognition ability of our UniDetector is revealed.\n5.3. Object Detection in the Wild\nTo further demonstrate the ability of our UniDetector to detect everything in every scene, we follow [28] to con- experiments o 13 ODinW datasets. duct These datasets various domains, such as airdrone, underwater; ther- cover diversity of categories. mal, thus also with Such prop erty makes it suitable to measure the universality of a de- We list the average AP these 13 datasets   in tector: on Tab. 3. Compared to GLIP-T; whose backbone (Swin-Tiny) requires (ResNet5o), little budget than more ours our higher average AP (47.39 VS: method achieves 46.59) In comparison, OuI method only utilizes 3% amount of data of GLIP-T This experiment corroborates the universality of UniDetector and illustrates its excellent data efficiency:\n5.2. Object Detection in the Closed World\nA universal object detection model should not only gen- eralize well to the open world, but also keep its superior- ity in the closed world that has been seen during training: using only images from the We thus train our UniDetector COCO training set and evaluate it on the COCO 2017 val- idation set. We compare our results with existing state-of- the-art closed-world detection models and present the detec- we utilize R(Det)? [29] tion AP in Tab 2. In this subsection, with the cascade structure [4] for our detector: For our CLN we introduce the Dyhead [9] structure, and focal loss [31] AdamW [22, 34] optimizer is adopted, for classification. learning rate. with 0.00002 for the initial\n5.4. Comparison with Open-vocabulary Methods\nsettings - We conduct experiments on the existing open - of vocabulary works for fair comparison with them to fur- ther show the effectiveness of our UniDetector; Specifically, the COCO dataset and the LVIS v] dataset are splitted in the 48/17 and 866/337 way separately for base and novel For the LVIS experiment; adopt the same Cen- classes_ we terNet2 [65] structure and image-level annotated images as learning, and Dyhead [9] for prO- Detic [64] for detection posal generation. The box and mask AP on novel and base classes is listed in Tab: 4 and Tab: 5\nWith the ResNet50 backbone and the Ix schedule, our pure CNN-based UniDetector obtains the 49.39 AP with\nComparison with existing open-vocabulary detection Table 5 methods on the LVIS vl dataset: For LVIS based training; Detic and our method use LVIS base images and image-level annotated images from ImageNet: For unrestricted open-vocabulary training; we only use 10% amount of images from Objects365.\nIllustration for probability calibration on the LVIS Figure 5. v0.5 dataset: We train the Centernet2 model on LVIS base images and image-level annotated images from ImageNet_\nis beneficial for open world detection, while this does not happen in traditional closed world detection. If we extract region proposals with a class-agnostic Faster RCNN, the AP is 19.7%. The 0.6% improvement indicates that the struc- ture with both RPN and Rol head is more suitable for gen- erating proposals in the open world than single RPN. If we adopt the OLN [21], also with the Rol head, the LVIS AP is still 19.7%, which indicates that pure localization in- bring further improvement: formation cannot Our CLN, with both classification score and localization quality, con - 21.29 tributes to AP is higher than AP This not only networks with similar budgets, but also more complicated models like Cascade RPN. This demonstrates the effective- learning manner and our CLN. ness of the decoupling\nAblation study 0n region proposal generation on the Table 6. LVIS v0.5 dataset:  The networks listed here are all trained on the subset of COCO and Objects365 dataset_\nThe obtained box AP powerfully demonstrates the gen- eralization ability of our UniDetector to novel classes On the COCO dataset, we obtain the 35.29 box AP for novel classes, which surpasses the best previous method (31.79 from RegionCLIP) by the LVIS dataset; 3.59. On we obtain the 29.3% box AP and 26.59 mask AP for novel which outperforms Detic classes (i.e. APr in this case), The extraordinary improve- by 2.69 and 1.6% separately: categories validates the excellent ability of ment on novel It is worth mentioning that our method for unseen classes_ one detection dataset is involved in this experimen- only little restricted tal setting, where our UniDetector is even When introducing multiple by the single source of images datasets for training, the superiority of our method is more prominent With only 10% amount of the training images, we surpass OWL-ViT by 1.39 for novel categories. The comparison well demonstrates the universality:\nProbability  calibration We   further the AP measure on novel and base categories separately to test the ability of We follow the settings of Detic [64] probability calibration on LVIS v0.5 and plot the box and mask AP in Fig: We 5_ notice that after calibration, both box AP and mask AP on novel classes improve significantly, more than 29. As a re- sult; the performance gap between base and novel classes is reduced remarkably. In comparison; the performance on This is because the base classes almost remains the same_ probability we design reduces the self-confidence of prior base categories significantly. As we can see, the probability calibration alleviates the bias of trained models, thus help- ing generate more balanced predictions in the open world.\n6. Conclusion\nwe propose UniDetector; In this paper; a universal ob- By utilizing images of multiple ject detection framework heterogeneous label spaces, and generalizing the sources, our UniDetector can directly detector to the open  world, detect everything in every without any finetuning: scene Extensive experiments on large-vocabulary datasets and di- strong universality verse scenes demonstrate its it behaves the ability to identify the most categories so far: Univer- vital issue that bridges the gap between artificial sality is intelligence systems and biological mechanisms_ We be- lieve our research will stimulate following research along the universal computer vision direction in the future.\n5.5. Ablation Study\nFinally, we conduct ablation studies in this subsection_ We mainly analyze the effect of decoupling region proposal generation and probability calibration here\nDecoupling proposal generation and Rol classification:\n6 analyzes the effect of the decoupling training man- Tab. trivial Faster RCNN trained COCO and Ob- ner: on jects365 obtains the 18.1% open-world AP on LVIS. If we decouple the two stages, the box AP is 19.1%. The 1.0% AP improvement demonstrates that the decoupling manner\nReferences \n Now the paper ends. \nGiven that the research aims to enable universal object detection, what are some real-world applications that could benefit the most from this technology?", "output": "Potential applications that could benefit include:\n\n\u2022 Autonomous vehicles: Universal object detection could help self-driving cars detect and identify any object on the road,  including novel objects they have not encountered before. This could improve the safety and reliability of autonomous driving.\n\n\u2022 Security and surveillance: The ability to detect any object, even previously unseen ones, could help security systems and CCTV cameras identify suspicious objects and activities in real time.\n\n\u2022 Industrial automation: Factories employing robotic arms, AGVs, and other automated systems could benefit from universal object detection to identify objects on the production line, even new ones not previously encountered. This could improve flexibility and adaptability.  \n\n\u2022 Logistics and warehousing: Automated sorting and picking systems in warehouses could utilize universal object detection to identify any package or item, helping to sort and route goods accurately and effectively.\n\n\u2022 Healthcare: Medical imaging and diagnostic systems could benefit from being able to automatically detect and identify any object or anatomical feature in scans and images, even novel ones. This could improve automated disease screening and diagnosis."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nAnswering non-factoid questions (NFQA) is challenging task, requiring passage-level an- swers that are difficult to construct and evaluate_ Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents_ Mean - while, modern models can generate highly co herent and fluent, but often factually incor- rect answers that can deceive even non-expert humans_ There is critical need for high- quality resources for multi-document NFQA (MD-NFQA) to train new models and evaluate grounding and factual consistency in answers relation to supporting documents_\nFigure I: An instance of proposed WIKIHOwQA\nTo bridge this gap, we present WIKIHOWQA a new multi-document NFQA benchmark built on WikiHow, website dedicated to answer- questions_ ing how-to The benchmark in- along cludes 11,746 human-written answers with 74,527 supporting documents We de- scribe the unique challenges of the resource provide strong baselines, and propose a novel human evaluation framework that utilizes high- lighted relevant supporting passages to mitigate issues such as assessor unfamiliarity with the including question topic. All code and data, the automatic code for preparing the human evaluation, are publicly available.\nHowever; even relatively to generate an answer: straightforward questions like how to fix my com- puter issue\" can have multiple solutions, requiring manually search through multiple re- the user to trieved documents t0 find the one that to applies their situation. Complex questions such as \"how to feel calm and relaxed\" may require an aggregated summary of the most methods from multi- popular There are also questions for ple relevant sources which answers have yet to be written, requiring a QA system to treat relevant documents as initial sources of information and then t0 reason out an answer based on them: For example, the question how to prepare to buy a house in [neighbourhood], [city]\" would require the system to retrieve relevant documents about buying houses in general and spe- cific articles about the local house market and the neighbourhood, and then construct a more sophis- ticated answer through reasoning rather than just summarizing multiple documents\nIntroduction\nNon-factoid   questions long, (NFQs)   requiring passage-level answers, explanations such as or opinions, pose challenges  for question current wering systems. While few datasets exist anse for NFQA (Cohen and Croft; 2016; Hashemi et al. 2019; Soleimani et al,, 2021), users are currently limited to seeing a summary of the most relevant document in a  search snippet result page on be insufficient for complex (SERP), which can questions that have scattered answers across multi- ple documents or require sophisticated reasoning\nTo address these challenges and move towards more advanced QA systems that can provide in- depth and comprehensive answers to a wide range of questions, benchmark for propose a new we the task of multi-document non-factoid QA (MD-\nhttps /lurunchik.github. io/WikiHlowQA\nNFQA) This task involves using multiple relevant documents to generate a complete and coherent an- given NFQ. We focus our benchmark on swer t0 a the INSTRUCTION category of NFQs, which of- These questions are under- ten begin with how to\" represented in current multi-document QA Bolo- tova et al, 2022) datasets, despite their popular- ity, as evidenced by a 140% increase in how to_ Google since 20042 and making up searches on over half of the most searched queries.3 Our bench- mark aims to fill this gap by specifically targeting INSTRUCTION questions.\ncrowdsourcing experiments. Inspired by series of the elaborate yet resource-intensive manual eval uation framework of abstractive models proposed by Dou et al. (2022), we introduce a simple human evaluation framework that leverages highlighted relevant passages to enhance the quality of NFQA Our findings demonstrate that incor assessments_ porating highlighted relevant supporting passages into the evaluation process not only aids evaluators in understanding the context of each question and answer but also contributes to the factual accuracy integrate this of the evaluation: As a result; we method into the evaluation of the newly proposed benchmark.\nWikiHow is for   INSTRUC web-resource TION questions that contains over 235,000 articles wide range of topics. These articles provide on comprehensive step-by-step instructions and are written by a community of experts and reviewed by an average of 16 people. The WikiHow website has proven to be a valuable resource for machine Wang, Yang learning tasks (Koupaee and 2018; et al,, 2021; Bhat et al,, 2020; Zellers et al,,2019; Ladhak et al, 2020; Boni et al,, 2021; Zhang et al. 2020; Cohen et al,, 2021; Anthonio et al., 2020), leverage WikiHow to create a and in our work, we high-quality benchmark specifically designed for MD-NFQA within the INSTRUCTION question category: Our benchmark consists of 11,746 ques- tions from the INSTRUCTION category each corresponding human-written answer; with a paired sourced from a diverse range of WikiHow articles. supported by corresponding parsed Each QA is pair relevant HTML pages from which the answer can be derived (Fig: 1). We evaluate several baseline models on the new benchmark that could serve as lower and upper bounds for model performance:\nThe new benchmark for MD-NFQA serves as valuable resource for the development of more advanced QA systems that can provide in-depth and comprehensive answers grounding informa tion in supporting documents. By including parsed passages from relevant HTML pages and provid- ing human judgments, our benchmark also has the potential to facilitate research in evaluating the fac - tual correctness of long-form answers. All data is publicly available4 on the dataset website.\nRelated Work 2\nMD This datasets for related section covers NFQA tasks such as long-form question answering (LFQA) and query-based multi-document summa rization (QF-MDS), highlighting the scarcity of resources available. We examine the strengths and limitations of these resources, aS well as the differ- ences between these tasks and MD-NFQA:\nLFQA: The task of LFQA, introduced by Fan al. (2019), involves providing long answers et to open-ended questions. While the associated ELIS benchmark includes supporting \"document\" to generate answers, the LFQA task; unlike MD NFQA task; does not assume that an answer is scattered across multiple documents. Moreover while MD-NFQA exclusively targets NFQs, ASQA LFQA dataset Stelmakh et al,, 2022) addresses ambiguous for long-form the need answers to factoid questions. Similarly, Natural Questions dataset (Kwiatkowski et al , 2019) contains both long and short answers for factoid questions (Xu et al., 2022).\nHuman annotation is often used as the standard for evaluating long-form answers, but research has shown that individuals without specific training only distinguish between human-generated and can equivalent to ran generated text at a level auto- This presents dom chance (Clark et al., 2021). challenge for evaluating the new benchmark for a NFQA, particularly when the answers are lengthy Or the topic is unfamiliar (Krishna et al, 2021) interesting (Bolotova et al,, 2020). Given or not that standard metrics struggle to detect factual in- consistencies, such as number swapping, negation, etc. (Kryscinski et al,, 2019), we delve into human sensitivity to such discrepancies by conducting a\nThe ELIS dataset (Fan et al,, 2019) for LFQA includes 272,000 questions from the \"Explain Like Im Five\" Reddit web forum; where questions and\nZhttps:_ /bit.ly/telegraph-google-how-to\n\"Under RMIT University DTA license for research use\n'https /bit.ly/most-asked-questions-on-google\nanswers must have a score of at least two to be in- cluded: The top-voted answer for each question is considered the correct answer; and the supporting content for each question is generated by extracting sentences with high tf-idf similarity from the top 100 web pages that match the question from the Common Crawl corpus. In contrast, our proposed MD-NFQA dataset uses texts parsed from relevant HTML pages chosen by the author of the corre- sponding article on WikiHow, rather than relying on automated methods for curating reference con- tent: In addition to a lack of grounding in support- ing content; Krishna et al. (2021) found significant Train/ Valid overlap in the ELIS dataset, and high- light challenges with both automatic answer eval- uation and human annotation for this benchmark proposed benchmark; we address these is- In our through the use of a novel human-evaluation sues framework utilizing supporting documents and by avoiding question overlap (Sec. 5 and Sec. 3).\nFigure 2: An example WikiHow article\nprovided in- clusions or inferences based on the formation, while MD-NFQA requires higher level reasoning and synthesis to provide complete and While there are a few resources accurate answers available for MDS (Litkowski, 2004; Angelidis and Lapata, 2018; Liu et al,, 2018; Dang; 2006; Fab- bri et al., 2019; Ganesan et al. 2010; Wang and Ling, 2016; Yasunaga et al,, 2019; Koupaee and 2018; Lu et al,, 2020) and QFS (Zhong et al. Wang; 2021; Nema et al,, 2017; Zhao et al,, 2021), they are scarce for QF-MDS.\nNakano et al. (2021) used questions from the ELIS dataset and collected new answers from hu- man annotators who were instructed t0 search for related documents and use them to construct their They trained the WebGPT model on that answers . dataset to answer long-form questions by mimick- ing the way humans research answers to questions online: it searches and navigates the web to find relevant pieces of information and concludes an citing answer based on them by sources for factual Authors report that the model answers accuracy. preferred by assessors 69% of the time to the are best human-written answer from Reddit when eval- uating on ELIS. While the WebGPT dataset is valuable, as of this moment; the authors have only released the model's answers and questions, not the supporting documents.\nThe QMDSCNN and QMDSIR datasets target QF-MDS task Pasunuru et al., 2021). The first dataset is derived from CNN/DailyMail having real summaries with simulated queries, while the sec- ond dataset is derived from a search engine query log and has simulated summaries with real queries\nlarge Another automated approach for curating query-focused summarization tasks is datasets for AquaMuse (Kulkarni et al,, 2020). This dataset supports both abstractive and extractive QF-MDS Queries and long answers from the Natu- tasks. ral Questions dataset (Kwiatkowski et al,, 2019), pre-processed version of the Common Crawl and a Long were used (Raffel et al., 2020). corpus an- swers from Natural Questions and the Common Crawl corpus are encoded into sentence embed- dings. Then a similarity search is performed over long the corpus and answers to find candidate doc- uments from the corpus for QF-MDS tasks (similar to ELIS).\nFinally, it's worth noting that while REASON and EVIDENCE-BASED categories of NFQs pre- vail in the ELIS (Bolotova et al., 2022) and We- bGPT datasets, our dataset focuses specifically on the INSTRUCTION category:\nQF-MDS: In contrast to MD-NFQA, which in- volves generating detailed passage-level answers to NFQs, QF-MDS (Tombros and Sanderson, 1998) creating concise summaries in response focuses on specific queries, which may include factoid ques- to queries not in question form, such as [En- tions or This requires a different set of tity], [Event], etc_ skills and approaches compared to MD-NFQA, as QF-MDS summaries do not involve making con-\nproposed HowSumm, a QF Boni et al. (20213 MDS dataset automatically constructed from Wiki- How content by utilizing the referenced articles as the summarization source, the corresponding ele- ments of WikiHow articles as the target summaries, and titles used as the queries. There are two types\nTable 1: Comparison of WikiHOwQA with other QF-MDS and LFQA datasets\nTable 2: Percentage of novel n-grams\nof QF-MDS tasks, one for methods (HOwSUMM METHOD) and one for steps (HOWSUMM-STEP). While this work is similar to the dataset we present, HowSumm is directed toward the QF-MDS task does not discuss train-test overlap, and only in- cludes the source URLS in their dataset:\nResource Description 3\nincluding This section presents Our new resource, its construction, text statistics, comparison with other benchmarks, and thorough quality analysis.\net al,,2019), Krishna et al. (2021) observed little to no evidence that analyzed models grounded answer They at- generation in the retrieved documents. significant overlap (around 43.69) tributed this t0 a in the training, validation, and test sets of ELIS. Similarly, Lewis et al. (2021) identified train-test overlap in various QA datasets. To avoid this issue questions in explicitly in our new resource, split _ we paraphrased questions a way that no same-topic o ended up in different splits. To do So, we clustered all questions prior to splitting and assigned all ques- tions belonging to the same cluster to the training split: We only assign clusters to the training split so that the validation and test splits will have more var- potential evaluation bias ied questions, to avoid a due to the presence of many similar questions.5\nData Collection 3.1\nFig: 2 illustrates an example of a WikiHow article accompanied by a high-quality, human-written arti- cle summary: These summaries are equivalent to passage-level question: shorter To answers to a build WIKIHOwQA, we first downloaded over 236,000 articles published on WikiHow before Jan- uary 2022 using the MediaWiki API: We then fil- tered out articles without references, resulting in a collection of 126,711 articles. Among these ar- ticles, only about 20% had human-written article summaries, which we used as target answers_ We downloaded the HTML content of all cited URLs using the Wayback Machine to provide the version closest to when the article was created or modified. snapshot ver- The final HTMLs were saved from the We simplified the HTML source code, and sions_ processed the content in two ways: (1) by extract- ing text content or (2) by converting it to Markdown format. The latter format preserves formatting such as tables and lists. Finally, we rejected articles with missing HTML snapshot links, empty HTML, Or empty extracted Each instance in our text. new dataset consists of a question; a human-written ar ticle summary treated as the target answer; and a list of parsed texts from the relevant HTML docu- ments cited by the article' s author: Fig: 1 shows a simplified example from WIKIHOwQA:\nDataset Statistics 3.2\nThe WIKIHOwQA dataset include a diverse range of topics, 19 in total, with the most frequent be- and garden' thome ing health\" \"pets and ani- and ~computer and electronics\".6 Table 1 mals to other QF-MDS and compares WIKIHOwQA LFQA datasets. ELIS is the largest dataset in terms of questions and has the lowest average words in reference documents (recall that the single refer- ence document is an aggregate of top-k passage retrieval and hence the absence of some of the reported statistics within Table 1). The number of answer words is higher than other datasets ex cept for HOwSUMM-METHOD. AQUAMUSE has\nAvoiding Overlap in Train-Test: When studying state-of-the-art model performance on ELIS Fan\nImplementation details can be found in App: B\n6The full category distribution is provided in App. A\nfewer questions, although the reference sources and answer content are most similar t0 the statis- tics of HOWSUMM-STEP and WIKIHOwQA. The HowSUMM-METHOD dataset statistics are unique, it has 11 source documents per instance on aver age, and the length of answers are greater with an average of 31 sentences per instance_ HowSUMM STEP also has a high number of source documents while the remaining statistics follow a similar trend to the other datasets   described: While our pro- dataset WIKIHOwQA shares many common pposed statistical characteristics of other datasets, the con- struction process, source-target mappings and task applications differ as discussed in Sec. 2.\nFigure 3: WIKIHOwQA quality annotation\nQuality Verification 33\nCardie (2020) propose additional metrics to com- pare datasets, some of which are reformulation of those presented by Grusky et al. (2018).\nSince the relevant passages in our WikiHowQA datasets are sourced from web pages cited by the they WikiHow article author; may not contain the exact words Or phrases from target answers. There- fore, to verify the quality of our benchmark; we as- sess the feasibility of constructing an answer from the given supporting documents As this is a cru- cial aspect for a MD-NFQA resource, we evaluate our benchmark using both automatic metrics and thorough human evaluation.\nThe above metrics glve a general indication Of the dataset quality although even when examples from our dataset are deemed lower quality, for ex similarity, we they ample when have low semantic find that there are instances where an answer can typically still be constructed from the supporting documents. Fig: 1 demonstrates that answers can be successfully constructed from documents even they have low semantic similarity. For instance, if in the first sentence of an answer for the question pork tenderloin\" (Fig: 1), although How to cook the target length of 55 minutes was not mentioned in any document; it can be inferred by adding up roasting time of 30 minutes 0 one side and 25 minutes on the other side. Similarly, the suggestion \"flipping it occasionally\" in the last sentence can be rephrased from \"turning every 1 1/2 to 2 minutes\" To verify our observations Human evaluation: and ensure the quality of our dataset; we conducted crowdsourcing study? with the of verifying goal the feasibility of answer construction: Each partici- \nAutomatic metrics: The relevance of the support- ing documents in our WikiHowQA dataset is first evaluated using the Novel N-Gram Percentage met- ric (See et al ,2017) as a measure of word intersec- tion between answers and documents. This metric commonly used for summarization datasets, allows for a strong upper bound comparison. Results pre- sented in Table 2 show that the passage answers correspond to the content of the sup in our dataset porting documents with a relatively high n-gram percentage score, similar to that of HOWSUMM We also include results for the Docu- METHOD. Understanding Conference (DUC) as an up- ment per bound as it is a high quality human crafted sum reported by Fabbri et al.) marization dataset as\nSecondly; the report we average coverage density compression and metrics for WIKI- Grusky et al,, 2018). These metrics are HowQA commonly used to characterize the quality and dif- ficulty of summarization tasks. We follow Fabbri for et al. (2019) who adapted these measures the The coverage, density and com MDS use case. pression scores for WIKIHOwQA are 0.89, 1.86 and 81.46 respectively. It is important to note that Tejaswin et al. (2021) and Bommasani and\ncrowdsourcing studies were reviewed and ap- All our proved by the Human Research Ethics Committee of RMIT University: Surge Al as our data labeling platform We use which provides workforce of highly skilled and educated native speakers, ensuring high-quality data labeling at scale allowing for higher quality labeling compared to traditional platforms such as Mechanical Turk_\nAll differences are statistically significant (Student t-test, p-value 0.01)\nThis is consistent with previous two annotations_ similar research (Qu et al,, 2019; Bolotova et al 2020) and indicates that the annotators had good agreement with each other 0n the parts of the doc- uments that could be used for associated answer construction. Overall, these findings demonstrate that the provided documents are relevant and con- tain the necessary information for the answers.\nof which was the original category derived from the WikiHow website, while the remaining three randomly selected from the other 18 cate- were gories This task was designed to ensure that the paying attention and had read the annotator was question carefully. The tasks were rejected if the classification was incorrect. The second evaluation required annotators to read the answer and the sup- provided highlighting porting document, and use a tool to mark any words or phrases in the supporting document that were directly relevant to the given answer; Or could be used to reason Or conclude it In the final step of the process, annotators evalu- ated the relevance of the provided documents to the :Very five-point Likert scale (from 0, answer on a to 4, \"Very Relevant\" ) Irrelevant\" , In total, 31 randomly selected QA pairs and their associated supporting documents were annotated, resulting in being 104 participant tasks, with only four rejected: Each assessment was carried out by three workers, receiving compensation of SO.7 per completed task The study8 involved 22 fluent English speakers. Fig: 3 illustrates a randomly selected result of this annotation process, as captured in the (simplified) annotation interface used in the study:\nBaseline Models 4\ndescribes   WIKIHOwQA This   section baseline The task is to generate an models. answer to NFQ grounded in a set of relevant documents from which the answer can be reasoned or concluded.\nfor performance evalua- As a reference point Retrieval-Augmented Generation tion; we use the (RAG) (Lewis et al,, 2020b) architecture, which is a common choice for abstractive summarization and QA tasks Fan et al,,2019). Since our dataset small predefined set of relevant support- provides ing documents for each question, use a retriever we to filter out passages from those documents that are less relevant to the question, to help fit more relevant passages into the limited input of a gener- ator: Then; a generator is tasked with generating an answer conditioned on input passages. We ex perimented with DPR (Petroni et al,, 2021) as the BART-large (Lewis et al,, 2020a) as retriever and the generator:\nThe results showed that the workers deem the supporting documents to be mostly \"Somewhat Relevant\" in relation to the given answers, with a mean document relevance score of 2.59. The inner- annotator agreement is moderate, with a Cohen'$ Kappa (Artstein and Poesio, 2008) value of 0.51 indicating reasonable consistency in the annota- assessments of document relevance In terms tors of related text selections within the documents, the overlap score between assessor was pairs mean 0.65, calculated using the overlap coefficient (Vi- jaymeena and Kavitha, 2016):\nbuilding  model input; When we first ranked passages   from   supporting documents based on using their relevance to the question DPR, then truncated them t0 retain the maximum number of complete passages in the ranking that fit with- ing the maximum input length of BART: Passages were then reordered based on their source docu- string ments, and combined into a single input of the format <q>question<Iq><d><p>doc 1 pas ~lp><ld><d><p>doc sage 1<lp><p>. 2 pas Finally, BART is trained? to 1<lp> . <ld> sage\nwhere H1 and Hz are the sets of unique words from\n9Training and decoding parameters are in App D\nInterface with exact annotation instructions in App:\nevaluation experiment. crowdsourcing' To facili- generated this, a set of modified answers by tate we deliberately introducing factual errors into a sub- set of the WIKIHOwQA. was achieved by This randomly selecting 5 QA from each of the pairs 19 question categories available on WikiHow and systematically incorporating various types of inac- curacies, aS outlined in Table 4, into 4 out of the 5 answers, resulting in an average of 4.5 modifi- cations per answer: We then manually reviewed they the deteriorated answers to ensure maintained both contextual relevance and grammatical correct- Following this, we conducted two separate ness evaluation rounds involving a total of 34 workers who assessed all 95 QA pairs It's important to note that no worker participated in both rounds ensuring an unbiased evaluation.\ngenerate answers conditioned on this context_ gold\nTo establish a stronger baseline, we also evalu - ated the performance of the GPT-3 (Brown et al. 2020) variant text-davinci-00310 in a zero-shot set- ting: larger than As model at least 4OOx times BART; it forms an upper bound for model perfor- We compared two prompt kinds for GPT-3, mance one with the question and another with the question and relevant passages retrieved via DPR.\nStandard automatic evaluation metrics for ab- stractive QA, Rouge-X and BertScore, are reported However; these scores are known to in Table 3 poorly reflect actual model performance (Deutsch and Roth; 2021; Krishna et al,, 2021). While re- cent QA-based evaluation metrics for summarisa- tion seem to better correlate with human judge- ments (Deutsch et al,, 2021; Scialom et al,, 2021), their adaptation to NFQA is not straightforward and remains an important area of research, as an swers in NFQA are expected to contain facts not mentioned in supporting documents. Instead, we report the results obtained through our human eval- uation framework as a more accurate measure of model performance (Sec. 5.2).\nThe first trial aimed to establish the baseline per- formance of the standard evaluation framework, in which assessors evaluate answers without the assistance of any on-screen relevant information. Each HIT included one QA where the answer pair may had been deteriorated. Similar to Sec. 3.3 HITs began with the attentiveness test of question Assessors then indicated category classification. their familiarity with the question and evaluated five-point Lik- the usefulness of the answer on Unfamiliar\" / *Very Useless' ~Very ert scale (from Useful\"). Finally; they Very Very Familiar ' to were tasked with highlighting spans within the an- misleading, swer that they found either useful 0 understanding thereby providing a more nuanced of the answer' s perceived value.\nHow To Evaluate How-To Answers 5\nreliability of MD-NFQA eval- To further ensure the uation, simple human evaluation we introduce a framework, which we then employ to assess the performance of baselines in our benchmark.\nEvaluating Human Evaluation 5.1\nThe second trial featured the same evaluation provided except this time assessors tasks; were with highlighted relevant passages for each sen- tence in the answer; accessible by clicking on the sentence. Top-ranked passages according to DPR ensuring (Sec. 4) were selected for each sentence, the overall length is under 5000 characters to fit Passages were displayed in their on the screen. original order; grouped by source documents. Pas- sage highlighting came from the model proposed by Bolotova et al. (2020) , designed to simulate user gaze during NFQA evaluation: Assessors could freely examine the passages while determining the usefulness of the answer and selecting useful O misleading spans in the answer\nconventional human eval- Initially, employed we as described in previous stud- uation  approach; ies (Fan et al,,2019; Krishna et al,,2021), where presented with both model-generated assessors are standard randomized and answers in gold or - der and asked to select their preferred response However; our findings indicated that, even when presented with standard questions and evalu- gold ated by high-performing assessors, simpler model- generated frequently (489 of the answers were time) preferred over reference answers, despite con- taining factual inaccuracies. Manual inspection re- vealed that choosing between two well-formulated challenging for participants, in line answers was with previous research of Krishna et al.; Clark et al. To quantify the ability of annotators to identify fac- we conducted tual inconsistencies in answers\nThe choice of this presentation form was based on several factors. Firstly, it allowed for a more in-\nFor interfaces and implementation details refer to App. E\nOhttps: /beta.openai com/docs/models gpt-3\nTable 4: Examples of deterioration types\nteractive and engaging experience for the assessors enabling them to delve deeper into the context of each sentence. Secondly, it provided a clear visual cue to the assessors about the relevance of each sentence, thereby facilitating a more accurate and efficient evaluation process. Given that assessors challenges due to their unfamiliarity with often face the topic O potential distractions, we aimed to en- hance their ability to assess with higher accuracy by providing relevant passages as supporting infor- This approach; designed to be simple yet mation_ effective, prioritized the provision of relevant infor- mation over more elaborate feedback mechanisms tracking, which are potential ar- like clicks or gaze After comparing different eas for future research forms of passage presentation through trial runs and discussions among the authors, we found that offering easy accessibility by clicking o the form the sentence was the most effective.\nFigure 4: Inconsistency detection rate (IDR) by type\ntion of almost all deterioration types in the second trial. Notably, the detection rate for sentence nega- tions and number swaps increased by 131% and 158%, respectively: These increases were statisti- cally significant; as confirmed by Student's t-tests 0.01 for sentence nega- statistic (t 4.2,p 0.01 for number tions; t statistic = 3.18,p A detailed explanation of the answer de- swaps). terioration process, including examples, as well as the interfaces utilized in the human study trials, is provided in App. E.\nmeasure of the ability to spot factual in- As consistencies, we calculated the inconsistency de- tection rate (IDR), defined as the average percent- age of identified deteriorations deterioration A was marked as identified if it was highlighted as misleading span. Results demonstrate a part of a statistically significant difference (Student's t-test; 2.57, p statistic 0.01) in IDR between t = the first and the second trials, with the IDR scores of 0.21 and 0.35. In addition to this, we also cal- culated the number of False Positive IDRs. False positives were defined as answers that were marked being misleading by an asseSSOr; even though as khey The first trial included four false were not. positives, and the second trial two.\nHow Good Are The Baselines, Really? 5.2\nWe evaluated the effectiveness of lower- and upper- using bound models from Sec. 4 our human evalua- tion framework  Specifically, we compared models through A/B testing (Krishna et al,, 2021) while incorporating highlighted relevant passages as ref- After classifying erence sources of information. the question category as an attentiveness task, as- sessors were instructed t0 evaluate the usefulness of a pair of answers and one model gold one and select the more useful answer: Answers were presented in random order without disclosing the provided ranked relevant passages for source. We separately; each sentence in each al- answer to low participants to make more informed decisions about factual consistency:\nWhile question familiarity scores were consis- tent between the two trials, IDR improvement persistent and statistically significant (Stu- was 2.18,p dent' s statistic 0.05) t-test t in the second trial for both high and low famil- iarity: This suggests that participants were able to assess more effectively with an access to rele- regardless of their fa- vant information sources; miliarity with a particular question. As shown in general improvement in detec- Fig: 4, there was\nIn this experiment; 28 workers evaluated gold questions against an- answers to 100 random test BART; text-davinci-003 , and swers from DPR To eliminate potential text-davinci-003 . DPR bias, we ensured that workers did not evaluate the\nAcknowledgments\nquestion twice. Each HIT was compensated same with $0.5.\ndeepest gratitude to We extend Mr: Luke our Gallagher for his invaluable contributions to this study: His insightful comments and contributions We also wish have greatly enriched our work: to  acknowledge the financial  support  provided by the Australian Research Council (Grant No DP180102687), which has been instrumental in facilitating this research:\nHuman evaluation results are available in Ta- All differences are statistically significant ble 3. 0.01). In comparison with (Student's t-tests, p the initial attempt at human evaluation, the sim BART model scored lower; while still pler DPR being preferred over answers in some cases gold Unlike the automatic evaluation; human evaluation supports our initial observation that the quality of answers from text-davinci-003 is much higher than from BART: When prompted with supporting pas- sages, text-davinci-003 generates significantly bet- ter answers based on human judgement (Table 3), which highlights the value of reference documents While text-davinci-003 mainly in WIKIHOwQA. generated grammatically plausible and logicaly sound answers, our manual analysis revealed that it frequently failed at factual consistency and overall coherency, especially for questions that require step-by-step instruction. Coupled with the fact that preferred answers from text-davinci- assessors still proposed evaluation 003 in some of these cases, the framework should be further improved to guide as- The problem is judgement. sessors better in their twofold; first, provided relevant passages do not al- ways include the required information to fact-check the answer; second, the presentation and fluency of model answers misleads assessors even when the overall instruction is impossible to follow. This re- quires an additional skill-set for assessors, and we leave the research of a better evaluation framework specifically tailored to instructions for future work\nLimitations\npossible limitations of In this section we discuss our work; and present interesting avenues for future investigation: First, not all documents in WIK - equally useful; IHOwQA documents some are are overall less relevant, some contain very rele- vant bits alongside generally irrelevant information More accurate passage or span selection may be required for models to generate better answers_\nAnother limitation is the focus on highlighted aiding human relevant passages as the method of evaluators in assessing factual correctness. While this approach helped to identify some factual incon- sistencies and was proven to also make assessments faster in previous research (Bolotova et al,, 2020), additional techniques should be considered for their potential to deliver further improvements.\nUnfortunately, due to limitations of the evalua- tion interface used, we were not able capture how frequently annotators clicked on sentences to see aiding - passages. Click ratio data could have yielded valuable insights into when and how often asses- provided information. sorS referred to the\nWhile we considered a range of answer deterio- ration types, it does not fully align with the kinds of hallucinations that neural NFQA models may prO- duce in the wild. Further research is needed to anal- yse the robustness of these models in real-world One direction is to adopt the compre- scenarios_ hensive evaluation framework of abstractive neural models proposed by (Dou et al,,2022) for NFQA\nConclusion 6\nIn this work we have presented a new dataset and benchmark for multi-document non-factoid ques- tion answering, WIKIHOwQA, sourced from the WikiHow website and consisting of passage-level how to\" questions. Our released test answers to critical gap in currently collections fills avail- able resources and evaluation testbeds for multi- document non-factoid QA, a widely occurring in- We have also presented baseline formation task: performance benchmarks, and introduced and em- ployed an information-augmented human evalua- tion framework that improves the reliability of QA annotations. Still, manual failure analysis revealed remaining challenges in the evaluation of convinc- ing but factually incorrect model answers_\nexperimented with text In our baselines, we parsed directly from the HTML source code How ever; it is important to note that we also provide of using the Markdown files, which pre- the option formatting information such as tables and serve lists. This could be useful for certain cases where formatting is important:   Furthermore, some ref- erence HTML pages contain pagination and long comment sections that we did not consider when scraping the data.\nFinally, the evaluation of passage-level QA re- challenging task; both for human and auto- mains a matic evaluation frameworks. Difficulties include assessment of  factual correctness and accurate overall consistency, especially when answers re- quire complex reasoning based on multiple sources Advancements in automatic evaluation of similar tasks, such as summarisation, could be adopted for NFQA.\nOuCmd DOMI , Uuy relgenlal, Guy LCV NIICHal Shmueli-Scheuer and David Benjamin   Sznajder; Konopnicki. 2021. multi-document Howsumm: summarization dataset derived from wikihow articles_ CoRR, abs/2110.03179. Tom B. Brown, Benjamin Mann, Nick Ryder; Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam; Girish Sastry; Amanda Askell, Sandhini Ariel Herbert-Voss Agarwal, Gretchen Krueger; Tom Henighan, Rewon Child, Aditya  Ramesh;, M Ziegler;  Jeffrey Daniel Wu Clemens Winter; Christopher Hesse, Mark Chen, Eric Sigler; Mateusz Litwin, Scott Gray, Benjamin Chess_ Jack Clark, Christopher Berner; Sam McCandlish, Alec Radford, Ilya Sutskever; and Dario Amodei 2020. Language models are few-shot learners_ In Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12 2020, virtual. Jorg  Ricardo J. G. B. Campello, Davoud Moulavi, and Sander: 2013. Density-based clustering based on hi- erarchical density estimates_ In Advances in Knowl- Pacific-Asia Mining; edge Discovery and Data I7th PAKDD 2013, Gold Coast, Australia, Conference, April 14-17, 2013, Proceedings, Part II, volume 7819 of Lecture Notes in Computer Science, pages 160 172. Springer: Elizabeth Clark, Tal August; Sofia   Serrano Nikita Haduong; Suchin Gururangan, and Noah A Smith: Evaluating 2021. All that' s human is not gold: In Proceedings human evaluation of generated text: Meeting of the Association for of the S9th Annual Computational Linguistics and the Ith International Joint Conference on Natural Language Processing, ACLJIJCNLP 2021 , Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7282-7296. Associa- tion for Computational Linguistics. Daniel Cohen and W. Bruce Croft. 2016. End t0 end long short term memory networks for non-factoid Proceedings of the 2016 ACM question answering: In International Conference on the Theory of Informa- tion Retrieval, ICTIR 16, page 143-146, New York; NY, USA Association for Computing Machinery: Oren Kalinsky,  Yftah Ziser; Nachshon Cohen; and Alessandro Moschitti. 2021. Wikisum: Coherent summarization dataset for efficient human-evaluation_ Proceedings of the S9th Annual Meeting of the As- In sociation for Computational Linguistics and the Ith International Joint Conference on Natural Language Processing Volume 2: Short Papers) , pages 212-219, Online. Association for Computational Linguistics. Trang Dang: 2006. DUC 2005: Hoa Evaluation of question-focused summarization systems. In Pro ceedings of the Workshop on Task-Focused Summa- rization and Question Answering, pages 48-55, Syd ney, Australia. Association for Computational Lin- guistics\nReferences \n Now the paper ends. \nQuestion: What are some ways the WikiHOwQA dataset and human evaluation framework could be further improved?", "output": "Answer: The paper suggests several potential ways the dataset and evaluation framework could be improved:\n\n1. Passage selection - More accurate selection of relevant passages from the supporting documents could help models generate better answers. Some documents contain both relevant and irrelevant information.   \n\n2. Evaluation techniques - While the highlighted relevant passages helped evaluators identify some factual inconsistencies, additional techniques could provide further improvements. Data on how frequently evaluators referred to passages could provide insights.\n\n3. Answer deterioration - The types of answer deterioration studied do not fully align with the kinds of hallucinations neural models may produce. Further analysis is needed.  \n\n4. Evaulation framework - Adopting the comprehensive evaluation framework proposed by Dou et al. (2022) for non-factoid QA could help identify remaining challenges in evaluating convincing but factually incorrect model answers.\n\n5. Automatic evaluation - Advancements in automatic evaluation of similar tasks like text summarization could be adapted for non-factoid QA evaluation. This remains challenging.\n\nIn summary, improvements to passage selection, evaluation techniques, test data, evaluation frameworks and automatic evaluation metrics could all help make the WikiHOwQA dataset and human studies even more effective."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n ABSTRACT\nWorks have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are lim Most works pay attention to some common mathematical properties ited: new type of implicit model such as sparsity. In this work; we propose inspired by the designing of the systems hidden objective functions; called the Multi-branch Optimization induced Equilibrium networks (MOptEqs)_ The model architecture is designed based on modelling the hidden objective function for the multi-resolution recognition task. Furthermore; we also pro strategy inspired by Our understandings of the hidden objective pose a new the proposed model can better utilize the hierar function_ In this manner recognition tasks and retain the abilities for interpreting for chical patterns the whole structure as trying to obtain the minima of the problem s goal Comparing with the state-of-the-art models, our MOptEqs not only enjoys better explainability but are also superior to MDEQ with less parameter consumption and better performance on practical tasks: Furthermore, we also implement various experiments to demonstrate the effectiveness of our explore the applicability of the model's hidden objective new methods and function_\nINTRODUCTION\nWhereas Deep Neural Networks (DNNs) have achieved great success many real-world in tasks such as computer vision and neural language process, the limited interpretability of DNNs greatly hinders their further development.\nmany traditional machine learning methods, matrix recovery However Zhang e.g et al: 20T6) , subspace clustering deblurring Yo et all [0T6) , image 2018b; [20LS; Lin XLi; Liu and s0 0n, can be interpreted into minimizing the following objective functions: et_all [A)\nRmxd1 B \u20ac Rmxdz X \u20ac Rmxn and f(:) and g(: designed where A are convex functions \u20ac by the modelling of their properties. For this count, the interpretability of such methods is Furthermore, these methods can also enjoy state-of-the-art and much better than DNNs. robust performance We call such interpretability these methods enjoy as on these tasks mathematical interpretability\" whether the whole network structure can be summa- i.e. rized as a compact mathematical model that can be mathematically analyzed.\nExcept for these methods, the Optimization Induced Equilibrium Networks (OptEqs) pro 12U24) recover its whole system to an optimization problem. posed by KXieet The forward all \u20ac Rd for input x \u20ac propagation for OptEqs tries to solve Eqn Rdin m) to output Y,z get OptEqs' equilibrium equation; which is the central part We call the front part of Eqn (m) as regarded for equilibrium models solving the hidden And its forward procedure can be as objective functions shown as EqnC\nCorresponding author\nWc are learnable weights, 9 is the convolution layers projecting input x from real- W U, f is the outputs' penalty: world domain to feature space like other neural architectures; and\nAlthough OptEqs' interpretability is satisfying; it still remains some weaknesses worth ex- First, although OptEqs performs better than some implicit models, these models ploring: only consider a single view (Or resolution) of the input in their implicit parts. We call these models single-view implicit models in the following: However , nearly all the state-of-the-art pattern recognition systems 2009b; Wang et all [UT9a; Huang et al] 20E: He Leeet_all benefit from the multi-layer O multi-resolution feature Adelson, [984 et_all; [2U6; Burt Secondly; limited extractors in domains like computer vision and audio processing: new which mainly consider the math properties of features modules are proposed in their work; like sparsity; but whether these properties can benefit image recognition tasks is unexplored.\nconstructs its equilibrium equation z Besides OptEqs, MDEQ (Bai et all F(z*;x [2O2U and block F with the inspiration of explicit models especially HRNet Wang et al] ZUYa] Then they solve the equilibrium equation by the accelerated algorithm for output Al- though it shows the state-of-the-art performance with efficient memory cost as other implicit models, such strategies will make the model lose since the MDEQ is a black interpretability' -finding method) , which makes anal box because its output is solved in an implicit way (root- ysis 0n its features of intermediate layers almost impossible. Furthermore , its complicated mathematical systems. from structure also hinders its analysis More efficient blocks with interpretability still needs exploring: good\ndesign a multi-scale DEQ Motivated by the limitations of the above models we would like to structure with state-of-the-art performance and preserve mathematical interpretability from the inspiration of OptEqs and multi-scale models. Our contributions are listed below:\nmulti-branch implicit model, called Multi-branch OptEqs (MOptEqs) , We propose which efficiently utilizes different scales inputs with better performance and smaller model size. Furthermore; it still retains its connection to an optimization problem We propose some properties modelled as new terms for the model's hidden objective function from Our analysis on the relationships between branches and their hierarchi- With new problem's formulation; we obtain the fusion module in cal dependencies_ MOptEqs, called Hierarchical Heritage and Diversity Module (HHRD Module). our from design; Apart the new module we also propose the Perturbation Enhanced strategy PE) for our MOptEqs from OUr analysis on the model's hidden objective strategy cannot only enhance the performance of our MOptEqs; function_ The new but also improve the robustness of our models.\n1.1 RELATED WoRKS\nImplicit Models: Nearly all modern deep learning approaches use explicit models, which explicit computation graph for the forward propagation. In contrast, the compu- provide an tation graphs for O1 can be assumed as having implicit models are flexible infinite\" depth For example, Neural ODEs (KKhen encode their neural et_all; [208; Massaroli et al; [2U20 differential system Then the implicit ODE with learnable parameters architectures by solvers they used is equivalent to continuous ResNet taking infinitesimal steps: Further- more, the training process of Neural ODEs can be depicted as finding a differential system of a certain type (like heat equations by updating its learnable parameters which demonstrate that Neural ODE also enjoy interpretability to a certain extent_\nMoreover, DEQ is another class of implicit models_ Bai et all, [2T9; Winston X_Kolten [2020] The central part of DEQ is the designation of the equilibrium equations 2 F(z*;x) and = Its forward procedure is trying to solve the equilibrium equations with input x to block F.\nequilibrium state 2 as output by accelerated algorithms. Since the z the is fixed given Iget regarded as forwarding an explicit network stacked by the same F and x, its inference can be For example, DEQ chooses their F by one set of Conv+ReLU block F for infinite times_ while MDEQ constructs an HRNet-like block as theirs. However n0 evidence shows that the best block constructions in explicit models can perform the best in the DEQ scheme_ The construction of DEQ blocks O equilibrium equations is an open question worth exploring DEQ blocks do not have any mathematical insights (like the diffusion Furthermore, most process in Neural ODE) and perform totally black box with limited interpretability:\nSince implicit  models  usually adopt accelerated root-finding algorithms for their forward enjoy the advantages of constant and more efficient they outputs and backward gradients, memory costs compared with DNNs. Due to the above advantages; the design of implicit models draws much attention these days Apart from (Ghaouiet all; P20T9; Gould etall; [0T9) the models illustrated above; many kinds of other implicit models are proposed, including differentiable physics engines logical Qiao et all [2UZU; de_Avila_Belbute-Peres et all; [2TS) structure learning and implicit neural blocks Wang et all 209bl Liet_ all 2U2U)\nModel Interpretaiblity:  Many researchers nowadays are trying to make their proposed Although there are many kinds of approaches to achieve this method more interpretable. marginally divide them Empirical' two parts: Mathematical into and inter- Igoal, we pretability: Many works, such as Zhang et all 2USa; [Zhang [208; Bai etall, [28 Lhu attempt to empirically disassemble the black box by characterizing some statistical O struc- tural information like the outputs and gradients of neural networks' middle layers: However these works cannot be directly implemented on DEQ models since implicit ones do not have explicit depths like DNNs, which makes analyzing such models by dissecting each layer'$ Apart from that; researchers behavior almost impossible. DDjolonga & Krause 2UE4; AAmos also managed to understand the neural Kolter; [2UA; Kie etall; [20Y KChan et aLl; [ZUZU_ Kz architecture by linking it to a mathematical problem In this way, researchers can analyze the black box by its corresponding mathematical problem Furthermore, new components proposed due to the analysis of these problems_ Our model aims to design can be new DEQ architecture with proper mathematical interpretability. Compared with former works; our model can perform better on the classification tasks with novel components_\nMULTI-BRANCH OPTIMIZED INDUCED EQUILIRIUM MODELS 2\nTHE MULTI-BRANCH OPTEQs 2.1 THE PROPOSED ARCHITECTURE FOR\nwe first design an objective function for Our tasks to solve and then Inspired by OptEqs; MOptEqs' equilibrium equation use its first-order stationary base of conditions The as function is formed by summarizing several objective functions of different scales defined our for OptEqs (Details are stated in Appendix EJ) However , such model can not obtain satisfying results since these branches independent. we need to design For this count, are describing the dependencies of each branch in the hidden objective function; as some terms following paragraphs equilibrium equations for Ou1 shown in the Then we can obtain the MOptEqs' block F by analyzing the first-order stationary condition of MOptEqs and our designed optimization problem. our\nHierarchical Heritage Modeling: State-of-the-art explicit models for image tasks are ex plicitly structured into sequential stages and process different resolutions hierarchically He which implies that features should et_all; [26; Shelhamer etall; [ZEA; Lee et all; I2uuYa be extracted hierarchically. In other words; the posterior branches should inherit from their we call such property hierarchical heritage. prior ones, In which our  work going we are to formulate such correlation of neighbouring branches in the hidden objective function by adding the following inner-product term into its origin:\nPi:j Xsj where zi \u20ac Rdi Zj \u20ac Rdj \u20ac Rsi denotes the Average Downsample (its transpose can weighted nearest upsample) o Identity matrix suited to the shape of Zi regarded be as This term estimates the summation for similarity of i-th and and 2j (i < j O j = 1,i = L): corresponding channels  j-th branchs channels with the same channel index\nWhen zi and 2j are similar the inner product will be large. Otherwise, the result will become small,  which implies that the relationship of corresponding channels is weak: In this way; similarity of corresponding channels of the near branches Except for ensure the we can the relations between different branches' corresponding channels; the relationships between dis-corresponding channels are explored in the following section\nDiversity Modeling: In addition to the hierarchical relations for corresponding channels; 202 W) have shown enhancing the diversities between works [20T9; AAmada et all PPang et all branches can also improve the model's improvements.  Therefore, we also consider making different branches extract various features to improve the representation abilities To achieve goal, we add a diversity term in the objective function Eqn for i such < j\nwhere is the inverse vectorization operator converting the vectorized feature vec Zi R#xc R#x1 (zi) (k) Rd;*1 with C channels and denotes the to matrix in k- vec Since we have already considered the relationships th channel of i-th branch in practice. between the corresponding channels in the hierarchical term; we only tries to estimate the diversities of dis-corresponding channels of the i-th and j-th branches in this term As the diversity term goes small, the diversities between the branches become stronger\nof MOptEqs and its hidden problem. The architecture With the two terms we proposed, we can reformulate the hidden objective for our problem as follows:\ng(x) is the input feature for the raw input X, A > 0 is a hyperparameter and zi \u20ac Rdix1 are the final outputs of MOptEqs: We can choose our demand and f to constrain will Zi on influence model's activation function. If we choose f(x) = Ix > 0} to ensure the outputs We set D(ZL; ZL+1) = D(ZL,21) and to be positive, then the activation function is ReLU. H(ZL,ZL+1) = H(ZL,21 = Lto complete the loop. The two terms we added tries to when i make the corresponding channels of different branches correlated by maximizing the H term and enhancing the diversities of different branches by minimizing D term: We note that H handling different pairs of channels for branches they and D are not conflict since are\nAs the following proposition shows finally the equilibrium equations (Eqn for we get problem G. MOptEqs by calculating the first-order stationary conditions = 0 for our\nThe proposed multi-branch structure induced by Eqn can be depicted as Proposition M 1 2E*JT e RZI_1 d;  for the following equations: solving the equilibrium points z [zT* =\nAnd h(z) [h1 (21,ZL, 22 hi(zi,zi-1,Zi+1) T hL(zL, ZL-1,21) ' ] T and each hi is de- fined a8 the mapping from Rd; X Rd,+1 X Rd;-i to Rdi\nFigure 1: The structure of the MOptEqs and HHRD Module. Dotted lines in the figure denotes the upsampling; downsampling O identity operators to suit the size of each branch_ denotes addition operator denotes multiplication operator RD operator processes matrix by removing its diagonal, SIGN operator convert each element of a matrix to its sign and the di C is the channel number and Hi, Wi are the height and CHiWi with = width of the feature map_\nThe equilibrium points for the above formula are also the first-order stationary points for the optimization problem Eqn\nThe proof of the proposition is  listed in   Appendix [33 In this obtain the manner we proposed architecture MOptEqs, its forward propagation is equivalent to find the equilibrium points of Eqn(@) regarded as solving the stationary points of and such process can also be the optimization problem The whole structure is also shown in Figure m and (0) we also draw figures to illustrate the practical process of hi (HHRD) on its right (Figure m (b)). modelling in Section: EAs experiments. We also evaluate the effectiveness of such\nTHE PROPOSED PERTURBATION ENHANCED STRATEGY FOR MOPTEQs 2.2\nResearches Kong et all] 2021) have discovered that small 2020; Kie et all, [20ZU; [Tang et all generalization abilities and varieties of branches for perturbations can enhance the neural gradients use adversarial perturbations based architectures Most efficient works KXie on training process strategy such Yuille; [2N020) for such a trick: However will slow the need at least one more back-propagation in each training step for perturbations they since which is time-consuming: Unlike these methods, we can acquire more computation efficient minimizing objective function can be formulated perturbations because our models as an can replace the maximum problem (details in Appendix @ for designed. Then we we adversarial perturbation with maximizing our architecture's hidden  objective function G MOptEq's performance will go worse if G(z; g(x)) changes lot at g(x) since we assume neighborhoods since the forward propagation of MOptEqs is minimization G(z; g(x))\nFor Assumption well trained OptEqs with its objective function denoted as G(z; g(x)) can be correctly classified. If an input perturbation Iolle natural sample x which and following changes: can cause one of the\n1, it may lead the OptEqs to the wrong results with high probability: with Ll, L2\nFor the second part of the assumption, it's common sense and widely used in the analysis of adversarial robustness With the above assumption Zhang et all 2021; Liet_all, [2UZU) generate the perturbations by maximizing the objective function G(z; g(x)) for Ou1 we can 'implicit\" ensembling that regarded Since kind of MOptEqs: MOptEqs  can be our as parallels several equilibrium models, we decide to inject the perturbations acquired for the prior branch to the posterior one, like boosting to enhance the performance\nThen the hidden objective function of MOptEqs with G(z; g(x)) is shown as follows \nwhere &0 And the problem becomes a bilevel optimization problem The solution to the 0_ esign(UT W lower-level problem is 0; Since the perturbation can be obtained by Zi ) _ feeding the output of the activation layer 0(Wz+ Ug(x) +b) to the transposed convolution with weight Ui we call it reconstructed perturbation layer Compared with the adversarial gradients, be acquired directly by matrix- perturbations perturbations  based on our can vector multiplication instead of iteratively built by gradients. Thus; such process does not require much computation cost. Following the above steps, harmful perturbed we obtain direction for the prior branches and then added them to their posterior branches. We note that Our perturbation is added to the input feature g(x; instead of the raw input_\na well trained OptEqs natural sample x and Proposition 2 For\nThe proposition demonstrates if we choose the perturbed direction to be esign(UTW-1Tz), at least one of the changes for the G or z will be around ellU 2*Il1 and implies each W-1 may perform bad on the perturbed data according to Ou assumptions branch With the perturbation is useful experiments in SectionBZ; we can conclude that our reconstructed Like boosting strategy; we can feed the perturbed data and our assumption is reasonable. for the prior branch to its posterior branch to enhance the performance; we call such method PE). Following experiments also show that PE strategy as the Perturbation Enhanced can indeed enhance the performance of our MOptEqs.\nMODEL OPTIMIZATION AND FORWARD CONVERGENCE 2.3\nForward Propagation and Implicit Differentiation: Like other equilibrium models; the forward propagation procedure is solving the given equilibrium functions Eqn @ For this count; our model also enjoys the constant memory cost advantages as other DEQs: In our -finding algorithms to solve the roots z [zi,-2L] of the following problem work; run root- to reach the equilibrium states; which is the same as MDEQ (shown in Appendix @\nAs for the backward propagation; adopt the implicit differentiation method widely we also Instead of tracing the gradients during the used in Baietall; PZ; PT9; Khen etall PT8) forward propagation, the implicit differentiation method directly backpropagates through the equilibrium state using the Jacobian of To VzG +z at Z* For given loss L(z = and the gradients can be written as where y is the target_\nComparison of the models with single-scales\nEvaluation on CIFAR-1O for different models Parallel-OptEqs\" (POptEqs) Table 1: means OUT model is built without utilizing HH&D fusion (Stated in AppendixAJ) which is formed by paralleling several OptEqs with the method stated in the brackets for fusion: w/o PE means MOptEqs is trained without our PE strategy:\nAs for the root- -finding algorithm, Broyden method Anderson IBroyden 0965] we can use ~finding methods to solve Eqnl Method IAnderson, [965; Baiet all, [22I) or other root- for for the backward gradient. the equilibrium state and Eqnl\nForward Convergence: Like other implicit models, we make some constraints on param MOptEqs Te(z;x) (z here is {zi}L_1) be eters in order to make the whole a contractive For the MOptEqs without considering HH&D (Eqn [) the models can easily mapping: converge with IWillz <  we use the HH&D module, we need to choose But since 1. < will be appropriate. or less than & A to ensure the convergence, around We also proper conduct experiments to verify the convergence in Appendix [7\n3 ExPERIMENTAL RESULTS\nIn this   section; conduct experiments for the   image classification CIFAR- tasks we on Krizhevsky and 10,CIFAR-1OO ImageNette implemented the Py [2UY et al: on (Paszke et_all, [201Z) platform to demonstrate the effectiveness of our model: Torch Details are listed in Appendix Ea\n3.1 COMPARISON OF PRIOR IMPLICIT MODELS\nthe superiority of MOptEqs in aspects. First , In this  part decide to verify two we we build the single-scale MOptEqs and compare the empirical results with other single-scale we compare the experimental results with prior implicit models implicit models_ And then multi-scale inputs like MDEQs, the state-of-the-art implicit  model that utilize the with multi-resolution_\nImplicit Models with Single View In this part; we compare our MOptEqs with other single-view implicit models with  comparable Like OptEqs model sizes who uses   three first blocks   for their our   small MOptEqs  with three branches experiment , construct we whose outputs share the same size and the channel number C 32. Results in Tablem = (a) demonstrates that our MOptEqs structure can even enhance the performance for recog nition tasks   under the single-view Compared to the multi-branch model without cases fusion (POptEqs) , our MOptEqs' HHRD efficiently utilize the relationships between can different branches and lead the model to better performance. Our superiority also holds (non-parametric module) and compared with former multi branches fusion methods: sum (parametric module). Meanwhile, \"conv we can also conclude the effectiveness of our per turbation enhanced strategy from the table since its performance is the best_\nTable 2: Evaluation on CIFAR-1OO and ImageNette for MDEQ and MOptEqs.\nexperiments compared Implicit Models with Multiple Scales:  Moreover; we conduct handling   multi-scale MDEQ; with other models which Like inputs_ construct we our resolution size equals to 32,16,8,4. MOptEqs  with four branches  with Other details can be found in Appendix E From Table.m (b) and as shown in Tablep a) , one can see that MOptEqs not only outperforms the widely used explicit model ResNet-18 'Heet all our but also shows better performance compared with MDEQ with fewer parameters on 2016) CIFAR, which is one of the best models. The empirical results for the multi-scale models further verify the superiority of MOptEqs and its strategy:\nApart from experiments 0n small images, we also conduct experiments on ImageNette; which subset of 10 classes from ImageNet Compared with MDEQ, our MOptEqs consistently is Nevertheless, see from the results, the perform better shown in TableD (6). as one can difference between MDEQ and the CIFAR-1OO and becomes much bigger in model our Imagenette with the same training hyper-parameters_ phenomenon demonstrates Such that the performance of Our model is much from more stable than MDEQ Apart these experiments, we also conducted ablation studies for our models in the following section_\nMOPTEqs THE COMPREHENSIVE UNDERSTANDING 3.2 OF\nFigure 2: Visualization of the channels correlations for the MOptEqs POptEqs_ and\nVisualization of the impact for the HHRD module. In this section; we try to estimate We finish experiments in this part using three-branch the effect of our HHRD module. MOptEqs with 16 channels for each branch to make the visualization more clear.\nWe tries to verify our HH&D module s effect in two aspects First,; we plot scatter figures for ZL_1 Zjen(i) D(zi,zj) EL_1 Eje(i) H(zi,2j) (denoted (denoted as Average D) and the values of L(Cz LC Average H) for 100 randomly chosen samples (N (i) denotes neighboring branch of the as Since the hidden optimization problem is to maximizing H while minimizing i-th branch) (Figure 0 D; we find that our MOptEqs can induce the output features to reach such compared with POptEqs (MOptEqs without HHRD) Figure Db) ) Igoal\nFurthermore, heatmaps first also of the branches' correlation plot two we score (21)T| \u20ac R16x16) for randomly chosen sample shown in Figure Dc) (d) (22 _ vec -1 vec MOptEqs' heatmap diagonal matrix One can see that Figure D(c)) is likely to be more while heatmap Figure Dld) for POptEqs looks random The heatmap shows that our HH&D can induce the model to perform a8 OU demand, which means corresponding channels for ad jacent branches can be more correlated, while the dis-corresponding channels are unrelated due to our architecture design. The visualizations and former results on different datasets modelling and our HH&D modules' design. verify the effectiveness of our\naccuracy changes the perturbation size for Figure 3: Test with respect different to perturbed directions. (b) Test Accuracy for small MOptEqs (W and w/o. PE) under PGD for the small MOptEqs with different inner iterations Plot of the accuracies attack without HH&D trained by the extended loss with respect to various\nEvaluation of the reconstructed perturbation: We compare our perturbation with randomly generated ones in a one-branch well-trained MOptEqs to validate its effectiveness acquire output We first feed a natural sample x to the model and z for X and generate our reconstructed perturbation using such output and then feed it to the model. Furthermore we add the perturbations generated by Binomial distribution (P(6; P(6; ~e) =0.5 , \u20ac) = = and Uniform distribution U[_\u20ac, e] to input features g(x) for comparison. The result of each perturbation generated by distribution is averaged for five trials. Figure @C shows our maximizing G is much more effective; which validates the effectiveness of Ou1 reconstructed perturbations and the rationality of our assumptions and analysis in SectionPz\nRobustness of our MOptEqs trained by PE strategy: In addition to improving the generalization abilities for models as shown in SecBJ; perturbations with proper size may We conduct experiments also enhance the robustness of our MOptEqs, shown in FigureB(b). Our perturbation small MOptEqs (the same setting as SecBJs single-view  model) . on enhanced strategy goes stronger The figure shows that accuracy for the increases_ as model trained naturally drops quicker than trained by PE strategy: Overall.  trained with appropriate reconstructed perturbation can partly improve the robustness of our MOptEqs.\nfrom MOptEqs vs. adding Regularizers in training loss. Apart our HH&D modules adding D and H to the training loss is one ofthe most popular methods for obtaining outputs with certain properties. In order to demonstrate the superiority of our module empirically; experiments for our MOptEqs trained by cross-entropy and POptEqs (MOptEqs we conduct (w/o. HHRD)) trained by the cross-entropy loss adding Ci_1(H(zi, Zi+1) AD(zi, Zi+1)_ regularizers (we call it augmented loss). Then we drew its test accuracy for different as trained by the augmented loss (A is the same as ours, shown in Figure B(c)\nWith proper Y POptEqs trained by additional loss can perform better than Such = 0. the effectiveness of our HHSD Modelling: phenomenon demonstrates Furthermore, the figure demonstrates the superiority of our model since the traditional method is consistently We left some other explorations for Ou1 model in the Appendix: worse than ours.\nCoNCLUSION 4\nWe introduce the multi-branch optimization induced equilibrium models (MOptEqs) , a new extension of OptEqs that can utilize multiscale information for recognition tasks and retain its ability to recover to an optimization problem whose solution is equivalent to the equilib modelling the hidden The model architecture is designed based on rium states of our model recognition task: Furthermore; objective function for the multi-resolution we also propose strategy inspired by Our understandings of the hidden optimization problem: The new empirical results show the advantages of our proposed methods. The success of our HHRD link between the optimization problem and module and PE strategy demonstrates the deep neural architecture and may motivate further explorations_\nAcKNOWLEDGMENTS\nYisen   Wang is partially supported by the National Natural Science Foundation of China under Grant 62006153, Project 2020BDOO6 supported by PKU-Baidu Fund, and Open Re search Projects of Zhejiang Lab (No. 2022RCOABO5).\nZhouchen Lin is supported by the NSF China (No. NSFC Tianyuan Fund for 61731018) Project   2020BDOO6 supported by PKU-Baidu Fund, Mathematics (No. 12026606) , and Qualcomm_\nREFERENCES \n Now the paper ends. \nHow do the authors' proposed multi-branch optimization induced equilibrium models compare to prior implicit models and explicit models in performance, parameter efficiency and interpretability?", "output": "The paper discusses several key points regarding this question:\n\n1) In terms of performance, the proposed MOptEqs model outperforms prior implicit models like OptEqs and MDEQ on CIFAR-10, CIFAR-100 and ImageNette datasets. It also achieves better results than explicit ResNet models with fewer parameters. \n\n2) In terms of parameter efficiency, MOptEqs enjoys the constant memory cost advantage of implicit models while achieving higher accuracy. The paper shows that MOptEqs uses fewer parameters than MDEQ but achieves better performance.\n\n3) In terms of interpretability, MOptEqs retains the ability to be recovered as an optimization problem like OptEqs. The authors propose the hidden objective function and formulate the multi-branch structure based on modeling properties of the optimization problem.\n\nIn summary, the experimental results and analysis in the paper suggest that the proposed MOptEqs model achieves a good trade-off between performance, parameter efficiency and interpretability compared to prior implicit and explicit models. The hierarchical heritage and diversity modules as well as the perturbation enhanced strategy help MOptEqs utilize multi-scale information effectively for recognition tasks."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\n1. Introduction\nSingle Image Super-Resolution (SISR important is an computer vision task that reconstructs High-Resolution (HR) image from Low-Resolution (LR) image. With the advent of Deep Neural Networks (DNNs), lots of DNN based SISR methods have been proposed over the past few While in real-world usages, the years [6, 17,24,27,34]. resolutions of display devices have already reached 4K O Apart from normal 2D images, even 8K. the resolutions of omnidirectional images might reach even 12K or 16K SR techniques with large input are becoming Therefore, crucial and have gained increasing attention from the com- munity [4,12,18,28].\nWith the development of high-definition display devices; scenario of Super-Resolution the  practical SR) usually like needs to super-resolve large input 2K to higher reso- 4K/SK) To reduce the computational and memory lution the large input into local cost, current methods first split patches and then merge the SR patches into the output. These methods adaptively allocate a subnet for each patch. Quantization is a very important technique for network ac- celeration and has been used to design the subnets: Current methods train an MLP bit selector to determine the propoer they uniformly sample subnets bit for each layer However; for training, making simple subnets overfitted and compli- cated subnets underfitted. Therefore, the trained bit selector fails to determine the optimal bit. Apart from this; the in- troduced bit selector brings additional cost to each layer of the SR network In this paper; propose a novel method we named Content-Aware Bit Mapping (CABM), which can re move the bit selector without any performance loss  CABM also learns a bit selector for each layer during training: Af training, we analyze the relation between the edge in- ter layer: formation of an input patch and the bit of each We observe that the edge information can be an effective metric for the selected bit  Therefore, we design a strategy to build an Edge-to-Bit lookup table that maps the edge score of during inference. The bit con- layer patch to the bit of each figuration of SR network can be determined by the lookup tables of all layers  Our strategy can find better bit configu- ration, resulting in more efficient mixed precision networks: We conduct detailed experiments to demonstrate the gener- alization ability of our method The code will be released\nSince the and  computational will grow memory cost quadratically with the input size, existing methods [4, 12, 18,28] first split the large input into patches and then merge They the SR patches to the output: reduce the computational cost by allocating simple subnets to those flat regions while heavy subnets for those detailed regions. lusing Therefore_ how to design the subnets is very important for these meth- ods. [4, 18] empirically decide the optimal channels after lots of experiments to construct the subnets. [28] proposes regressor to predict the incremental capacity of to train they each layer: Thus adaptively construct the subnets can by reducing the layers. Compared with pruning the chan- layers, quantization is another promising technique nels or speedup. [12] trains an MLP bit selec and can achieve more layer patch. tor to determine the proper bit for each given MLP of each layer brings addi- However; the introduced tional computational and storage cost Besides, we observe 12] uniformly samples the subnets for training, mak- that ing simple subnets (low average bit or flat patches) tend to overfit the inputs while complicated subnets (high average bit or detailed patches) tend to underfit the inputs. There - fore, uniform sampling fails to determine the optimal bit for layer: each\nThis work was supported by the Fundamental Research Funds for the Central Universities (2022JBMCO13), the National Natural Science Foun- dation of China (61976017 and 61601021), and the Beijing Natural Sci ence Foundation (4202056). Shunli Zhang is the corresponding author:\nTo solve the limitations of 12], a novel propose we\nFigure The pipeline of our CABM method. pi \u20ac {pi}i=1. K is the probability of choosing ith quantization module and each quantiza- tion module uses different bit-width to quantize the input activation. During training, our method learns an MLP bit selector to adaptively While during inference, we use the proposed CABM to build an Edge-to-Bit lookup table to choose the bit-width for each convolution_ determine the bit-width with negligible additional cost.\n2. Related work\nmethod named Content-Aware Bit Mapping   (CABM), which directly uses a lookup table to generate the bit of each lookup table layer during inference. However; building is difficult since there are thousands of patches and cOrTe- We observe that edge information can sponding select bits_ Therefore, be an effective metric for patch representation_ we analyze the relation between the edge information of a and the bit of each layer: Inspired by the fact that a patch MLP selector learns the nonlinear mapping between a patch and the bit; instead of building the Edge-to-Bit lookup ta- ble based on linear mapping, we design a tactful calibration strategy to map the edge score of a patch to the bit of each The bit configuration of SR network can be deter- layer: mined by the lookup tables of all layers. Our CABM can achieve the same performance compared with the MLP se a lower average bit and negligi- lectors while resulting in ble additional computational cost: Our contributions can be concluded as follows:\nDNN-based  Image   Super-Resolution With the rapid development of DNNs, lots of DNN-based SISR methods have been proposed over the past few years_ SRCNN [6] the   pioneering   work that  applies DNNs is to the SISR Their network consists of three modules including task: non-linear mapping, and image recon- feature extraction_ The following works mostly follow the network struction_ design of SRCNN and improve the performance of SISR: For instance, VDSR [17] proposes to use a very deep neu- ral network to predict the residual instead of the HR im- age. SRResNet 19] introduces the residual block proposed to SR network and improves the perfor- by ResNet 11 EDSR [24] finds that the BN layer will impair the mance. SR performance and removes it from the structure of SR- ResNet; further boosting the SR performance. RCAN [34] deeper net- uses the attention mechanism and constructs Real-ESRGAN [29] extends the powerful works for SR They ESRGAN [30] to real-world blind SISR. introduce high-order degradation modeling process to simulate com- plex real-world degradations. USRNet [33] proposes an unfolding network that leverages both end-to-end trainable learning-based methods and model-based methods_ There - they can handle the SISR problem with different scal- fore, ing factors, blur kernels, and noise levels under unified framework:  SwinIR [23] is a strong baseline model that in- troduces Swin Transformer [25] t0 image restoration Their non-linear mapping module is composed of several resid- To reduce the computational ual Swin Transformer blocks.\na novel method that maps edge informa- We propose tion to bit configuration of SR networks, significantly reducing the memory and computational cost of bit se- lectors without performance loss_ tactful calibration strategy We present to build the Edge-to-Bit lookup tables, resulting in lower average bit for SR networks We conduct detailed experiments to demonstrate the generalization ability of our method based on various SR architectures and scaling factors.\nTable 1. The comparison of uniform sampling and BitOPs sam- pling: Feature Average Bit (FAB), PSNR, reported SSIM are U and B denote uniform sampling and for EDSR on Urban[00. BitOPs sampling respectively:\ncost; there are also many efficient SISR methods_ For ex- ample, ESPCN [27] invents the pixel-shuffle layer to obtain the HR output given the LR input: LAPAR [22] presents linearly-assembled adaptive regression method based on Restormer [32] proposes an efficient Transformer network designs in the building blocks key model by making several All of those methods train one the SR model on large-scale image datasets such as DIVZK [1] and test on the given in- they large in- put images_ are not desig for However; gned as the practical scenario of SR usually needs to super- put resolve large input like 2K to higher resolution (4K/SK):\nSR networks.\n3. Method\nSingle   Image   Super-Resolution with   Large   Input With the development of display devices, the resolutions of monitors have reached 4K or even 8K. Recently, there super-resolving large input to a higher are some methods of resolution: Since the computational and memory cost grows quadratically with the spatial resolution, recent methods all input into local patches and merge the SR large split the patches to the output: ClassSR 18] is the seminal work that They explores the problem of SISR with large input: use difficulty classification network t0 choose the restoration of each patch and allocate the optimal subnet for SR. How storing ever; ClassSR has two limitations. Firstly, it requires all the subnets. Secondly, the classification network brings additional computational To solve the above limi- costs tations, APE [28] proposes to train supernet for weight sharing: predict the incremental ca- They use regressor to pacity of each for the patch: ARM [4] also trains a supernet for weight sharing, however their subnets are constructed by reducing the channels instead of layers. CADyQ 12] uses network quantization to design the content-aware subnets They train an MLP bit selector to determine the proper bit input patch: layer for each based on the content of an How brings additional computational ever; the MLP bit selector Besides, the selected bit is not optimal and storage costs_ sampling: due to the uniform\n3.1. Preliminary\nIn this section, we first introduce the background of net- work quantization since quantization to our method uses given input activation construct the subnets_ For I, the quantized output %g can be formulated as:\nclamp(x) (min(x,0) , a) is the clamping where max function that uses trainable upper bound to limit the range of input and T is the mapping function that (n) 2n=T symmetrically scales the input to low bit output The quan - Different from tization of weights is similar t0 activations quantize weights fol- activations, we use fixed bit-width to lowing existing methods [12, 21]. quantized model, For the complexity can be measured by the number of opera- tions weighted by the bit-widths (BitOPs).\nCW* Our CABM method builds for  SISR supernet tasks, where W* denotes the quantized weight and T de- notes the activation bit configurations obtained by the Edge- to-Bit lookup tables. All the subnets share the same weight Ct{p) of supernet and each subnet can be represented as W * where ts(p)   represents bit   configuration  given certain patch p and the edge s( . determines which subinterval belongs to. score of p\nNetwork Quantization Network quantization is very They effective technique to accelerate the speed. map the 32-bit floating values of feature and weight to lower point bit values [3,5, 9, 16, 20, 37]. Recent works also propose to allocate different bit-widths to different layers [7, 8,31]. However; these works are mainly focused on high-level vi- sual understanding tasks such as image classification. Dif- ferent from high-level tasks, super-resolution is more sen- sitive to the network quantization PAMS [21] proposes to train the learnable upper bounds for quantization due to the absensce of BN layers DAQ [13] uses different quan- tization parameters for each feature channel: DDTB [35] novel activation quantizer to accommodate the presents asymmetry of the activations. CADyQ [12] designs mixed precision subnets for the input patch and uses MLP selectors to determine the bit configuration. However; the introduced brings additional computational and storage MLP cost t0\n3.2. Motivation\n12,26] have introduced additional mod- Recent works ules such as MLP to adaptively determine the network quan - Based on the results of MLPs, we find that MLP se- tization_ lectors usually choose bit configurations with high BitOPs for those patches with high edge scores However; we notice that MLP selectors sometimes choose low BitOPs for those patches with high edge scores as shown in Fig: 2. Based on this observation, we realize that the bit configuration de- termined by MLP selectors might not be optimal. This is because recent methods uniformly sample the subnets for training, which makes simple subnets tend to overfit the in- put while complicated subnets tend t0 underfit the input:\nFigure 3. The illustration of Edge-to-Bit mappings_\n(LR) images, {hn}n=1.. are the low-resolution H K training process HR) images. high-resolution are the The MiVv is to balance the overall SR performance and the of BitOPs of each subnet: Since this part is not our contribu- For the details, we tion, we simply summarize the process_ refer readers t0 the recent works 12,26].\nBuild a Supernet with CABM As illustrated in the mo tivation, the uniform sampling makes simple subnets over- fitted and complicated subnets underfitted. Therefore, the optimal for the bit configuration of MLP selectors is not given input patch: MLP selectors use two kinds of infor- mation to decide the bit for each layer:   Among them, the standard deviation represents the feature importance of the layer while the edge score of is constant current patch For different patches, the importance of through layers. lay- might be different  However; for patches with the same ers edge score, the layer difference is almost negligible. This inspires US to build Edge-to-Bit lookup tables for determin- ing the optimal bit configuration.\nFigure 2_ The motivation of our method. The darker color in the image means higher BitOPs Edge Score. As can be seen in (a), patches with high edge scores often correspond to the bit config- they urations with high BitOPs. However; (b) shows that are not always positively correlated, indicating the MLP selectors might fail to find optimal bit configurations\nTo further demonstrate the problem of uniform sampling; experiment that samples the subnets accord we conduct an ing t0 the BitOPs. We define three levels of difficulties for easy, medium; and hard. subnets: The probability of sam- pling each type is calculated as follows.\nspecific, for all LR patches o the valida- To be more we compute the edge scores denoted tion set of DIVZK, Mw {ei}i=1. as E Then to generate the use we 0- corresponding bit configurations for all the patches. As- sume that the edge detection retains precision F, e.g. 0.01, we can then split the edge score interval [0, max(E)] to 10 mac(E)AF subintervals S R {sr}r=l__R S0 that 5. F rth subinterval can be defined as:\nIr  denotes the probability of choosing mth level, where BitOPs(:) calculates the BitOPs of a subnet; Nm indicates the number of samples belongs to each level. As shown in Tab: sampling based on BitOPs achieves better per- 13 formance compared with uniform sampling  Therefore, we believe the bit configuration determined by MLP selectors optimal. is not In this paper; a novel method we propose named Content-Aware Bit Mapping (CABM) to choose the optimal bit configuration for SISR networks_\nGiven a patch p with its edge score D(p we can determine the index of subinterval for D(p) according to the R subin- tervals_ We denote the process that determines the subin- For terval of p as s(.). those subintervals without corre- configura- sponding bit configurations, we choose the bit tions from the nearest subintervals for them. Therefore, we simple mapping between edge scores and bit can build configurations using M;.\n3.3. CABM Supernet Training\nTrain a Supernet with MLP Selectors We first need t0 train a supernet that can adaptively generate all the bit con- Following existing meth- figurations for model inference. 12,26], we introduce a supernet MiN that uses MLP to ods automatically decide bit configurations for various inputs. We choose the standard deviation of each layer's feature and the edge score as the input of each MLP selector:  Specifi- {L, H}, where L cally, given a training {ln}nz1__K set =\nHowever; one subinterval might correspond to thousands\nTable 2_ Quantitative comparison of full precision models, PAMS, CADyQ and our method on Urban100, Test2K and Test4K Computa- tional complexity is measured by BitOPs of the backbone network for generating a 720p/2KIAK image accordingly: Feature Average Bit ISSIM results are also reported for each model. The scaling factor is x4_ (FAB and PSNR dB\nTable 3 Quantitative comparison of full precision models, PAMS, CADyQ and our method on Urban100, Test2K and Test4K Computa- tional complexity is measured by BitOPs of the backbone network for generating a 720p/2KIAK image accordingly: Feature Average Bit (FAB) and PSNR (dB ISSIM results are also reported for each model. The scaling factor is x2_\ndesign simple yet effective strat- of bit configurations_ We one optimal bit configuration for each to determine egy subinterval We observe that high bit widths retain more information of features and improve the accuracy of quan- tized models. However; we need t0 minimize the computa- using low bit widths. Therefore, we first sample tional cost the bit configuration with minimum BitOPs for each subin- a small number of patches within However; for terval. certain precision range, the bit configurations determined by MLPs may fall into the local minimum. Our solution to this problem is expanding the range of subintervals. Thus\neach expanded subinterval can contain more bit configura- tions. The adjusted subintervals can be formulated as:\nhyper-parameters. In this way; we can where Ae and 6 are find one optimal bit configuration by choosing the bit con- figuration with minimum BitOPs for each expanded subin- We build the optimal one-to-one Edge-to-Bit lookup terval: tables T = {t,}r_1__R to reduce the BitOPs of MLP selec-\nTable 4 Ablation study of CABM Fine-tuning: FAB, PSNR (dB), and SSIM results are reported for each model on Setl4 and Ur- ban[00 datasets_\nFig: The process of CABM is illustrated by After 3_ tors. building the Edge-to-Bit lookup tables with CABM to con Cw\"> simple fine-tuning struct the supernet we conduct for the CABM supernet:\nIli  denotes the commonly-used C1-norm in the where {Wp}p=1 - {ln}n=1.. SISR task; and lp Ln \u20ac \u20ac .P respectively the {hp}p_1_P \u20ac {hn}n=1.B are hp hn = LR image patch and HR image patch in the current training Bis the batch size and Pn is the patch number for iteration is initialized from the weight W of Mf - image. W each\nand Test4K [18]. In Testzk and Test 4k; images are gener- following previous work from DIVSK datasets (index ated 1201-1400) [10]. Without special mention, all test input im- scaling factor x4. ages are split to 96x96 LR patches with respectively set to Ae For EDSR and SRResNet; and are For CARN and IDN, they are set to 10 and 10 and 9000. 6000.\n3.4. CABM Supernet Inference\nDuring inference, we first split the whole large input into local patches of Laplacian given size, and then use the edge detection operator to calculate the edge scores. Based on the edge scores, we can quickly obtain the corresponding subnet for each patch from CABM supernet: After super- resolving all the patches; we merge the SR patches to the output: Compared with MLP selectors, CABM can achieve lower BitOPs with negligible additional computational cost. The whole pipeline of CABM is shown in Fig:\n4.2. Quantitative and Qualitative Results\nfully To prove the effectiveness and generalization of our proposed CABM method, we compare our results with full precision models, PAMS [21] and CADyQ [12]. PAMS is SISR quantization method, which uses fixed bit-width for CADyQ uses MLPs to adaptively adjust the bit- network widths according to the input patch similar to our method using the official All the models are trained by ourselves codebases and instructions to avoid unfair comparison. As shown in Tab: 2 our CABM method reduces the compu- tational overhead and achieves the accuracy as full preci- sion models 0n scaling factor x4. For SRResNet, CABM outperforms the full precision model by 0.11 dB (PSNR) and 0.008 (SSIM) with only 2.8% BitOPs. As for IDN it is obvious that MLP bit selectors fail to learn the opti- mal bit configurations for the quantized models Besides, the introduced MLPs brings additional computational cost to SR networks. Compared with them; our CABM is much more efficient when the BitOPs of quantized models are low bring enough since the lookup tables negligible additional computational cost We also show the comparison on scal- ing factor x2 in Tab. 3. Although our method fails to achieve with full precision models, competitive results compared our method obtains similar results as PAMS and CADyQ shows the qualita- while our FAB is much lower: Fig: our CABM method achieves tive results_ To summarize, almost lossless model performance compared with PAMS and CADyQ while significantly reduces the computational overhead.\nExperiment 4.\n4.1. Experimental Settings\nthe  generalization effectiveness To   verify and of our method, we conduct detailed experiments on four represen For plain models, we choose two widely used tative models_ SISR models, ie EDSR [24] and SRResNet 19]. Since the quantization for activations mainly affects the flow of information between blocks, it is not enough to apply our Therefore, we apply CABM to CABM models_ plain to IDN 15] and CARN [2] that have a hierarchical feature leaning- Besides, we notice that the extraction mechanism_ based quantization method we choose performs better when bit-width is greater than 4, and Tensor Cores mainly sup port 4/8-bit, so it is reasonable to select 4/6/8-bit as our quantization details of activation and candidate bits. As for weight; we follow the settings of previous works [12,36].\nImplementation Details All models trained are on DIVZK datasets [1] which contains 800 images for train- testing: 100 images for validation; and 100 images for ing; The lookup tables are built based on the validation set uS ing the proposed CABM method and the precision of edge -tuning; scores is F 0.01. For CABM fine- we use the pre- For testing; we use the trained weight from supernet M{V. and Structural Similar- Peak Signal-to-Noise Ratio (PSNR ity (SSIM) as the metrics to evaluate the SR performance of all methods on three test datasets: benchmark 14], Test2K\n4.3. Ablation Study\nCABM Fine-tuning  After the MLP  supernet we get MA and generate our CABM look-up tables, one straight- forward choice is using the original weight W to build\nFigure 4 a) (b) Qualitative comparison 0n two images from Urbanl00 with EDSRICARN (c) (d) Qualitative comparison 0n one image As can be seen, our method can achieve similar performance while reduce the FAB_ from Test4K with SRResNet_\nThe comparison of different calibration settings on EDSR Table 5 FAB, PSNR (dB), and SSIM results are reported for with CABM_ each setting on UrbanlO0 datasets_\nCABM supernet CW_ the proposed method con- Instead fine-tuning after CABM: To verify the effectiveness duct the of CABM fine-tuning, we show the results in Tab: The 4_ experimental settings are consistent with our main results. fine-tuning, CW fails to achieve reasonable results Without after removing MLPs. Especially for IDN [15], a network with dense information, the PSNR value is 1.26 dB lower This further provides experimental support to Our motiva- tion in Sec. 3.2 that bit configurations and edge scores are Therefore, not always positively correlated. in order to layer; fine-tuning is necessary after CABM: adapt each\nDifferent Calibration  Settings To evaluate our  pro- posed CABM method in Sec 3.3, we conduct more exper- settings.  Specif- iments to evaluate the impact of different ically, we choose EDSR as our backbone and the B is set to 9000 with edge precision F' 0.01. Tab: 5 reports the quantitative results of different expanding settings. As we have mentioned, the results obtained by MLP selectors are optimal due to the uniform subnet sampling: often not As we increase the expanding range Ae, the possibilities of finding better solutions and worse solutions increase at the By analyzing the results of expanding ranges, same time.\nempirically set the optimal Ae for the proposed we can Fig: CABM method_ compared the visual In we have 5 results of the mappings before and after Our strategy, and As can be seen, both mappings can Ae is set to 80 here. achieve similar performance, strategy can cali- while our brate the mapping and further reduce FAB. This demon - strate that our calibration method can improve the original one-to-many mapping and reduce the computational cost.\nBit Selection Strategies Our method expands the range of subintervals and select one bit configuration with mini-\nFigure 5. Quantitative comparison On an image in Test4K with backbone network EDSR. Comparison between patches before and after calibration, and the corresponding bit configuration of different layers. (b) FAB heat maps before and after calibration.\n5. Limitation\nThe comparison of different strategies for bit selecting Table 6. on EDSR with CABM. FAB PSNR (dB), and SSIM results are reported for each model on Setl4 and UrbanlOO datasets_\nAlthough CABM can further reduce the computational overhead of existing SISR methods, our method still has some limitations For example, our network uses mixed pre- cision quantization, which requires specific hardware sup port to achieve practical speedup. Besides, applying CABM to Video Super-Resolution (VSR) is not a trivial task since VSR needs to consider the temporal information and has more modules compared with SISR. Mixed precision quan- tization of VSR networks is still unexplored to the best of knowledge. These problems will be our future works our\nmum BitOPs for each subinterval. We denote our minimum sampling strategy as S]. The other two alternatives are sam- pling one bit configuration with maximum BitOPs (S2) and randomly sampling one bit configuration (S3). To evaluate analyze these bit selection strategies, and compare Sl we We fine-tune different supernets CW- and de- S2 and S3. Cw-, CW and CT3 note them as where T1, T2 and T3 W * are different look-up tables generated by S1, S2, and S3 re- CW  achieves spectively: It can be observed from Tab. 6 that the best performance. However; from the perspective of bal- ancing performance and computation, CW_ may not be an appropriate choice since the PSNR on Setl4 is only 0.02 dB higher than CW- while the FAB is 0.77 higher:  Therefore, we choose the S1 strategy for the proposed CABM\n6. Conclusion\nTo summarize, a novel Content-Aware Bit we propose large Mapping (CABM) method for SISR with input: Ex isting methods learn the MLP selectors to determine the bit they configuration for uniformly given patch: However; the subnets and fail to obtain optimal bit configura- sample using the MLPs, CABM builds the Edge- tion_ Instead of to-Bit lookup tables to determine the bit configuration. In order to further reduce the computational cost, we present a novel calibration strategy to find a better mapping between edge score and bit configuration Our method can achieve performance as existing methods with negligible ad- similar ditional computational and storage cost:\nReferences \n Now the paper ends. \nWhat are potential limitations or disadvantages of the proposed CABM method?", "output": "Potential answers:\n\n1. CABM targets mixed-precision quantization networks which require specific hardware support to achieve actual speedup. Without hardware support, CABM may not result in real acceleration. \n\n2. Applying CABM to video super-resolution tasks is non-trivial as VSR needs to consider temporal information in addition to spatial information. CABM has only been evaluated on single image super-resolution.\n\n3. CABM relies on edge detection to determine bit configurations. The edge detection may not be very accurate, especially for noisy or distorted images. This could lead to suboptimal bit configurations.\n\n4. The lookup tables used in CABM need to be rebuilt for different network architectures and scaling factors. This adds additional training overhead compared to methods that directly predict bit configurations.\n\n5. The calibration strategy used to refine the Edge-to-Bit mappings may not work well for all types of images and tasks. Better calibration strategies could be explored to improve the performance of CABM.\n\nIn summary, while CABM shows promising results for efficient SISR networks, there are still some limitations that need to be addressed to make CABM a more general and robust method. The potential directions include hardware support, extension to video super-resolution, more accurate edge detection, and refined calibration strategies."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nlanguage processing [7] tasks This state-of-the-art perfor- huge representation mance is mainly attributed t0 the ca- According to the Kolmogorov pacity [2] of DNNs. SU- perposition theorem [14] and the universal approximation DNN is capable of approximating theorem [29], uni- formly any continuous multivariate function with appropri- large To achieve better performance, weights. ate num- ber of parameters and computations assigned t0 the are which  seriously limits its   application on DNN 10, 40], memory- and computation-constrained devices. Hence, nu- approaches have been proposed to compress and ac- merous including pruning [26, 38], quan- celerate neural networks, and neural architecture search [34,37]. tization [35,41-\nnew family of deep neural networks, We introduce instead of the conventional representation of net- where work layers as N-dimensional weight tensors, we use along continuous layer representation the filter and chan- nel dimensions We call such networks Integral Neural Net- In particular; the weights of INNs are rep works (INNs) resented as continuous functions defined on N-dimensional and the discrete transformations of inputs to hypercubes; the layers are replaced by continuous integration opera- During the inference stage, tions,  accordingly our con- tinuous layers can be converted into the traditional tensor via numerical integral quadratures: representation Such kind of representation allows the discretization of a net- arbitrary size work with various discretization to in- an This approach can be ap- tervals for the integral kernels: plied to prune the model directly on an edge device while suffering only a small performance loss high rates of at structural pruning without any fine-tuning: To evaluate the practical benefits of our proposed approach, we have con- ducted experiments using various neural network architec - Our reported results show that the tures on multiple tasks: proposed INNs achieve the same performance with their being able to pre - conventional discrete counterparts, while approximately the same performance (29 accuracy serve loss for ResNetl8 on Imagenet) at a high rate 30%) (up to of structural pruning without fine-tuning, compared to 659 accuracy loss of the conventional pruning methods under the same conditions. Code is available at gitee.\nDNNs are particularly successful in dealing with chal- lenging transformations of natural signals such as images or audio signals [27]. Since such analogue signals are in- evitably discretized, neural networks  conventionally per- form discrete representations and transformations, such as matrix multiplications and discrete convolutions_ However; for such kind of representations the size of neural networks cannot be adjusted without suffering severe performance training degradation during the inference stage, once the procedure is completed. Although several network prun- ing methods [26, 38] have been proposed to extract crucial channels from the trained model and generate efficient mod- they significant accuracy degrada- either suffer from els; require to fine-tune the model tion or on the whole train- development of hardware, Along ing database with the there have been diverse edge devices with various capacities for memory and computation, from ordinary processors to dedicated neural network accelerators.  The model size for different devices varies significantly [4]. Moreover; many driving) require different response (e.g: tasks autonomous speeds on the same hardware according to various scenar- driving speed and weather condition). (e.g: ios or conditions The conventional way to deal with such problems is to de- sign multiple model architectures for all possible scenarios and store them together: However; the downside of such\n1. Introduction\ndeep Recently, neural networks (DNNs) have achieved wide range of practical ap impressive breakthroughs in plications in both computer vision 13,20,32] and natural\nThe authors contributed equally to this work:\nCurrently affiliated with Garch Lab\nleads t0 continuous-width neural networks In [24] the au thors proposed deep neural networks with layers defined as functional operators. Such networks are designed for learn ing PDE solution operators, and its layers are continuously parameterized by MLPs only along the kernel dimensions investigated in terms of training on A re-discretization was smaller data resolution and testing o higher input resolu- tion. However; the proposed framework in [24] does not in- clude continuous connections between filters and channels dimensions\nstrategy is that it requires huge resources for training and Hence, it is crucial to design memory space for storage. self-resizing ability during neural networks that feature inference, while preserving the same level of performance.\nInspired by the inherently continuous nature of the in- signals, we challenge the discrete representation of neu pput ral networks by exploring a continuous representation along the filters and channel dimensions. This leads to a new class of networks which we refer t0 as Integral Neural Networks (INNs). INNs employ the high-dimensional hypercube to present the weights of one layer as a continuous surface Then, we define integral operators analogous to the conven- tional discrete operators in neural networks_ INNs can be converted into the conventional tensor representation by nu- merical integration quadratures for the forward pass. At the inference stage, it is convenient to discretize such networks into arbitrary size with various discretization intervals of the integral kernels. Since the representation is composed of in- tegral operators, discretizing the continuous networks could as the numerical quadrature approximation be considered procedure [9]. The estimated values with various discretiza- tion intervals are close to the integral value when the inter val is small enough. Hence, when we discretize an INN with different intervals to generate networks of various sizes, it capable of preserving the original performance to some is extent without the need of additional fine-tuning: Such kind of representation of neural networks can play a crucial role in dealing with the important problem of efficient network deployment in diverse conditions and hardware setups.\nStructured pruning of neural networks Structured net work pruning aims at reducing the redundancy of a model by removing channels with negligible impact o its per The authors in [33] were the first to propose formance. structured sparsity learning (SSL) method to regularize the structures according to the magnitude of weights. However weights do not always play a less informative smaller norm role in the performance. Hence, [38] explored the valid cri teria using a direct simplification of the channel-to-channel computation graph. Further; in [26] collaborative chan- nel pruning method that takes the inter-channel dependency into consideration was introduced. On the other hand, loss pruning methods [17, 23] have also been widely in aware However; pruning methods re vestigated by researchers quire network fine-tuning to improve the accuracy of small To solve this issue, in the next section we propose networks underlying represen- self-resized DNN with a continuous which allows network compression without the need tation, of any fine-tuning:\nTo evaluate the performance of INNs, extensive exper image classification and super iments were conducted on The results show that the proposed contin - resolution tasks. performance with their discrete uous INNs achieve the same training procedure is finished. DNN counterparts, when the Moreover; such kind of networks approximately preserve the performance at a high rate of structural pruning without the aid of additional fine-tuning:\n3. Neural Networks and Integral Operators\nThe    intuition behind proposed idea is based our on lay- the observation that fully-connected and convolution ers could be considered as numerical integration of specific we consider the following ex integrals To illustrate this_ Let W(x) , S(z) be univariate functions, then ample_ we have 11]:\n2. Related Works\nW(z) S(x)dz ~ qW(Ti)S(Ti) = w4 - s , i_0 W(zn)) , (qoW (xo) , Wq where s Yn (S(zo), S(zn)) , q (qo; are the weights = Yn the integration quadrature, and Pr (co, is the Tn following inequality: segment partition that   satisfies  the The 0 1_ 8o 81 Tn In-1 Pr is called a numerical integration method [16]. pair General built   using numerical methods integration are different approximations of input functions we refer t0 the examples depicted in Fig: 2). From Eq: (1) we can see that the integral of product of two univariate functions can approximated by the dot product of two vectors using be\nContinuous representations Continuous parameter rep resentations have already been utilized in deep neural net- work architectures. In [31], the authors propose to use con tinuous convolutions t0 extend CNNs for non-uniform grid [30], the weights of convolution layers were con- data_ In tinuously parameterized in each section defined by the filter Such parameterization allows index and the channel index. for a flexible image upsampling but not pruning, since fil- ters and channels are not connected by a continuous func- In [5] the authors proposed continuous-depth neural tion_ networks based on ordinary differential equations. The step lay- size of the numerical ODE solver defines the number of In our work, i.e , depth: we have constructed the in- ers, tegral neural networks based on Riemann integrals, which\nVisualization of different channels selection methods without fine-tuning compared with our proposed integral neural networks_ Figure 1 4x EDSR on Div2k validation set: By compression a) ResNet-18 on CifarlO. b) NIN architecture on CifarlO. c) ResNet-18 on ImageNet: we denote the percentage of deleted parameters_\nThe weights of this layer are represented by an tiplication: integrable function Fw(A,zout,zin) . Similar to the convo- lution operator; a vector of trainable parameters A defines of the integral kernel. The input and output functions are represented by the integrable functions Fi(zin),Fo(zout) respectively, and are connected via the weight function as follows:\nspecific numerical integration method The size of vectors change to arbitrary values by selecting a largert Wq and $ can partition Pr or smaller For more details on the numerical integration of multiple integrals Appendix we refer to A The proposed use of these integrals for representing basic and  fully-connected network layers, such as   convolution allows for various segment partition lengths along ones, filters and channels dimension or height and width: This leads to the generation of layers with the desired number of filters, channels, height; and width:\n3.1. DNNs layers as integral operators\nPooling and activation functions Pooling layers also ex hibit a meaningful interpretation in terms of integration or signal discretization. Average pooling could be interpreted the spatial dimensions with a piece- along a convolution as MaxPooling could be interpreted as wise constant function_ a way of signal discretization. Activation functions in inte- networks are naturally connected with activation func- gral following equation: tions in conventional networks by the\nCommonly used linear network layers can be presented specific integral kernel. integral operators with While as such layers act as linear operators on the real linear space Rk , integral operators act as linear operators O the linear space of integrable functions L?. Therefore, not all input data could be considered as continuous integrable functions meaningful way. Nevertheless, digital images and audio in a signals are discretizations of analog signals and, therefore, they can be naturally used in integral networks_\nlayer Convolution or cross-correlation The convolution transform of layer defines multichannel signal to an- other multichannel signal. In the case of integral operators layer represented by an integrable func- weights of this are where xs is scalar Or tion Fw (A, XS ) , vector rout 2in representing the dimensions, over which the convolution is performed, and a vector of trainable parameters. Input A is and output images are represented by integrable functions and are connected through the xs) , Fo(x FI Xs\" (zin_ oul following way: weight function in the\nwe denote the discretization operation where by D that scalar function 0n the given partition Pc_ This evaluates equation implies that applying an activation function on discretized signal is equivalent to discretizing the output o the activation function to the continuous signal. applied\nFor fast integral evaluation, the integral kernel goes through discretization procedure and is then passed to a conven- tional layer for numerical integration. It turns out that any composite quadrature may be represented by such a conven- tional layer evaluation: For backpropagation through inte- gradients of the gration we use the chain-rule to evaluate the The validity trainable parameters A as in discrete networks_ of the described procedure is guaranteed by the following lemma, whose proof can be found in Appendix A\nfully-connected layer defines Fully-connected layer transform of by means of matrix mul- a vector t0 a vector\nFigure 2_ Different integration quadratures a) left Riemann quadrature, b) right Riemann quadrature, trapezoidal quadrature. Riemann qudratures are first-order methods, while the trapezoidal quadrature is a second-order method. The trapezoidal quadrature computes the fewer required number of points in the segment partition. integral more precisely than the Riemann quadratures with\nalong out Figure 3. Visualization of the integral layer evaluation  Continuous weights go through discretization the variables x and en adjusted by an element-wise product with the integration quadrature Q\nLemma 1 (Neural Integral Lemma Given that an inte kernel F(A,x) is smooth and has continuous partial gral derivatives #FC) [0, 1]n, composite on the unit cube any 2A quadrature can be represented aS a forward pass of the cor responding discrete operator: The backward pass of the dis- crete operator corresponds to the evaluation of the integral aFKA) using the same quadrature operator with the kernel JA aS in the forward pass:\nficiency purposes, we suggest to exploit the available hard existing Deep Learning (DL) frameworks and rely ware and Fig: on the cubic convolutional interpolation as shown in which is used for efficient image interpolation o GPUs. Despite its slight deviation from the cubic spline interpola this approach is significantly faster and yet preserves tion; the details better than linear interpolation_ In case of mul tiple dimensions, we propose t0 define the interpolation on the cube [0, 1]n with separable kernels, which is fully com patible with the existing DL frameworks_\n3.2. Continuous parameters representation\nThe continuous representation is discretized into a stan- dard weight tensor W which is used by the corresponding layer in the forward pass_ schematic visualization of A continuous parameter representation and discretization is depicted in Fig. 4.\nThe richer and more generalized continuous parameter representation allows to sample discrete weights at infer ence time at any given resolution We propose to compactly parameterize the continuous weights as a linear combina- tion of interpolation kernels with uniformly distributed in terpolation nodes on the line segment [0, 1]: Fw(A,.) Cino Aiu(zm 1) , where and Ai are the number m interpolation nodes and their values, respectively. For ef\nfor Representation of weights fully-connected and con Fully-connected layers are defined by volutional layers two-dimensional weight tensor and, thus, we represent them\nFigure 4 Visualization of continuous parameter representation and sampling along one dimension The continuous representation (c) is a) with interpolation nodes (b). During the forward phase it is discretized the result of a linear combination of a cubic convolutional kernel (d) and combined with an integration quadrature_\nwith a linear combination of two-dimensional kernels on uniform 2D grid within the square [0,1]2:\neterization guarantees that the result is correctly defined (sorted) partition P stretched over the whole segment [0,1].\n4. Training Integral Neural Networks\nNowadays, there exists a large variety of pre-trained dis- Therefore, it would be beneficial to have crete networks. in place a process of converting such networks to integral ones. Such converted networks can serve as a better initial- ization for the training of integral networks. To this end, we algorithm that permutes the filters and channels propose an of the weight tensors in order to obtain a smooth structure A visual illustration of this strategy is in discrete networks_ provided in Fig: 5. We also propose an algorithm to opti- mize the smooth parameters representation of INNs using gradient descent: This allows uS to obtain a network which re-sampled (structurally pruned) without any fine- can be tuning at inference time.\nThe discretized weight tensor Wq of the fully-connected layer is obtained by sampling the continuous representation partitions Pout and Pin and by weighting the result ac- on cording to the integration quadrature of Eq: (1):\nUniform partitions with steps hout and hin are defined as {khout} k and Pin follows: Pout {lhin }z- Fewer or filters and channels at inference time are obtained with more varying partition size.\nAs for convolutional layers, in this study we omit re sampling convolution kernels along the dimensions spatial Therefore, the continuous representation of weights xs already sampled at each spatial loca- could be viewed as Fw t and defined by with location dependent set of tion (t). interpolation nodes\nConversion of DNNs to INNs To find a permutation that leads to the smoothest structure possible, we minimize the along a dimension of the weight ten- total variation specific SOr: This problem is equivalent to the well-known Traveling along Salesman Problem (TSP) [15]. In our task; the slices out dimension in the weight tensor (i.e,, filters the corre - to the *cities\"' and the total variation to the \"distance spond between those cities. Then, the optimal permutation can be optimal \"route\" considered as an in TSP terms_ We use the 2-opt algorithm [6] to find the permutation of filters that minimizes the total variation along that dimension:\nTrainable partition we considered only uniform So far; partitions fixed   sampling   step. However; with non- uniform sampling can improve numerical integration with- increasing the partition size. This  relaxation of the out sampling points introduces new degrees of freedom fixed and leads to a trainable partition. By training the separable partitions we can obtain an arbitrary rectangular partition in technique opens up the smooth and efficient way. Such pruning approach: opportunity for a new structured Com- bined with the conversion strategy of Section 4, this can reduce the size of pre-trained discrete DNNs without tuning Instead of using a direct partition the rest of the parameters_ parameterization P latent representation by we employ following holds (0,61, the vector 0n So that the Snorm P 6norm Such param - cumsum 02 sum\ndenotes the permutation where W is the weight tensor; is the new position of the i-th element defined by the permutation, and Sn is a set of all permutations of length n _ The permutation is performed in such a way SO that the fil permutation in the preceding layer matches the chan- ters permutation in the following layer: nels Since the model\noutput stays exactly the same, our algorithm allows to ini- using tialize the integral neural network discrete one with- experiencing any quality degradation. out\nAlgorithm Training of the integral neural network\nAny Optimization of continuous weights available gra- dient descent-based method can be used for training the prO- posed integral neural networks. We use Lemma to con- struct the training algorithm as described below. We train predefined range cout   from our networks  with random denotes the number of filters ((cout or rows in the convo- lution or the fully-connected layer). Discretization of zin of the next layer is therefore defined by discretization of Training integral neural layer: of the previous cout net- using general- approach allows for such an works a better ization of the integral computation and avoids overfitting of the weights to a fixed partition, since the size of the partition changes at every training iteration. training Formally, our algorithm minimizes the differences between different cube following objective: using partitions for each layer the\nis the neural network evaluated on input where Net(X, Pi , data X with labels Y, and P1; Pz are two different partitions note that the optimization under for each layer: One can stochastic sampling of the partition sizes leads to reduc- tion of differences between the outputs of integral neural networks of different sizes. Such an optimization therefore ensures that a trained integral neural network has similar performance when pruned to arbitrary sizes_\nsee that the INN fine-tuned from the pre-trained discrete network has the same or higher performance as the corre- sponding discrete network and significantly outperforms the INN trained from scratch (see Table 1) The INN fine-tuned using Algorithm can be re-sampled corresponding to discrete network of any desired size. Figure shows a com- parison of the EDSR INN with its discrete counterpart: It can be seen that even after 409 pruning the INN preserves almost the same performance\n5. Experiments\nWe have implemented a general framework for numeri- cal integration using the PyTorch library [25]. Our frame- work allows to use custom integration quadratures for nu- merical integration Our integral neural on Nvidia GPUs: networks use PyTorch layers for fast evaluation of integral One could use our framework for numerical in- operators. tegration of multiple integrals independently on the integral network: To validate INNs we have conducted experiments for image classification and image super-resolution tasks_ For image classification the CifarlO [12] and ImageNet [28] For the image super-resolution task datasets were used. have selected the 4-X EDSR 19] and 3-X SRCNN [8] we models trained on the Div2k dataset [1] and the 91-image dataset [36], respectively: Validation was performed on Set5 [3], Setl4 [39] and B1OO [21] datasets. A schematic de- scription of the main validation pipelines is shown in Fig: 6 Details of the implementation and the experiments setup can be found in Appendix D.\nfine-tuning Structured   pruning Pipeline B. without In this experiment; we have through conversion to INN pruned discrete networks through their conversion to INNs using the method described in Section 4, and tuned the inte- gration partition On Results are presented in few samples. partition tuning). It is important to 1 (denoted as INN Fig: note that the permutation step is very important in Our con - algorithm: higher accuracy Indeed, we observe version tuning drop when partition is deployed without the permu- This is in line with the theoretical connection of tation step_ errors (see Appendix A) and with variation and integration empirical evaluation reported in Table 2. our\nPipeline C. Structured pruning without fine-tuning of straightforward  approach discrete   NNs obtain to compact neural network is to remove some of the parame- Typically, the ters of a trained network in a structured way: importance of neurons or filters of each layer is assessed based on various criteria p(W, X) that depend on the net- work weights W and data X, such as the /1-norm of the weights 18] or the contribution to the loss [22]. In our\nComparison Pipeline A with  discrete NNs We have trained INNs with two different initializations: from scratch and from a converted pre-trained discrete network We can\nToy Figure 5_ example illustrating the permutation of filters in a discrete weight tensor in order to obtain a smoother structure.\nFigure 6. Illustration of the main validation experiments_\nFigure Example of 4x image super-resolution with 4 methods bicubic interpolation, EDSR discrete neural network; EDSR integral neural network of full-size and pruned by 40%_\nstudy, structural pruning is applied to convolutional layers as in 18]. The neurons t0 prune are selected based on the C1-norm of their kernels or various statistics of feature maps estimated on the whole dataset. Finally, the results from all 3 pipelines are presented in Fig: 1. From these results we can see that INNs significantly outperform other alternative methods equipped with the ability of pruning without fine- tuning   Further; we observe that the non-uniform trainable\npartition outperforms pre-trained INNs with uniform sam- pling, while the uniform sampling provides much faster and data-free partition of the re-discretization:\nTrainable partition for the improvement of continuous representations Cubic convolution interpolation is typ ically applied to discrete data such images or volume_ as However; the uniformly sampled partition can limit the re- trainable partition prO- construction quality: Therefore,\nFigure &. Image reconstruction with 3 methods (from left to right): original image, interpolation kernels with fixed partition, with separable trainable partition and non-separable trainable partition.\nvides additional flexibility to enrich the signal reconstruc- higher quality representation. tion and may lead to We assess the representation capability of interpolation kernels with fixed and separable trainable partitions by performing image reconstruction (Fig: 8). Additionally, we have tested the reconstruction with the use of a non-separable trainable partition parameterized by a set of independent 2D coordi- nates. Since the partition parameterization introduces addi- equalize the total number of parame- tional parameters, we ters by adjusting the number of interpolation nodes and the partition size\n6. Conclusions and open problems\nIn this paper; proposed a novel integral representa- we tion of neural networks which allows us to generate conven- tional neural networks of arbitrary shape at inference time simple re-discretization of the integral kernel. by Our that the proposed continuous results   show INNs   achieve the same performance as their discrete DNN counterparts, being stable under structured pruning without the use while of any fine-tuning: In this new direction, the following ques- tions/problems are worth to be further investigated:\nTable 1. Comparison of INNs with discrete networks on classifi- cation and image super-resolution tasks for different architectures Discrete refers to the conventional DNN, INN refers to the integral network trained from scratch, while INN-init refers to the integral network trained according to pipeline A indicated in Fig: Table 6_ indicates accuracy [%] for classification tasks whereas table (a, (b) indicates PSNR [dB] for super-resolution tasks.\nINNs open up new possibilities for investigating the capacity of neural networks The Nyquist theorem can sampling be used t0 select the number of points. Adaptive integral quadratures In this work, we have investigated only uniform partitions for training INNs. Investigating data-free non-uniform partition estima- tion could also have strong impact on INNs. Training INN from scratch requires improvement for Current accuracy drop proba- classification networks. bly caused by absence of batch-normalization layers. Smooth analogue of normalization is required.\nTable 2. Tuning integration partition of INN with and without per- during conversion from pre-trained DNN. mutation step All mod- els were compressed at 40 %_\nReferences \n Now the paper ends. \nQuestion: What are some potential benefits of converting standard DNNs to integral neural networks (INNs)?", "output": "Answer: The paper suggests that converting standard discrete DNNs to INNs and using a continuous representation has several potential benefits:\n\n1. Self-resizing ability: INNs can be discretized to generate neural networks of various sizes without significant loss in performance. This allows the same model to be deployed on devices with different hardware constraints. \n\n2. Structured pruning: INNs can be pruned by simply changing the discretization interval of the integral kernels. This allows pruning the model without any fine-tuning, unlike standard pruning methods for DNNs.\n\n3. Flexibility: Since INNs represent layers as integral operators, they can be discretized with various discretization intervals, generating layers with different numbers of filters, channels, etc. This provides more flexibility compared to standard DNNs.\n\n4. Better initialization: Converting a pre-trained DNN to an INN provides a good initialization for training the INN, resulting in similar or higher performance compared to training the INN from scratch.\n\nIn summary, the continuous representation and self-resizing ability of INNs can help address the challenges of deploying DNNs on edge devices with different hardware constraints and resource limits."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nEffectively localizing an agent in a realistic, noisy setting is crucial for many embodied vision tasks: Visual Odome- practical substitute for unreliable GPS and try (VO) is compass sensors, especially in indoor environments: While solid performance SLAM-based methods show without they large data requirements; are less flexible and robust to noise and changes in the sensor suite compared wrt. VO models; Recent deep learning-based approaches: to however; limit themselves to a fixed set of input modalities, while training RGB and depth, on millions of sam- e.g, change, ples: When sensors fail, sensor suites or modali- ties are intentionally looped out due to available resources, e.g , power consumption, the models fail catastrophically Furthermore, training these models from scratch is even more expensive without simulator access or suitable exist- ing models that can be fine-tuned. While such scenarios mostly ignored in simulation, they commonly hinder get model's reusability in real-world applications: We propose a Transformer-based modality-invariant VO approach that can deal with diverse or changing sensor suites of naviga- tion agents. Our model outperforms previous methods while training on only a fraction of the data: We hope this method applica- opens the door to a broader range of real-world tions that can benefit from flexible and learned VO models:\nFigure An agent is tasked to navigate to location goal uS - ing RGB-D sensors. Because GP S+Compass are not available the location is inferred from visual observations only: Neverthe- availability can change during less, sensors can malfunction, or test-time (indicated by ~) resulting in catastrophic failure of the We localization train our model to react to such scenarios by ran domly dropping input modalities_ Furthermore, OUr method can be extended to learn from multiple arbitrary input modalities, e.g- surface normals, clouds, or internal measurements. point\ncamera and sensor and teach it t0 nav- a GP S+Compass igate to goals in unseen environments [2]. With extended through simulators [28,39, 40, 47,57], photo- data access realistic scans of 3D environments [7, 28,46,56,58], and large-scale parallel  training, recent approaches reach al- perfect navigation results in indoor environments [55]. most However; these agents fail catastrophically in more real noisy; istic  settings   with partially  unavailable, failing or readings, noisy actuation, O no RGB-D sensor access to GP S+Compass [6,64].\n1. Introduction\nVisual  Odometry (VO) is close this per- to way one formance   gap and localize the agent from  only RGB-D deploying such observations [2], and model has  been to be especially beneficial shown when observations are [12, 64]. hoisy However; those methods are not robust to any sensory changes at the test-time, such as a sensor fail- being intentionally looped out: ing,  underperforming; or In practical applications [43], low-cost hardware can also experience serious bandwidth limitations, causing RGB (3 channels) and Depth (1 channel) to be transferred at dif- Furthermore, mobile edge devices must bal ferent rates_ battery usage by switching between passive (e.&- RGB) ance depending on the specific (e.g , and active LIDAR) sensors Attempting to solve this asymmetry by keeping episode.\nArtificial intelligence has found its way into many com mercial products that provide helpful digital services. To in- crease its impact beyond the digital world, personal robotics and embodied AI aims to put intelligent programs into bod ies that can move in the real world Or interact with it [15]. One of the most fundamental skills embodied agents must learn is to effectively traverse the environment around them allowing them to move past stationary manipulation tasks and provide services in multiple locations instead [40]. The ability of an agent to locate itself in an environment is vi- tal to navigating it successfully common setup [12,64]. is to equip an agent and Depth) with RGB-D (RGB an\nexchange at EPFL Work done on\nseparate models in memory, relying on active sensors, or using only the highest rate modality is simply infeasible for high-speed and real-world systems Finally, a changing sen- sor suite represents an extreme case of sensor failure where during modality is lost These points test-time. access to demonstrate the usefulness of a certain level of modality in- variance in VO framework. Those scenarios decrease the robustness of SLAM-based approaches [32] and limit the transferability of models trained on RGB-D to systems with only a subset or different sensors_\nto various modalities, e.g,, point clouds, surface normals, gyroscopes, accelerators, compass, etc. To the best of our knowledge, VOT is the first widely applicable modality- invariant Transformer-based VO approach and opens up ex- kiting applications of deep VO in both simulated and new real-world applications. We make our code available at github com/memmelma/VO-Transformer.\n2. Related Work\nSLAM- Learning-based  Navigation: Simultaneous VS Localization and Mapping (SLAM) approaches decompose the navigation task into the components of mapping; local- ization, planning; and control 49]. These methods rely on explicit visual feature extraction and, therefore, fail in real- settings noisy observations [64], while learning- istic with ambiguous ob- more robust to noise, based  methods are servations, and limited sensor suites [27, 32]. However; learning-based methods require an order of magnitude more data, e.g-, available through simulation [40]. To deal with data   requirements,  SLAM- and learning-based large the methods can be combined [5,8,9,11,48,61,63].\nWe introduce optional modalities as an umbrella term to describe settings where input modalities may be of lim- ited availability at test-time typical Figure visualizes a indoor navigation pipeline, but introduces uncertainty about modality availability (i.e. at test-time, only a subset of all While previous approaches modalities might be available) completely neglect such scenarios, we argue that explicitly accounting for \"optional\" modalities already during train- ing of VO models allows for better reusability on platforms trading-off costly or unre- with different sensor suites and during test-time. Recent methods [12, 64] liable sensors Network (ConvNet) architectures use Convolution Neural constant channel size of the input; that which assume makes it hard to deal with multiple \"optional' modalities. In contrast; Transformers [51] are much more amenable to variable-sized inputs, facilitating the training of models that can optionally accept one or multiple modalities [4].\nVisual Odometry for Realistic Indoor Navigation: While change from most  VO methods estimate an s pose agent more than two frames [52, 53] 0 optical flow 66], subse - quent frames in indoor environments share almost no over- lap and contain many occlusions due to the large displace- ment caused by the discrete action space [64]. Datta et 12] propose to estimate the pose change from consecu- al. tive frames via a ConvNet architecture and decouple learn- the VO from the task-specific navigation policy to allow ing retraining modules when dynamics change or the actua for tion experiences noise. Zhao et al. [64] improve the model $ robustness to observation and actuation noise through ge ometric invariance losses [54], separate models for mov ing and turning, pre-process observations, and introduce 44]. Finally, Partsey et al. [35] explore the need dropout for explicit map building in autonomous indoor navigation. They apply train- and test-time augmentations and concate- embedding similar to Zhao et al. nate an action 64] to the extracted visual features. trend is t0 exploit simulators to gather large datasets (1M [64], SM [35]). While this is progression, it is infeasible to re-train the VO a reasonable model whenever dynamics or sensor configurations change_\nrequire large amounts of data Transformers are known to for training Our model's data requirements from scratch: significantly reduced by incorporating various biases: are We utilize multi-modal pre-training [4, 17,30], which not only provides better initializations but also improves perfor- subset of modalities are accessible dur- mance when only ing test-time [4]. Additionally, we propose token-based action prior: The action taken by the agent has shown to learning VO [35,64] and primes the model be beneficial for towards the task-relevant image regions.\nOdometry Transformer (VOT) We introduce the Visual modality-agnostic framework for VO based on the a novel Transformer architecture. Multi-modal pre-training and an drastically reduce the data required to train the action prior we propose explicit modality- Furthermore, architecture_ invariance training: By dropping modalities during train- ing, a single VOT matches the performance of separate uni- modal approaches. This allows for traversing different sen maintaining performance in the during test-time and sors absence of some training modalities.\nMulti-modal Representation Learning: The availability pseudo-labeled [4] data [13, 16, 34,38, of multi-modal or 59,65], e.g,, depth, video, and audio, makes it possible to learn feature-rich representations over multiple modalities_ Together with Transformer'$ [51] ability to process a token sequence of arbitrary length, this leads to general-purpose architectures that can handle various modalities 23] like video, images, and audio [30] single-view 3D geom- or Multi-modal  Multi-task Masked In particular; etry [17]. Autoencoder (MultiMAE) [4] is a multi-modal pre-training\nWe evaluate our method on point-goal navigation in the Habitat Challenge 2021 and that   VOT outper- show training forms   previous on   only 59 methods [35] with Beyond this simple demonstration, of the data we stress that our framework is modality-agnostic and not limited to input or discrete action spaces and can be adapted RGB-D\nFigure 2_ The Visual Odometry Transformer architecture for RGB D input: Image patches are turned into tokens through modality-specific linear projections fixed positional embedding is added to them: before We pass an action token that embeds the action taken then estimates the VO parameters B, \u20ac from the output token corresponding to the input action token_ by the agent: An MLP-head By randomly dropping either RGB or Depth during training; the Transformer backbone modality-agnostic, allowing it to deal with becomes a subset of these input modalities during test-time without performance. When more modalities are available during training; other losing modality-specific linear projections can be added t0 process the additional information_\nstrategy that performs masked autoencoding 19] with RGB Depth, and Semantic Segmenation We SemSeg) show that fine-tuning pre-trained MultiMAE model can significantly increase VO performance using only 5% of the training data amount of previous methods 35].\nWe then learn VO model fo with parameters predicting 8 , & from observations Ot, Ot+1: B,& fo(ot, 0t+1) Fi- = nally, we transform gt in coordinate system Ct to the new agent coordinate system Ct+1 by 9t-1 H gt-\n3. Proposed Method\n3.1. Preliminaries\nIn the realistic PointGoal Navigation task [2], an agent position in an unseen environment and spawns at a random is given a random location gt relative t0 its starting pO goal sition. At each time step t of an episode, the agent perceives its environment through observations Ot and executes an ac tion at from a set of discrete actions (move fwd 0.25m, turn left and right by 309). The stop action indicates the agent'$ confidence in having reached the goal. Because the relative goal position gt is defined at the beginning of each episode, it has to be updated throughout the episode as the change the agent'$ position and orientation. Follow actions 12,64], we update gt through an estimate of the agent'$ ing With access to GP S+Compass coordinate transformation computing this transformation is trivial. However; since those sensors are unavailable, we estimate the transforma tion from the subsequent observations Ot,0t+1 and agent' taking update the estimated relative position gt: When goal agent'$ coordinate system Ct transforms an action @t, the Ct+1. into Because the agent can only navigate planarly in we discard the 3rd dimension for simplic the indoor scene ity: We define the estimated transformation as H \u20ac SE(2) with SE(2) being the group of rigid transformations in 2D plane and parameterize it by the estimated rotation an-\n3.2. Visual Odometry Transformer\nModel Architecture: When facing \"optional\" modalities, it is not yet clear how systems should react  Options range constructing an alternative input; e.g,, noise [29], to from falling back on a model trained without the missing modal- training the network with placeholder inputs [31]. ities, to Besides these, recent approaches depend on fixed set of modalities during train- and test-time due to their ConvNet- based backbone. Transformer-based architectures can prO cess a variable number of input tokens and can be explicitly during trained to accept fewer modalities test-time while ob- serving multiple modalities throughout training [4,51]. Fur thermore, the Transformer's global receptive field could be beneficial for VO, which often solved with correspon gets dence or feature matching techniques [41]. We, therefore propose the Visual Odometry Transformer (VOT), a multi modal Transformer-based architecture for VO.\nVisual Odometry Estimation: To estimate the VO param we pass the encoded Action Token ([ACT]) token eters, prediction head. two-layer Multi-layer Per- We use to composed into ceptron (MLP) with learnable parameters Rdxdh , bo Rdn *3 Wo W1 b1 R Rdh and \u20ac \u20ac \u20ac \u20ac\n4. Experimental Evaluation\n768, and hidden dimensions with token dimensions d Gaussian Error Linear Unit (GELU) [21] d/2. dh A acts as the non-linearity between the two layers. The VO model can then be defined as function f/,w (Ot, Ot+1. Wt taking as input the action at and the observations Ot, 0t+1 corresponding to either RGB, Depth, or RGB-D and pre- Simplifying the VO parameters B,&. dicting the back- b6(0t, bone that returns extracted visual fea- Ot+1,' as @t_ e Rlxd and governed by parameters the tures Ut-t+1 resulting model is:\n4.1. Setup\nSimulation: We use the AI Habitat (Habitat) simulator for data collection and model evaluation, following the Habitat specifications The guidelines PointNav Challenge 2020 define an action space of Ewd (move forward 0.25m), left right (turn right by 309), (turn left by 30*), and stop (indicate the agent reached its goal), and include a sensor suite of RGB-D camera; and GP S+Compass (not used in the realistic PointGoal navigation task) The RGB obser [0, 255] range while the Depth vations get returned into map is scaled to [0, 10]. Both sensors are subject to noise noisy actuations [33] and observations [10]: i.e  Further more, collision dynamics prevent sliding, behavior that allows the agent to slide along walls on collision. Cosmetic changes bring the simulation closer to the LoCoBot [18], low-cost robotic platform with an agent radius of 0.18m and height of 0.88m2. An optical sensor resolution of 341 x 192 height) emulates an (width Azure Kinect camera_ An episode is successful if the agent calls stop in a radius two point times its own; i.e., 0.36m, around the and does goal By specifica- So in T 500 total number of time steps_ tion, the 3D scenes loaded into Habitat are from the Gib [57] dataset; precisely Gibson-4+ [40], subset son more of 72 scenes with the highest quality. The validation set which are not part of the training set. contains 14 scenes,\nAction Prior: The action Gt taken by the agent to from get powerful is on the VO parameters. To Ot+1 prior Ot to provide this information to the model, we embed the aC embedding layer [36]. using This layer acts as tion an learnable lookup for each action, mapping it to fixed-size embedding: With the embedding size equal to the token di- mensions, we can create an [ACT] and pass the information directly to the model (cf. Figure 2). In contrast to [35,64] we pass the token directly to the encoder instead of concate nating it to the extracted features This practice conditions the visual feature extraction on the action and helps ignore irrelevant parts of the image. Note that this approach is not limited to discrete actions but tokens could represent contin readings like accelerometers, gyroscopes, and uous sensor compasses, allowing for flexible deployment, e.g-, in smart- phones or autonomous vehicles 43].\nFor training VOT, training-  and Dataset: we collect validation dataset.  Each set consists of samples containing the ground truth translation \u20ac and rotation parameters B re- perfect GP S+Compass sensOr; observations trieved from We keep samples where the and taken action @t. Ot; Ot+1- agent collides with its environment as the transformations strongly differ from standard behavior 64]. The collection procedure follows Zhao et al. [64] and is performed as: 1) initialize the Habitat simulator and load a scene from the dataset; 2) place the agent at a random location within the environment with a random orientation, 3) sample a naviga- ble PointGoal the agent should navigate to, 4) compute the shortest path and let the agent follow it, and 5) randoml sample data points along the trajectory: We collect 250 training and 25 observation-transformation from the pairs from the validation scenes of Gibson-4+, which is signifi cantly less than comparable methods (1 M [64], 5 M [35]). we apply data augmentation during training Furthermore to the left and right actions by horizontally flipping the observations and computing the inverse transformation.\nExplicit Modality-invariance Training: Explicitly train- ing the model to be invariant to its input modalities is one way of dealing with missing sensory information during drop To enforce this property, we test-time modalities dur- ing training to simulate missing modalities during test-time Furthermore, this procedure can improve training on less in- formative modalities by bootstrapping model performance with more informative ones_ For example, RGB is more overfitting than Depth the model can because prone to latch onto spurious image statistics, e.g. textures_ Train- ing on RGB-only would likely cause the model to latch onto those and converge to local minima generalizing well not to unseen scenes By increasing the amount of Depth ob during training, the model learns to relate servations seen both modalities, acting as regularization. We model this no tion as multinomial distribution over modality combina Depth, RGB-D) with equal probability tions (here: RGB sample from the distribution to For each batch;, we draw determine on which combination t0 train:\nLoss Function: Our loss function is the L2-norm between the ground truth VO parameters and their estimated counter We further add the geometric invariance losses Linv parts. proposed by Zhao et al. 64] and use the Adam [26] opti- mizer (81 0.9, 82 0.999, \u20ac le to minimize the Elz + Ib resulting loss function L = Ile BI2 + Linv;\nFigure 3_ Top-down map of the agent navigating the Cantwell scene [58] from start The plot shows the shortest path goal to 'imaginary\" path the agent took; i.e. its VO estimate We evaluate the model without RGB or the taken by the agent 3), and the path Depth (Drop) to determine performance when modalities are missing: As expected, the VOT relies heavily on both modalities, causing the estimation t0 drift when either RGB Depth is unavailable. The localization error accumulates over the course of the trajectory and Or causes the true and imaginary path to diverge, resulting in failure to complete the episodes Training a VOT to be modality-invariant VOT removes those reliances and leads to success even when modalities are missing: wl inv\nthe trained VO models as it has shown minimal navigation performance gains in [64] and was abandoned in [35].\nWe train all models, including baselines, for 100 epochs learning rate lin- with 10 warm-up epochs that increase the early from 0.0 to 2e and evaluate the checkpoints with We further find gradient norm the lowest validation error: 62] (max gradient norm of 1.0) to stabilize the clipping training of VOT but to hurt the performance of the Con- training vNet baselines_ The was done with batch size of 128 on an NVIDIA ViOO-SXM4-4OGB GPU with au- tomatic mixed-precision enabled in PyTorch [36] to reduce memory footprint and speed up training: Our backbone is patch size of 16 X 16 and 12 encoder ViT-B [14] with 12 Multi-head blocks with Attention (MHA heads each, and token dimensions 768. To encode the input into tokens positional embedding and separate we use a 2D sine-cosine linear projection layers for each modality Note that if addi- tional modalities are available, our model can be extended by adding additional linear input projections or fine-tuning existing ones [4]. Finally, we pass all available tokens to the model and resize each observation t0 160 X 80 X c (width X height X channels c) and concatenate modalities along their computation. height to 160 X 160 X We keep c t0 reduce running mean and variance to normalize RGB and Depth to zero mean and unit variance.\nduring Table 1_ Results for dropping modalities test-time_ Training VOT to be modality-invariant (w/ inv:) leads to no performance VOT trained on a single modality drop in comparison to VOT VOT Depth) This shows that single VOT RGB, wl inv can replace multiple modality-dependent counterparts. Previous ap- proaches [12,35,64] become inapplicable, converging to a Blind behavior: Metrics reported as e Bold indicates best results\nPre-training: Pre-training is practice to deal a well-known with the large data  requirements of Vision Transformers (ViTs) [14, 60], especially in VO setting where data is scarce [14,25,45]. We use the pre-trained MultiMAE (RGB SemSeg) made publicly available by Bach- Depth mann et al. [3]. Since SemSeg is unavailable in our setting, corresponding projection layers. we discard the\nEvaluation Metrics: Anderson et al. [2] propose the Suc- weighted by (normalized inverse) Path Length (SPL) cess to evaluate agents in a PointGoal or ObjectGoal navigation setting: crucial component of this metric is the success episode (success of an failure 0). With S S 1 starting position and p the shortest distance from the path the length of the path taken by the agent; the SPL over N 1( i) N_1 S(i) episodes is defined as SPL Civo max(pli) ((i\nTraining Details: We follow work [12,35,64] and prior navigation policy and VO model separately before train our jointly evaluating them on the validation In contrast set: 12, 64], we do not fine-tune the navigation policy on to\nFigure 4 Absolute difference between ground truth translations \u20acx , \u20acz and rotation angle G to their estimated counterparts We compare along Zhao et al. [64] (Table 2, 2) to the VOT Table 2, 13). Our model estimates fwd translation the z-axis (middle); left, right 2-, T-axis (left, middle), and the turning angle B (right) more accurately than the baseline. We successfully capture the displacements along caused by the noisy actuation with an average error (over both axis \u20ac, 2) of 0.25 cm fwd), 0.7cm (right), and 0.65 cm (left)\nWhile SPL  depends on the success of episode, [12] an Soft Success Path Length (SSPL) that pro the propose more holistic view of the agent's navigation per vides The authors replace the binary success S formance. of consisting of the ratio episode with a soft value be- an the (geodesic) distances dinit tween to target upon start resulting metric and termination of an episode dg The Xiol (1 - 49) /dfi)) 1(i) is then SSPL p(i) ,l(i) max The closer the agent gets to the goal, the higher the SSPL even if the episode is This   softening unsuccessful. al- lows distinguishing agents that fail to complete single significantly close multiple episodes but to the or move from ones that move away from it. Without access Igoal to GPS+Compass, SSPL becomes significantly more im portant as an agent might call stop prematurely due to in- We report the SPL, SSPL, success accurate localization. S,and (geodesic) distance to goal on termination dg on the validation scenes of Gibson-4+ with decimals truncated.\nDDPO [55], distributed version of Proximal Policy Op timization (PPO) [42].\n4.2. Dealing With Optional Modalities\nWe evaluate the models robustness to missing modal- training ities by randomly dropping access to one of the modalities. This setup probes VOT for dependencies on the input modalities, which directly influence the downstream performance under limited access. In case of sensor mal functioning, e.&- only single modality might be avail predetermined as it requires able, ConvNet  s failure is the system converges to fixed-size input: If not given, Blind behavior; exemplified in Table 1. Limiting access to modalities reveals VOTs dependency on Depth: Dropping RGB barely decreases performance, while dropping Depth causes the localization to fail more drastically. Comparing VO es 'imaginary\" i.e , the true agent localization and its timate, it becomes clear why. Figure 3 shows how the er causing the true location to drift away from rors accumulate, While the effect is less drastic when dropping the estimate_ RGB, the agent still fails to reach the goal.\nNavigation Policy:  Similar to work [12,35,64], we prior replace the GP S+Compass with our VO model to estimate position, which input to the relative as the goal serves pre-trained navigation policy. We use the same pre-trained policy as Zhao et al. 64] for our experiments, which was using goal position updated by ground truth local- trained The policy architecture consists of Long Short- ization: Term Memory (LSTM) [22] with two recurrent layers that 512-dimensional encoding of the agent'$ ob- process 1) servations Ot (here: Depth), 2) 32-dimensional embed- Iding of the previous action, and 3) a 32-dimensional em bedding of the updated relative position The observa- goal encoding obtained by passing the observations 0 tion gets through a ResNet-18 [20] backbone, flattening the resulting dimensionality 2052, and projecting it to di- map feature to mensionality 512 with a fully-connected layer: Finally, the output of the LSTM is fed through another fully-connected layer to produce distribution over the action space and using a   value function estimate The policy was  trained\nTraining VOT with the proposed invariance training sampling RGB for 209, Depth for 30%, and inv ), i.e for S0% of the training batches, eliminates  this RGB-D shortcoming: Removing RGB now only decreases the suc cess rate by 1.6%, while removing Depth also leads to stronger performance. This observation suggests that RGB is less informative for the VO task than Depth: Especially navigating narTOW passages, when RGB might consist of uniform observations, e.g , textureless surfaces like walls making it hard to infer the displacement; unlike Depth which would still provide sufficient geometric information (cf . Figure 3). However; this information asymmetry only decline in the metrics that are sensitive t0 sub leads to tle inconsistencies in the localization, i.e, S, and SPL In specting the SSPL, the drop of -3.5 is less drastic. Ex plicit modality-invariance training keeps VOT-B (RGB-D\nWe further investigate pre-training with MultiMAE [4] in models Table 2 Ablation study of architecture design and input modalities_ 11-14. Losses L, Success S, SPL, SSPL; and dg reported as e Bold indicates best results_\nTo capture the raw VO performance detached from the we inspect the absolute prediction indoor navigation task, We differentiate between translation \u20ac error in Figure 4. and y- direction (G1, &y), in and taken action_ VOT is T - accurate up to 0.36 cm 1.04cm (right), 1.05 cm fwd) fwd), 0.41 cm (right) (left) in x- direction and 0.20 cm 0.38 cm (1eft) in z-direction_ Note how the baseline strug gles to capture \u20acz, corresponding to the forward-moving di- when taking the fwd action. rection\nTable 3 Habitat Challenge 2021. Results for the Point Nav Test- Standard Phase (test-std split) retrieved on 05-Nov-2022_\nfrom exploiting this asymmetry matches the and perfor- during mance of VOT-B (RGB) when Depth is dropped test-time Tab. 1\nGiven results   in Table advise   using VOT 2 the we trained on Depth-only when access is assumed, as the to using GPS+Compass difference S is 4.5, mere SPL-3.1, SSPL-1.1. When \"optional\" modalities are during expected to change they test-time, needed, are e.g. invariance training should be used. Trained on RGB-D, this setup also reaches GPS+Compass like performance with differences of only S 5.2, SPL 4.2, SSPL-1.8\n4.3. Quantitative Results\napproach to Zhao et al. We compare [64] in terms our of downstream navigation performance, i.e, the VO model GP S+Compass replacement for learned navigation as We use the same publicly available navigation policy agent for both approaches and the published VO models of the Using only 259 of the training data, VOT baseline 64]. improves performance by S + 12.3, SPL+9.7, SSPL+2.O (cf . Table 2 15) and S + 7.2, SPL+5.7, SSPL+l.3 (cf . Ta- training the baseline on our smaller data ble 2 16). When ResNet-50), this improvement set (cf . Table 2 2 unified, increases to S + 29.8, SPL +22.8, SSPL+6.6 (cf. Table 2 15) and S + 23.7, SPL +l9.0, SSPL+5.9 (cf . Table 2 16).\n4.4. Ablation Study\nWe identify the impact of different input modalities and model design choices in our ablation study (cf . Table 2). Without observations, the Blind VO model cannot update position: This means the agent can only act without the goal goal-related feedback, resulting in a 0% success rate.\nExtending the model with proposed [ACT] token our allows it to surpass the Blind performance. Able to up\nFigure 5. Attention maps of the last attention layer of VOT (cf . Table 2 13). Brighter color indicates higher and darker color lower The VOT learns to focus on regions present in both time steps t,t + 1,i.e. outer image regions for turning weighting of the image patch. left, and center regions for moving fwd. Artifacts of the Gibson dataset get ignored (cf . Figure Sb).\ndate the relative goal position, the agent reaches an SSPL of 46.3 , but due to the actuation noise, it calls stop correctly only 13.3% of the time. Access to RGB or Depth allows the VO model to adjust to those unpredictable displacements. While the RGB and Depth observations correlate with the also contain information about the noisy they [ACT] token, Vice versa, [ACT] disambiguates corner cases actuation_ where the visual observations do not provide explicit infor underlying action. For instance, a fwd ac- mation about the tion colliding with a wall might be hard to distinguish from noisy left turning less than 308 64].\nbehavior emerges for a fwd action which leads to more at regions, e.g,, the walls and the end of tention on the center hallway (cf . Figure Sb). These results are particularly in- knowledge about the VO teresting as the model has no prior something about its underlying structure. task but learns\n4.6. Habitat Challenge 2021 PointNav\nWe compare approach (cf . Table 2 16) to several our Challenge 2021 bench- baselines submitted t0 the Habitat Using navigation policy as Part- mark in Table 3. the same sey et al. [35], VOT achieves the highest SSPL and on par SPL and dg training on only 59 of the data. These re- sults clearly show that reusability doesn't come with a price of lower performance and that scaling data requirements solving deep VO. doesn't seem t0 be the answer to\nOur results show that MultiMAE pre-training provides useful multi-modal features for VO that fine-tuned outper- In addition, these features form the ConvNet baselines. are complementary to the [ACT] prior; together achieving We conclude that the [ACT] state-of-the-art results_ prior biases the model towards the mean of the corresponding transformation, while the pre-training supports the learning of the additive actuation noise.\n4.7. Limitations\nIn we separate the VO model from the nav work our igation policy and only focus o the modality-invariance of the former;  neglecting that the navigation   policy ex pects Depth as input [12,35,64]. Designing policies to be modality-invariant is subject to future research. Assuming an accurate sensor failure detection when dropping modal- ities, additionally, is an idealized setup. Furthermore, OUC experiments in the Habitat's simulator limit the available Even though SemSeg has shown to modalities t0 RGB-D be beneficial for some VO applications [37, 50], there is no specific sensor for it However; SemSeg could be estimated While our experiments focus on discrete actions from RGB and RGB-D, our architecture could be adapted to continu- However; training might ouS actions and other sensor types_ become more difficult due to a lack of pre-trained weights.\nTraining separate models for each modality reveals that modality than Depth is a more informative RGB for VO We assume this to be a direct result of its geometric proper ties, i.e , the 3D structure of the scene We find that training VOT on noisy RGB even hurts the localization. The model overfits the visual appearance of the scenes and is unable to generalize to unseen ones In turn, Depth does not suffer from this issue as it only contains geometric information:\n4.5. Action-conditioned Feature Extraction\nWe what image regions the model attends show to by visualizing the attention maps of the last MHA-layer (cf. Table 2 16) corresponding to the [ACT] token in Figure 5_ To reduce the dimensionality of the visualization, fusel we the heads' weights via the max operator and align the atten- tion maps with the input images We normalize the maps to show the full range of the color scheme.\n5. Conclusions\nWe present  Visual Odometry Transformers for learned Visual  Odometry: Through multi-modal pre-training and action-conditioned feature extraction, our method is sam ple efficient and outperforms current methods trained on an magnitude more data. With its modality-agnostic order of design and modality-invariance training, a single model can deal  with different sensor suites during training and can trade-off subsets of those during test-time.\nWe find that passing different actions to VOT primes it to attend to meaningful regions in the image. When passed turning actions left Or right, VOT focuses on regions present at both time steps. This makes intuitive sense, as turning action of 309 strongly displaces visual features or pushes them out of the agent'$ field of view. A similar even\nReferences \n Now the paper ends. \nOne limitation of the method is that the navigation policy used expects Depth as input. How could you improve the model to make the policy modality-invariant so that it can be used effectively even when only RGB observations are available?", "output": "A possible approach would be to:\n\n1. Train the navigation policy using both RGB and Depth observations at the same time. This will expose the policy to both modalities and allow it to learn modality-invariant features.\n\n2. During training, randomly drop either RGB or Depth observations to simulate the scenario where only one modality is available. This will force the policy to become less reliant on a specific modality.\n\n3. Pre-train the navigation policy using a multi-modal dataset containing both RGB and Depth observations. This will provide a good initial set of features that are modality-invariant.\n\n4. Fine-tune the pre-trained policy on the specific navigation task using the approach described above - training on both modalities simultaneously but randomly dropping observations to simulate test conditions.\n\nThis multi-modal training strategy, combined with pre-training and fine-tuning, could potentially make the navigation policy more robust to missing modalities and  modality-invariant. This would then allow the policy to be used effectively even when only RGB observations are available during test time."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nExpression Recognition Facial DFER) is Dynamic rapidly developing field that focuses on recognizing facial expressions in video format: Previous research has con- noisy frames; sidered non-target frames but we pro- as pose that it should be treated as a weakly supervised prob- We also identify the imbalance of short- and long- lem: term temporal relationships in DFER Therefore, in- we Dynamic Facial Expression Learn- Multi-3D troduce the ing (MBDFEL) framework which utilizes Multi-Instance Learning (MIL) to handle inexact labels: MBDFEL gen- erates 3D-instances to model the strong short-term tem- poral relationship and utilizes 3DCNNs for feature extrac- The Dynamic Long-term Instance Aggregation Mod tion: DLIAM) is then utilized to learn the long-term temporal ule relationships and dynamically aggregate the instances. Our experiments on DFEW and FERV39K datasets show that M3DFEL outperforms existing state-of-the-art approaches The source code is avail- with a vanilla R3DI8 backbone. able at https: Igithubcomffaceeyes/MBDFEL\nIn the first row Figure 1. In-the-wild Dynamic Facial Expressions_ of images, the subject appear predominantly Neutral, yet the video is labeled as happy without specifying the exact moment when the In the second rOw, the emotion is evident emotion is expressed: from the perspective of a few figures but any single one of them noisy and unclear: In the third rOw; all frames appear Neutral, is but a closer analysis of facial movement over time reveals a rising of the corner of the mouth;, indicating a smile.\n1. Introduction\nFacial expressions are essential in communication [26, 27,45]. Understanding the emotions of others through their facial expressions is critical during conversations. Thus, recognition of facial expressions is a significant automated challenge in various fields, such as human-computer inter- action (HCI) [25, 34], mental health diagnosis 12], driver fatigue monitoring [24], and metahuman [6]. While signif- icant progress has been made in Static Facial Expression Recognition (SFER) [23,43,44,55], there is increasing at- tention on Dynamic Facial Expression Recognition.\nWith the availability of large-scale in-the-wild datasets 11] and FERV39K [46], several methods have like DFEW been   proposed  for DFER [21, 22, 31, 47, 54]. Previous works [31, 54] have simply applied general video under- standing methods to recognize dynamic facial expressions large Later on, Li et al. [22] observe that DFER contains number of noisy frames and propose a dynamic class token snippet-based filter to suppress the impact of these and Intensity Aware Loss to frames_ Li et al. [21] propose an large intra-class and small inter-class differ- account for the ences in DFER and force the network t0 pay extra atten- confusing class. However; tion to the most we argue that being DFER requires specialized designs rather than con-\nduring Both author contributed equally to this work. Work done Hanyang Wang 's internship at Tencent Youtu Lab and Bo Li is the project lead.\nCorresponding libraboli @ tencent.com,   Isttoy 163.com authors_ amzhou @cs ecnu.edu.cn\nsidered a combination of video understanding and SFER: Although these works [21,22,47] have identified some is- sues in DFER, their models have only addressed them in rudimentary manner\nsistency at both the bag-level and instance-level by perform ing dynamic normalization\nOverall, our contributions can be summarized as follows:\nweakly supervised approach to model We propose Dynamic Facial Expression Recognition (DFER) as Multi-Instance Learning (MIL) problem: We also identify an imbalance between short-  and long-term inap- temporal relationships in DFER, which makes it propriate to model the entire temporal relationship Or use time-irrelevant methods_ We propose the Multi-3D Dynamic Facial Expression Learning (M3DFEL) framework to provide unified solution to the weakly supervised problem and model the imbalanced short- and long-term temporal relation- ships in DFER. experiments We   conduct DFEW and extensive on FERV39K, and Our proposed M3DFEL achieves state- of-the-art results compared with other methods, even when using vanilla R3DI8 backbone. We also con- duct visualization experiments to analyze the perfor- mance of M3DFEL and uncover unsolved problems\nFirstly, these works fail to recognize that the existence of non-target frames in DFER is actually caused by weak Su- When collecting large-scale video datasets, an pervision. notating the precise location of labels is labor-intensive and challenging: dynamic facial expression may contain change between non-target and target emotions, as shown in Without a location label that can guide the model Figure 1. to ignore the irrelevant frames and focus on the target; mod- els are likely to be confused by the inexact label: Therefore, modeling these non-target frames as noisy frames directly is superficial, and the underlying weakly supervised problem remains unsolved\nSecondly, the previous works directly follow to use se- quence models without a dedicated design for DFER. How- we find that there is an imbalance between ever; short- long- term temporal relationships in DFER. For exam and ple, some micro-expressions may occur within a short clip, while some facial movements between expressions may dis- rupt individual frames, as shown in Figure 1, In contrast Happy face there is little temporal relationship between Happy at the beginning of a video and another face at the Therefore, neither modeling the entire temporal rela- end_ tionship nor using completely time-irrelevant aggregation methods is suitable for DFER: method should Instead, learn to model the strong short-term temporal relationship term temporal relationship differently: long- and the weak\n2. Related Work\n2.1. Dynamic Facial Expression Recognition\nFollowing  the of    DNNs in vision computer success tasks [3,4,15-20, 35-38, 50, 51, 56,57], automatic Facial Expression Recognition (FER) has been improved via Deep they learning: DFER methods differ from SFER methods as need to consider temporal information in addition to spatial features in each image. Some methods employ CNNs to ex spatial features from each frame and then use RNNs to tract analyze the temporal relationship [28, 52]. 3DCNNs have been proposed to model 3D data and learn and tem- spatial Fan et al. [S] proposed features jointly: hybrid net pporal work that combines recurrent neural networks (RNN) and using 3D convolutional networks (C3D) late fusion_ Lee et 14] proposed a scene-aware hybrid neural network that al. combines 3DCNNs, 2DCNNs, and RNNs in a novel way deep network for et al. [13] presented CAER-Net; Lee context-aware emotion recognition that exploits both hu- man facial expression and context information in a joint and boosting manner:\nusing To address the first issue, we suggest weakly su pervised strategies to train DFER models instead of treating non-target frames as noisy frames. Specifically, we propose modeling DFER as Multi-Instance Learning (MIL) prob containing bag lem, where each video is considered as a set of instances. disregard In this MIL framework, we non-target emotions in a video and only focus on the target existing MIL methods are time However; emotion. most independent, which is unsuitable for DFER. Therefore, dedicated MIL framework for DFER is necessary t0 address the imbalanced short- and long-term temporal relationships.\nThe M3DFEL framework proposed in this paper is de- signed to address the imbalanced short- and long-term tem- poral relationships and the weakly supervised problem in DFER in unified manner: combination of 3D It uses Instance and R3DI8 models to enhance short-term tem- learning: they Once instance features are extracted, pporal are fed into the Dynamic Long-term Instance Aggregation which  aggregates the features into Module (DLIAM), bag-level representation The DLIAM is specifically de signed to capture long-term temporal relationships between Additionally, the Dynamic Multi-Instance Nor- instances_ malization (DMIN) is employed to maintain temporal con-\nRecently, transformer-based networks have gained pop ularity in extracting both spatial and temporal information For   example, Zha et al. [54] propose dynamic   facial expression recognition transformer (Former-DFER) which consists of a convolutional spatial transformer (CS-Former) temporal transformer (T-Former). and Ma et al. [31- pro pose the spatial-temporal Transformer (STT) to capture dis criminative features within each frame and model contex- tual relationships among frames_ The dynamic-static fusion\nmodule [21,22] is used to obtain more robust and discrim- inative spatial  features from both static features and dy effectively reduce the interfer- namic features, which can noisy frames on the DFER task: Wang In addition, ence of et al. [47] propose the Dual Path multi-excitation Collab- orative Network (DPCNet) to learn critical information for key facial expression representation from fewer frames_\nthe samples of lab-controlled DFER are more unambigu- ous and the environment and facial expression dynamics are fixed, while the in-the-wild samples are more complex and challenging Their ways of applying the MIL method are applicable to our situation with in-the-wild DFER. not\nWith high-level hypotheses and observations to DFER, fusing design our novel MIL framework through the we modeling of the imbalanced temporal relationship within using the MIL pipeline. In contrast to existing MIL meth- ods t0 fuse the handcrafted feature, we model the strong during short-term  temporal relationship feature extraction and learn the long-term relationship during instance fusion.\nmethods mentioned above   approach DFER The as general video understanding task and do not consider the nature of the problem weakly supervised due to inexact they Moreover; crowd-sourced annotation. overlook the is- sue of imbalanced short- and long-term temporal relation - ships in DFER and rely solely on sequence model: By contrast, the proposed M3DFEL framework addresses these challenges at their root by tackling the weakly supervised problem and modeling the imbalanced short- and long-term temporal relationships in a unified manner:\n3. Method\n3.1. Overview\nThe MIL pipeline typically involves four steps: Instance Generation, Instance Feature Extraction, Instance Aggre- gation, and Classification: In the case of DFER, the pro posed M3DFEL framework follows this pipeline and uti- lizes 3DCNNs to extract features from the generated 3D instances and learn the short-term temporal relationship. The DLIAM is used to model the long-term temporal rela- tionship while dynamically fusing the instances into bag To maintain temporal consistency on both the bag-level and instance-level, the DMIN is introduced. An overview of the proposed M3DFEL framework is illustrated in Figure 2.\n2.2. Multi-Instance Learning\nMIL is technique designed to address the inexact la- beling problem [8]. Traditionally, each sample is treated as bag of instances, where the bag is labeled negative only negative. Otherwise, the bag is con- when all instances are sidered positive. MIL is commonly used in scenarios where number of samples with oly one label. In large there are a these situations, the methods must accurately identify and recognize positive instances within a dataset that contains a significant proportion of negative instances_\n3.2. Proposed Method\nMIL has been in various fields, such as WSOD applied (weakly supervised object detection) [9, 39], action location [30], and WSI (whole slide image) classification [49, 53]. no research that formulates in-the-wild Although there is DFER as a MIL problem we can draw insights from WSOD methods, which also solve the MIL problem in video-based For instance, Feng et al. [7] propose an end-to-end tasks_ weakly supervised Rotation-Invariant Aerial Object Detec - tion Network to tackle object rotations without correspond Meanwhile, Tang et al. [39] introduce ing constraints algorithm to in- novel online instance classifier refinement tegrate MIL and the instance classifier refinement procedure deep - into a single network, and train the network end-to-end with only image-level supervision.\nThree-Dimensional Instance Generation.  Generating instances by cropping video into frames is a common approach for MIL tasks, they are usually frame-based as supervised object detection or action tasks such as weakly However; in DFER, some frames may location_ not cap typical facial expression when the subject is talking: ture they While such frames appear abnormal on their own, aC - Addition- tually represent the motion of facial movement: ally, compared to other MIL tasks, the differences in facial movements between classes are subtle, which means that changes in the predicted even small movements can cause emotion and features\nThe proposed 3D-Instance Generation addresses these problems with simple yet effective approach: Given video V that contains T frames of images, we crop video Then, the bag into N in dimension T. can be de- parts sequence of instances I = [I1; 1z, fined as Iw]; where e RCxTxHxW denotes the n-th 3D instance. This de- In sign enables the feature extractor t0 model the strong short- term temporal relationship by capturing the motion of fa- cial movement across the instances, as well as the consis- tent emotion when the subject is talking: This is crucial in DFER, where the facial movements and emotional differ- ences are subtle, and even tiny movements can significantly\nThe use of MIL has been explored in recognizing emo- existing Romeo et al. [33] explores the usage of some tions MIL-based SVMs in detecting the emotion using physiolog- ical signals. Chen et al. [2] mainly focuses on Action Unit encoding for detection, and clustering-based pain applies maximum operation for instance fusion in MIL. Wu et al. differentiable OR operation for MIL [48] employ with Model as the classifier in lab-controlled Hidden Markov DFER, using facial landmark as input feature. All of these methods use handcrafted features and employ conventional learning MIL methods on their tasks Moreover; machine\nFigure 2 An overview of the proposed M3DFEL framework: The pipeline of MBDFEL: Three-Dimensional Instance Generation (a Instance Feature Extraction, Long-term Instance Aggregation and Classification. (b) The structure of the proposed DLIAM: (c) The sketch of Dynamic Multi-Instance Normalization (DMIN):\nshort period of time, such few continuous in as sec - we draw inspiration from onds_ To address this concern, [29] and design Dynamic Multi-Instance Normalization (DMIN) method to maintain temporal consistency at both bag We define a set of normalizers the and instance levels_ in and dynamically adjust the importance weights, bn , K = where bn denotes the bag-level normalizer and in denotes and Anc be the c-th Anc the instance-level normalizer: Let channel value of the n-th instance before and after normal- ization, and the normalization procedure can be presented as follows,\nimpact the predicted emotion_\nvanilla R3DI8 is Instance Feature Extraction: The used t0 extract the feature for each instance In within Fn bag: The R3DI8 model extracts compressed frame rep- the resentations and incorporates the temporal information of neighboring frames for each instance. This  results   in bag of feature representations for the instances, denoted as RNxc where C represents the number of channels. F 6\nDynamic Long-term Instance Aggregation. As afore- long- mentioned, there exists an imbalance between and short-term   temporal  relationships in DFER As the 3D Instance-based MIL setting strengthens the short-term tem- poral learning; the Dynamic Long-term Instance Aggrega- tion Module (DLIAM) is proposed to dynamically aggre- the instances while modeling the long-term temporal ggate BiLSTM to capture relationship. The first step is to use the long-term temporal relationship between instances_\nwhere variance   values, pk:  and are the and mean Ok: re- using the normalizer k for the specific spectively, estimated channel value of the instance. \u20ac is a small number added for numerical stability:   The learnable affine transform param- The importance weights eters are represented by and 8. of the normalizer k are represented by Wk and wk, and are dynamically adjusted:\nAfter that, to dynamically aggregate the representations we first apply the Multi-Head Self-Attention of instances, (MHSA) to learn the inter-instance relationship and obtain RNxc an attention weight A \u20ac\nMoreover find out that the recognition results of we the instances are rather unstable which violates the com mon sense that the emotion status is relatively stable and\nThe difference between the two normalizers is the value The bag-level normalization set t0 estimate the statistics.\ncomputes the statics along the dimension of N and for bag each\nIt is the first DFER dataset with large-scale num- scenes ber of 39K clips, scenario-scene division, and cross-domain supportability: Each video clip in FERV3OK is annotated by 30 professional annotators to ensure high-quality labels and assigned to one of the seven primary expressions as in testing training and provided by DFEW. We use the sets FERV39K for fair comparison:\n4.2. Implementation Details\nusing Our entire framework is implemented PyTorch- GPU and trained on Tesla VIOO GPUs. For feature extrac- employ the vanilla R3D8 model and utilize its tion, we pre-trained weights provided by Torchvision: The models are trained for 300 epochs with 20 warm-up epochs using learning the AdamW optimizer and cosine scheduler: The rate is set to Se-4, the minimum learning rate is set to Se-6 and the weight decay is set to 0.05. We use batch size of 256 and apply label smoothing with a value of 0.1. Our aug techniques consist of random cropping; horizon- mentation tal flipping; and 0.4 color jitter: For each video, we extract In all experiments, we a total of 16 frames as our sample_ weighted average recall (WAR) and unweighted aver- use age recall (UAR) as evaluation metrics, with more empha sis placed on the WAR as it is considered to be the critical following experiments, using metric. In the we focus on DFEW [11] for further analysis and discussion.\nRC , suggesting that the instance-level where plin , Oin \u20ac statistics are shared within the same channel of each bag:\nFor the importance weights Wk: and w we use the soft- operation to ensure Ckek Wk = 1and 13 Ckek Wk max = the scalars are restricted between 0 and 1a\nwhere A is the learnable parameter to adjust the weights for different normalization approaches.\nFor the final aggregation of the instances, the weights are sigmoid function first multiplied with the instances after Then, a ConvlD layer is utilized to aggregate the instance- level features X into bag-level feature Z \u20ac RNxc\n4.3. Comparison with the State-of-the-art Methods\nWe compare our method with the state-of-the-art meth- ods on two in-the-wild datasets DFEW and FERV39K\nDFEW: The results, obtained under 5-fold cross - validation, presented in Table 1. It can be observed are that the proposed M3DFEL achieves the best performance using in terms of both WAR and UAR, vanilla R3DI8 as the backbone_ The results are better than those obtained with difference of 1.06% in NR-DFERNet [22], with terms of WAR and 1.899 in terms of UAR: The performance of M3DFEL on each expression is also shown in Table 1, and more detailed analysis is presented in Section 4.5.\nThe bag-level feature is then fed into fully connected layer to obtain the prediction result; and Entropy a Cross Loss is used to supervise the results.\nExperiments 4.\n4.1. Datasets\nDFEW [11] is large-scale in-the-wild dataset intro- 16,000 video clips duced in 2020, which contains over These clips are collected with dynamic facial expressions. from more than 1,500 movies worldwide, and they contain various challenging interferences, such as extreme illumi- nations, self-occlusions, and unpredictable pose changes Each video clip is annotated individually by ten well-trained annotators under professional guidance and assigned to one Happy; of the seven basic expressions, including Sad, Neu- adopt Fear. tral, Angry, Surprise, Disgust, and We the 5- fold cross-validation setting provided by DFEW to ensure a fair comparison among different methods.\nFERV39K . The results are shown in Table 2. FERV39K challenging DFER dataset,  resulting in is lower over- M3DFEL outperforms all accuracy compared DFEW: to NR-DFERNet [22] by 1.709/1.959 of WARIUAR No- using vanilla R3DI8 and LSTM, M3DFEL signifi- tably; 10] and RI8+LSTM [46] by cantly surpasses 3DResNetl8 10.109/9.279 and 4.729/5.029 of WARIUAR, which in- dicates the effectiveness of the framework:\n4.4. Ablation Study\nbag Evaluation of different sizes. We conduct ablation bag studies on DFEW to demonstrate the impact of size in bag the MIL setting: When the size is set to 16, the same as the number of sampled frames, the 3DMIL setting degrades\nFERV39K 46] is currently the largest in-the-wild DFER dataset, comprising 38,935 video clips collected from 4 sce- are further subdivided into 22 fine-grained narios, which\nTable 1. Comparison( % of our M3DFEL with the state-of-the-art methods on DFEW. indicates the result is calculated according to the Bold: Best result; Underline: Second best) confusion matrix reported in the paper:\nTable 4 Ablation Study of the proposed Dynamic Long-term In- DMIN is the abbreviation of Dy stance Aggregation Module_ namic Multi-Instance Normalization_ MHSA is the abbreviation of Multi-Head Self-Attention_\nTable 2. Comparison(%) of our M3DFEL with the state-of-the-art methods on FERV3OK\nare shown in Table 3 When the 3DMIL setting degrades bag size of 16, the model achieves to 2D with the WAR setting has of 66.36% and UAR of 53.56%. large This margin compared to the others, probably because DLIAM weak temporal relationship and lacks model- only learns a ing the strong temporal relationship. Although it is essential to learn a strong temporal relationship, the experiment with bag size of 1 shows that it is not always the best solution UAR of 55.369, using the With WAR of 68.049 and settings. whole video as input falls behind the other 3DMIL sample rate of 16, setting the The results show that; with bag size to 4, where each instance contains four frames, is appreciable choice. an\nbag The ablation Study of different Table 3_ sizes_ The video sam- indicates that the entire sampled video is Bag ple rate is 16. size fed into the feature extractor; rendering the MIL pipeline and in- aggregation module inapplicable. Bag size 16 denotes that stance each instance consists of a single frame.\nWe conduct additional experiments to analyze the clas- sification performance on single instance. As depicted when the subject expressed emotions through in Figure 3 subtle facial movements, the 3D-instance-based MIL model was able to capture these movements and make accurate predictions_ In contrast; the 2D-instance-based MIL model only succeeded in a few frames_\nto 2D, where ResNetl8 is used as the backbone. Setting the bag represents feeding all frames into the feature size to leading to supervised learning extractor at once, a normal paradigm where the aggregation module fails. The results\nfusing expressions with the confidence given by the second instance, and then successfully predicts the emotional status of these confusing samples.\nTSNE Visualization: We utilize t-SNE 42] to visu - alize the distribution of dynamic facial expression features extracted by our baselines and M3DFEL. The t-SNE plot in Figure 5 illustrates that the features obtained by the base- significant overlap lines lack discriminative power; with between  categories proposed M3DFEL In contrast; our clearer boundary between categories, with method shows Nonetheless, the t-SNE fig- more concentrated clusters. ure indicates that many instances of the Neutral expres - are present in other emotions, and sion some other emo- In DFER, many tions may also be classified as Neutral. expressions have lower intensity than in SFER, and anno- more information and be more tators may have access to confident with these micro-expressions. However; recog- nizing these low-intensity expressions is a challenging task for the model, leading to difficulty in distinguishing Neutral and low-intensity expressions\nFigure 3_ Evaluation of 2D-instance-based MIL and 3D-instance- based MIL. The target emotion is Sad.\nConfusion matrix: We visualize the confusion matrix of proposed M3DFEL evaluated on DFEW Fold 1 5 to an- our From Figure 6, alyze the results_ we observe that the model struggles to predict the emotion of videos labeled as Dis- This is due to the severe label imbalance in DFEW gust. where the proportion of Disgust videos is only 1.229. As result; the model is more likely t0 ignore videos with la- during  training, leading to pOOr performance on bel Disgust A similar situation occurs for the Fear la- this emotion. proportion of 8.14%_ bel, which has The model tends to predict some videos with label Fear as other emotions, due to the lack of sufficient training examples for this emotion_ Additionally, we observe that the model tends to predict the label Neutral more frequently. This is because predicting these samples as any other emotion is more risky than pre- dicting them as Neutral.\nFigure 4 Visualization results of different MIL methods\nEvaluation of the DLIAM: We conduct experi- an study the effectiveness of the proposed DLIAM on ment t0 DFEW, using Average Pooling as the baseline method. The The comparison shows that results are presented in Table 4_ using an attention-based method to aggregate the instances, as done in existing MIL methods, performs worse than the BiLSTM-based settings. Furthermore , the results demon- strate that the proposed plug-and-play DMIN improves per- formance at a fraction of the cost. The complete DLIAM (e) outperforms the baseline by 1.029/0.66% of setting indicating the effectiveness of WARIUAR on DFEW, fully DLIAM and the importance of long-term temporal relation- ship modeling in DFER\n5. Discussion\nIt is clear that there are still many unresolved issues in DFER, despite the proposed M3DFEL framework: Our analysis of the failure cases reveals that most of them occur during the classification stage rather than instance fusion in MIL. For example, if the majority of the video frames are bag neutral, the fusion result of the whole is the non-neutral emotion; as expected However; the model often misclas - classifying sifies the non-neutral emotion, e.g. a Fear as performance is This indicates that the current Surprise. largely limited by the model's classification ability:\n4.5. Visualization\nTo further evaluate the effectiveness of our method, vi- sualization studies are conducted_\nVisualization  of different MIL methods: To inves- we obtain the classification tigate how M3DFEL works, single instance in result on sample with strong facial the 2D-instance-based As shown in Figure 4 movements. MIL is greatly influenced by the facial movements when the subject is talking: simple 3D- At the same time, instance-based MIL captures the information that the sub- ject is talking happily in the second instance, but it still predicts the other instances as other non-target emotions_ With the DLIAM, M3DFEL can further recognize the con-\nOne major issue is the imbalanced label problem where Disgust and Fear is sacrificed due to the the accuracy on lack of samples with these labels in the dataset: This prob- indicating that lem is more severe in DFER than in SFER, solely utilizing DFER datasets may be insufficient Possi-\nby 42] of dynamic facial expression features obtained different MIL methods, including 2D-instance Figure 5_ 2D t-SNE visualization based MIL, 3D-instance-based MIL and M3DFEL The features are extracted from the DFEW dataset.\nFigure 6. The confusion matrix of our proposed M3DFEL evaluated on DFEW Fold 1-5_\nble solutions to this issue include transfer learning Or self- supervised pre-training methods. Another issue is that some expressions in DFER have much lower intensity than static key problem in micro- expressions, which is similar to the expression recognition (MER) Utilizing MER techniques such as optical flow may help to solve this problem. Ad- ditionally, knowledge like landmarks Ac- prior some or tion Units may provide useful hints to the model. Except for these issues, the noisy label problem, the uncertainty problem and the hard sample problem all influence DFER greatly. More importantly, it is difficult for us to distinguish emphasize learning of the sam- if we should or weaken the ples. Beyond the existing problems, we hope  that the model should not overfit on the dataset itself: As FERV39K pro- vides cross-domain supportability, domain-generalization is important research direction. an\nvelop the M3DFEL framework to address the weakly super- vised problem and imbalanced short- and long-term tem- poral  relationships   in unified The M3DFEL manner: framework includes the 3D-Instance Generation  module, temporal relationship, which learns the strong short-term and the Dynamic Long-term Instance Aggregation Module (DLIAM), which models weak long-term temporal relation - ships. The proposed framework also implements dynamic normalization to maintain temporal consistency at both bag- Our extensive experiments support level and instance-level perspective on the DFER problem and demonstrated the our effectiveness of the proposed M3DFEL framework: In addi- tion, we have identified several research directions that may future studies in this field, such as the imbalanced la- (guide bel problem; the uncertainty problem; and others.\n7. Acknowledgments\n6. Conclusion\nThis research is funded by the Science and Technol- of   Shanghai Grant   (No Commission Municipality ogy 22511105901).\nthorough analysis of the In this   study; we conduct DFER problem and proposed a new learning paradigm: We Learning (MIL) pipeline and de- utilize the Multi-Instance\nReferences \n Now the paper ends. \nHow can deep learning methods be improved to better handle the imbalanced data problem for dynamic facial expression recognition?", "output": "The paper notes that the imbalanced label problem negatively impacts the accuracy for certain expressions like Disgust and Fear in dynamic facial expression recognition (DFER). This is due to the lack of sufficient samples with those labels in the datasets. The authors propose using transfer learning or self-supervised pre-training as potential solutions.\n\nSome other potential ways to improve deep learning methods to handle imbalanced data in DFER could be:\n\n1. Oversampling the minority classes. This involves generating synthetic samples for the under-represented classes to balance the dataset.\n\n2. Focal loss or class-balanced loss functions. These loss functions give higher penalties for misclassifying samples from minority classes.\n\n3. Ensemble methods. Combining models trained on balanced resampled versions of the original imbalanced dataset can improve performance.\n\n4. Data augmentation. Generating more synthesized samples for minority classes through techniques like jittering, cropping, flipping, etc. \n\n5. One-shot learning. Training the model to learn  from a single or few samples for the under-represented classes.\n\nIn summary, a combination of strategies like transfer learning, data augmentation, loss functions targeting imbalanced data, and ensemble methods have the potential to improve deep learning models for DFER and help them better handle the long-tail distribution of expressions in real-world datasets."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nExisting semantic segmentation methods improve gen- eralization capability; by regularizing various images to canonical feature space While this process contributes to generalization, it weakens the representation inevitably  In existing methods, we instead utilize the differ- contrast to better ence between images to build a representation space, where the distinct features are extracted and stored style as the bases of representation    Then, the generalization to is achieved by projecting features to image styles unseen this known space:  Specifically; we realize the projec style weighted combination of stored bases, where the tion as a similarity distances are adopted as the weighting factors: we extend this process to the Based on the same concept; decision part of model and promote the generalization of semantic prediction By measuring the similarity distances bases (i.e , prototypes), we replace the common to semantic deterministic prediction with semantic clustering  Compre hensive experiments demonstrate the advantage of proposed up to 3.69 mloU improve method to the state of the art; unseen scenarios: Code and models ment in average on available at https /aitee com / mindspore are\nFigure 1. Illustration of instance normalization/whitening (IN/IW proposed style projected clustering method. [5, 20, 40] and our IN and IW regularize image features from different domains to canonical space (a-c). Our method builds style and semantic rep resentation spaces based on the data from known domains (d).\nresentations [5, 16, 20, 25, 40, 42, 66, 72]. Researches in in general, this line share the similar that is to cap goal ture the domain-invariant characteristics of object contents and eliminates the domain-specific ones (i.e, image styles). As two representatives, Instance Normalization (IN) [56] and Instance Whitening (IW) [17] regularize image fea- tures from different domains to a canonical space, as illus Fig: 1(a) and 1(b). Specifically, IN achieves center trated in level feature alignment via channel-wise feature normaliza- tion [33,40],and IW realizes uniform feature distribution by removing linear correlation between channels [5,41]. More- of these two methods is proposed the combination over; Fig: 42] for a better generalization, as shown in in 1(c).\n1. Introduction\nDomain generalization methods aim to promote the per- formance of model (trained on source datasets), when ap plying it to unseen scenarios (target domains) [9,19,29,36, Recently, domain generalization for semantic 62,74,75]. segmentation (DGSS) has attracted increasingly more at- tention due to the rise of safety-critical applications, such as autonomous driving [3,12,22,45].\nNevertheless, feature regularization inevitably weakens the representation capability, as part of feature informa- Theoretically, it works under tion is eliminated. strong assumption that the eliminated information is strictly the Yet in practice, the perfect disen- domain-specific ones_ tanglement between image style and content is difficult to\nExisting DGSS methods  improve the pixel-wise gen- eralization performance by learning domain-agnostic rep-\nThis work was done during W: Huang'$ internship at Noah's Ark Lab Corresponding author\n2. Related Work\nachieve. It means that a part of content features will also be eliminated in the process of feature regularization, and thus degrades the segmentation performance.\nDomain adaptation and generalization. To reduce the burden of pixel-wise annotations on target domains, do- main adaptation (DA) technologies are proposed to narrow the domain gap between source and target domains via im- 14,24,37], feature alignment [55, 60, 61], age translation self-training [2, 39, 77] and meta-learning [13, 34] strate- DA methods require the access of gies. However; these data on target domains. Domain generalization (DG) aims practical problem to address where the target do- more main cannot be accessed. Numerous DG works have been proposed for image classification via style augmentation [19, 59, 68, 75], domain alignment [29, 31], feature disen- tanglement [27,44] and meta-learning [9,26,28].\nInstead of seeking common ground by feature regular- ization, we aim to address DGSS in a different way: In this paper; we propose style projection as an alternative, which utilizes the features from different domains as bases to build representation space, as shown in Fig: 1(d). a better The motivation of style projection comes from a basic concept of generalization, that is to represent unseen data based on the Specifically, following the common practice, known ones. adopt the statistics (i.e., mean and variance) of features we in channel dimension to represent image styles The im- iteratively extracted and from source domains are age styles stored as the bases of representation.  Then, we project the style of given unseen images into this representation space to promote generalization: This projection process is im- weighted combination of stored style bases, plemented as a where the similarity distance between styles are adopted as the weighting factors, i.e.. A1 and A2 shown in Fig: (d).\nDomain generalization for semantic segmentation. Sim- image classification, DG for semantic segmentation ilar to (DGSS) methods proposed to learn  domain-agnostic are representations, including style augmentation 16,25,43 normalization/whitening [5, 40, 42, 66] and 72], feature meta-learning [20]. To avoid overfitting on source domains 16] adopt style augmentations in the DRPC [72] and FSDR image space t0 extend the number of source samples, while WildNet [25] realizes it in the feature space with the aid of whitening ImageNet [8]. Alternatively, normalization and investigated to achieve distribution alignment between are different domains_ 40] and RobustNet [5] adopt IBN-Net whitening, respectively, to elim- instance normalization and inate the specific style information of each domain. Further- SAN-SAW [42] proposes  semantic-aware instance more , normalization and whitening to enhance the distinguishabil- ity between classes. In addition, PintheMem [20] combines memory-guided network with the meta-learning strat- the egy and obtains competitive performances. from Different these DGSS methods, our method embraces the differences from multiple known domains and takes advantage of their diversity to build a better representation space, realizing the representation of unseen images by the known data.\nBased on the projected style features, we further devise the decision part of model, which is elaborated for semantic segmentation. Typically, existing methods learn paramet- ric function t0 map pixel-wise features t0 semantic predic- We replace this deterministic prediction with seman- tions tic clustering, where the class of each pixel is predicted by the minimal similarity distance to semantic bases, as shown Fig: Notably, it follows the same concept of style in 1(d). projection, that is to predict unseen data based on the known More concretely, to facilitate the performance of se- ones. mantic clustering, we propose a variant of contrastive loss align the semantic bases of same classes and enhance dis- to criminability between different classes_\nWe conduct comprehensive experiments 0n single- and multi-source settings to demonstrate the superior general- ization of Our method over existing DGSS methods. In ad dition, visually analyze the effective representation of we proposed method for unseen images in both style and our semantic spaces_\nPrototype learning   Inspired by the cognitive psychology that human use the knowledge learned in the past to judge the class of unknown things [51,69], prototype-based classi- increasing attention, where fication methods have attracted the class of unknown images is determined by its nearest neighbors in the feature space [7, 10]. Owing to its excel- lent interpretability and generalizability, prototype learning potential in many fields, such as few-shot learn- shows good ing [1, 52], zero-shot learning [67, 71], unsupervised learn- ing [30,65]. Recently, prototype learning is also introduced in the dense prediction task; including supervised [76], few- shot [54,63] and domain adaptive [53,73] semantic segmen - tation: To facilitate the learning of prototypes, metric learn- samples belonging to ing [23,50,64] is often adopted to pull  together and push those of different classes the same class from each other in the embedding (i.e- feature) space_ laway\nContributions of this paper are summarized as follows:\nBeyond existing feature regularization  methods, we propose style projected clustering, pointing out a new avenue to address DGSS. We propose style projection, which projects  unseen style   representation   space built styles into the on known domains for a better representation_ clustering to predict the class of We propose semantic each pixel in unseen images by the similarity distance to semantic bases which further improves the general- ization capability for unseen domains. Our proposed method outperforms the current state of the arts on multiple DGSS benchmarks.\nFigure 2_ The framework of style projected clustering; which consists of two components, i.e,, style projection and semantic clustering: We In style projection iteratively extract the style and semantic information of seen domains as style bases (pm, pm and semantic bases pmn - the shallow feature Fm and style bases (pm; pI from we first calculate the similarity between the unseen style (pm as weighted Om Then, the weighted combination of style bases (pm , In semantic factors is projected on Fm to obtain the projected feature Fm Am . O m clustering; we calculate the similarity between pixel embeddings in the deep feature Then, the class of Fd and semantic bases pmn - During training phase, the cross-entropy loss LcE, variance loss each pixel is determined by the nearest semantic base the and Luar discrimination loss Ldis are adopted to supervise the learning of style and semantic bases:\n3.2. Style Projection\nSimilar to these methods, we adopt the form of prototypes Yet these semantic bases (i.e. bases) to represent semantics. are learned in different way to facilitate domain general using ization, by a new variant of contrastive loss\nThe style difference of images is the main factor leading ability to the domain shift, which limits the generalization of the learned model. Pioneering works 11,18,40,75] have demonstrated that the feature distribution shift caused by style differences lies mainly in shallow layers of networks_ It also shows that the shallow feature distribution of net- works can reflect the style information of the input image Thus, existing works always adopt the channel-wise Im: mean and variance of the shallow feature t0 represent the style distribution of Zm [18,25]. Following these works, let RDxH.XWa be the shallow feature of Em from the Fs \u20ac where D denotes the number of channels. The network RD \u20ac RD and variance channel-wise mean pr Om \u20ac the feature Fs can be calculated as follows:\n3. Style Projected Clustering\nThe overall architecture of our proposed method is de picted in Fig: 2 which consists of two components, i.e style projection and semantic clustering: In style projec- project the unseen style into the style representa- tion, we bases, according to the similarity tion space built on style between the unseen style and style bases. In semantic clus tering, we estimate the similarity between pixel embeddings and semantic bases (i.e , prototypes to determine the class of pixels in unseen images by the nearest semantic base.\n3.1. Problem Formulation\nIn the domain generalized semantic segmentation prob lem, we are given M source domains S {S1,S2, SM_ = that are from multiple datasets with different data distribu- The m-th source domain Sm can be represented as tions_ RHxWx3 is image {(xm; Ym)}, where Sm Tm an \u20ac RHxWxC is the cOI from the m-th source domain, Ym responding pixel-wise label, C is the number of semantic classes, are the height and width of the image H and W respectively. In this work; our is t0 train a seman goal Tm , tic segmentation model 0 to obtain the best generalization performance on multiple target domains T which cannot be accessed during the training phase.\nTo eliminate the specific information of images, in- style stance normalization [40] is adopted to standardize the fea- standard distribution (i.e., zeros mean and one ture Fs to \" standard deviation) as follows:\nwhere Fm stands for the normalized feature, and \u20ac is a small value to avoid division by zero_\n3.3. Semantic Clustering\nAlthough instance normalization achieves to remove the specific style information of images, it also eliminates the natural differences between domains, which weakens the representation for target domains and produces limited gen- eralization performance. Therefore, to preserve the specific style information of each domain, we propose style bases {(pE,pe)}M_1 Psty to store the style information of source domains, and then leverage the preserved style bases realizing the prO- Psty to build a style representation space, Fig: 2. Specifically; we jection of unseen style, as shown in first leverage Wasserstein distance [57] to estimate the style distribution discrepancy between the input image and Cm the m-th style bases (p/n, PIL - as follows:\nTo obtain the final pixel-wise predictions, we further prO- clustering on the deep pose semantic feature extracted by deep Let Fd HaxWd be the the network &. feature RDx _ \u20ac I Existing DGSS methods of the input image Em from $. segmentation classifier $cls generically apply a learnable prediction: on Fd for the dense However; the parameters deep features of source domains of Bcls is learned on the and thus its generalization ability o target domain S, In addition, the semantic information between is limited. different domains is implicitly encoded in the same param specific semantic information eter space, which causes the of domains to be eliminated.\nBased on the concept of style bases; we introduce seman - C,M {pan- tic bases Psem to preserve the semantic in- G,m=1 e RD formation of each domain and each class, where pmn embeddings belonging is the cluster center of training pixel to the c-th class from the m-th source domain in the feature space. Following the prototype theory [7, 10,76], the class of each pixel embedding e Fd can be determined by its nearest semantic bases as follows:\nwhere dm denotes the distribution distance between the cUr- Then, we use rent image Tm and the m-th source domain. the reciprocal of dm to characterize the similarity between Tm and m-th style bases as follows:\nwhere the softmax operation is utilized to make the sum M} equal to 1. of Based on the {Amlm 1,2, estimated similarity A we can obtain the projected style by the weighted sum of style bases as follows Om Wlm\n25], Finally, following previous works [11,18,19, we inject the projected style (pm into the normalized feature Om to obtain the projected feature as follows: Fn\nduring To facilitate the training of the network the training phase, we estimate the probability value of pixel embedding belonging to class follows: c as\nDuring the training phase, we adopt the momentum up date strategy to achieve the online collection of style infor mation as follows:\nminm {dm M_1 denotes the similarity between where dc e and its closet semantic base belonging to class Then, adopt the standard cross-entropy loss to supervise the we training of the network as follows:\nwhere & \u20ac [0, 1] is a momentum coefficient: In addition, we randomly initialize Psty to start training, where pm and pm zero-mean and one-mean distribution, are initialized with respectively. By Eq: 7 we realize the style statistic of source domains and store it as style bases efficiently:\nAfter style projection, the projected feature Fm is input into the next layer of the network 0. Our style projection is designed as plug-and-play module that can be applied layer is deeper; layer: behind any network However; as the the style information loosens while the semantic informa important role. Thus, in this work; tion plays stylel a more projection is only used in the first two layers of to obtain the best generalization performance\nwhere ym is the pixel-wise label corresponding t0 the input image Cm:\nHowever; the naive cross-entropy loss only optimizes the relative relations between intra-class and inter-class dis- which ignores the absolute distance constraint be- tance, embeddings and semantic bases. That is to tween pixel say\n4. Experiments\nwe expect that the pixel embedding belonging to class c is closer to the c-th semantic base and is farther away from the semantic bases belonging to other classes. Inspired by learning [21, 23], metric we further propose variance and training objectives The discrimination terms aS tWO extra former is an intra-class cluster that the pixel embed pulls ding e belonging to class from the m-th source domain I towards the semantic bases pmn\n4.1. Datasets\nSynthetic datasets: GTAV [47] contains 24966 images with a resolution of 1914 x 1052 captured from the GTA-V game engine . Synthia [48] contains 9400 images with a res- olution of 1280 X 760 generated from virtual urban scenes_\ncontains 10004 images Real-world datasets. IDD [58] from x 968 captured with an average resolution of 1678 Indian roads.  Cityscapes [6] contains 5000 fine annotated images  with a resolution of 2048 1024 captured from 50 different cities primarily in Germany: BDDIOOK [70] contains 10000 image with a resolution of 1280 x 720 cap tures from different locations in US. Mapillary [38] contains 25000 images with an average resolution of 1920 1080 captured from all around the world.\nThe latter is designed in contrastive learning way which encourages the current cluster center em is closer to the c-th semantic bases Pc+ (i.e  positive keys) and to be far away from semantic bases belonging to other class pc _ (i.e , neg- ative keys):\n4.2. Implementation Details\nFollowing the previous work [5], we adopt DeepLabV3+ 15], MobileNetV2 [49] ResNet-101 [4] with ResNet-5O, and ShuffleNetV2 [35] backbones as our segmentation net where all backbones are pre-trained ImageNet works, on [8]. During the training phase, we adopt the SGD optimizer a momentum of 0.9 and weight decay of 5e [46] with The initial learning rate is set to 0.01 and is decreased using polynomial scheduling with power of 0.9. the We train all for AOK iterations, except for the three-source set models ting, the model is trained for 1OOK iterations In addition to some common data augmentations used in [5], we adopt extra strong style augmentations to enrich the style informa- tion of urban-scene images [32], which aims to enhance the proposed style projection ability in networks More details supplementary materials. can be found in our\nembedding) where is the cluster center (i.e  of mean pixel embedding em in the current feature and Fd is 1n > temperature hyper-parameter: By Eq: 12, we realize the alignment of semantic bases belonging to the same class existing pixel-wise from different domains Different from contrastive learning paradigm [64], the positive and nega tive samples in Eq: 12 are semantic bases rather than pixel embeddings. Thus, we don't need t0 construct memory bank to store sufficient embedding samples, which also sig- nificantly reduces the computational cost.\nTo achieve the online collocation of semantic informa- tion from source domains, we adopt the same momentum strategy update to update semantic bases Psem as follows:\n4.3. Results\nComparison methods We extensively compare our pro existing DGSS methods, which can posed method against be classified into three groups, including style augmenta tion (WildNet [25]), feature normalization/whitening (IBN Net 40], RobustNet [5] and SAN-SAW [42]), and meta- learning (MLDG [26] and PintheMem [20]). Since SAN SAW 42] and WildNet [25] are only implemented on the reproduce them on single-source setting in their paper; we setting In partic - comparison. the multi-source to make ular;  WildNet [25] utilizes the external dataset (i.e- Ima- geNet) to extend the style and content information of source Thus, we re-implement it by replacing the exter- domains. fair comparison nal dataset with the source dataset for which is marked with in our tables_\na the momentum coefficient. Like style bases, where we Psem also randomly initialize the semantic bases with zero training: distribution to start our mean\n3.4. Training and Inference\nDuring the training phase, we combine above three loss terms for the end-to-end training as follows:\nweighting coefficients to balance these where 6 and are training iteration, in addition to the three terms_ For each parameter update of the network @, the style and semantic bases are also updated online by Eq: 7 and Eq: 13.\nMulti-source   setting: the effectiveness To demonstrate of our proposed method, we first conduct contrast exper- iments O the multi-source DGSS setting, where multiple efficiently used to build a diverse source domains can be\nDuring the inference phase, we leverage Eq: 8 to obtain final pixel-wise predictions by the nonparametric cluster of pixel embeddings outputted from the learned network 0.\nMean Table 1. Source (G+S) Target (C, B, M): IoU(%) comparison of existing DGSS methods, where all networks with the ResNet-50 Avg backbone are trained with two synthetic (GTAV, Synthia) datasets_ The best and second best results are highlighted and underlined: T, Avg:-S and Avg -A denote the average results on target, source and all domains, respectively. Results with the f and t sign are from [5] indicates that we replace the external dataset (i.e  ImageNet) used in WildNet [25] with the source dataset for a and [20], respectively. fair comparison.\nTable 2. Source (G+S+I) Target (C, B, M): Mean IoU(%) comparison of existing DGSS methods, where all networks with the ResNet- 7 50 backbone are trained with two synthetic (GTAV, Synthia) and one real (IDD) datasets. Results with the t sign are from [20].\nrepresentation quantita- As listed in Table 13 space. we tively compare our results with existing DGSS methods on both target and source datasets, where all networks with the ResNet-50 backbone are trained with two synthetic datasets Remarkably,  compared and  Synthia). (i.e. GTAV with the state-of-the-art method (i.e , WildNet [20]), our method not oly shows superior generalization capability on target datasets (up to 3.69 mloU in average), but also significantly improve the performance o source datasets (up to 7.69 mloU); which demonstrates our method can enhance the representation ability of the learned model on both source and target domains Furthermore, We provide visual predic- qualitative comparisons as shown in Fig: 3. tion results for Our method obtains the best visual results on different target datasets. Following [20], we add one real dataset (i.e, IDD) to source domains to further verify the superiority of our method on more source datasets. As listed in Table 2, our method also outperforms existing methods on both source large and target domains by a margin:\nthe single-source setting to make a comprehensive compar- ison, where all network with the ResNet-50 backbone are trained with one synthetic (i.e. GTAV) dataset: As listed our method shows superior generalization per- in Table 3, existing DGSS methods formances over Compared with brings the naive baseline, our method approximately 149 gains in average 0n target datasets. mloU\nDifferent backbones: To demonstrate the wide applica- bility of our method, we compare our results with classic DGSS methods (i.e., IBN-Net [40] and RobustNet [5]) with different backbones. As listed in Table 4 our method shows superior performances 0n both large (i.e. ResNet-101) and lightweight (i.e-, MobileNet and ShuffleNet) backbones.\n4.4. Ablation Studies\nconduct  comprehensive We ablation studies with the source domains (i.e. GTAV ResNet-50 backbone on two following: and Synthia) as\nProposed strategies: As listed in  Table 5 our method shows the best generalization capability when two strategies\nSingle-source setting: We further implement our method in\nFigure 3. Source G+S) Target (C, B, M): Visualization comparison with existing DGSS methods on three different target domains:\nTable 4. Source (G+S) Target (C, B, M): Mean IoU(%) com- parison of existing DGSS methods with different backbones\nTable 3 Source (G) Target (C, B; M): Mean IoU( %) com- 5 parison of existing DGSS methods, where all networks with the synthetic (GTAV) ResNet-50 backbone are trained with the one dataset. indicates that we replace the external dataset (i.e. Im- ageNet) used in WildNet [25] with the source dataset for a fair comparison\nTable 5 . Ablation results for each strategy used in our method Sty.-Pro. and Sem.-Clu_ indicate style projection and semantic clustering, respectively:\nadopted at the same time. Remarkably, compared with are projection the first and second lines, we can find that style approximately bring 129 mloU gains in average over can the baseline, which fully demonstrates its effectiveness for the generalization on unseen domains_\nDifferent ways of style projection: As listed in Table 6 we investigate the effect of different ways of style projec- There are two intuitive ways as follows_ tion_ One way is using the naive instance normalization to project images from different domains into a normalized feature space (i.e. using Normalization) The other way is the extracted style bases to directly substitute the unseen style (i.e. Substitu- weighted combination of style We can find that the tion). bases can effectively enhance the representation of unseen style, producing better generalization on unseen domains.\nTable 6. Ablation results for different ways of style projection_\nperiments to demonstrate the effectiveness of two comple- mentary loss functions in Eq: 11 and Eq: 12. Compared with the naive cross-entropy loss, adding any complemen- bring the performance tary loss which  verifies gain, can supplement the main loss LCE: each of them can effectively\nLoss terms: As listed in Table 7 we conduct ablation ex-\nTable 7 Ablation results for each loss term\nFigure 4 t-SNE visualization of style statistics between different domains before (a) and after (b) style projection, where the style statistics (concatenation of mean and variance) is computed from the first layer's feature map of the ResNet-SO trained on two syn- thetic datasets_ Triangles indicate the preserved style bases\nTable 8. Comparison of computational cost_ Tested with the image size of 2048x1024 on one NVIDIA Tesla VIOO GPU. We average the inference time over 500 trials\n5. Discussion and Analysis\nDistribution analysis. We adopt the t-SNE visualization tool to analyze the effectiveness of OUr proposed style prO- jection and semantic clustering strategies As shown in we show the variations of style distribution between Fig 4 domains before and after style projection: different We can find that the style distribution of different domains is well separated before style projection (Fig: 4(a)) , while their style distribution is approximately constrained between two style bases after style projection (Fig: 4(b)), which demon- projection successfully projects unseen styles strates style into the style representation space built on style bases_\nFigure 5. t-SNE visualization of semantic statistics between dif- ferent classes (a) and domains (b), where the semantic statistics is computed from the last layer'$ feature map. Triangles indicate the preserved semantic bases_\n6. Conclusion\nFurthermore, we visualize the semantic distribution be- as shown in Fig 5. different classes and domains tween From Fig: S(a), we can find that pixel samples belonging to the same class are well clustered while those belonging In addition; the pre- to different classes are well separated. served semantic bases are approximately located in the clus- From Fig: 5(b), we can find that ter center of pixel samples_ these pixel samples from different domains are well clus- according to their classes, which demonstrates OUI se- tered mantic clustering successfully achieves the class prediction between different domains by the preserved semantic bases. Complexity of networks: As listed in Table 8, we com- pare the number of parameters and computational cost with existing DGSS methods_ Since we need to store style and semantic bases and estimate the similarity between them and unseen images, the number of parameters and compu- tational cost in our method are slightly higher than the naive competitive to ex- baseline. However; our inference time is iting DGSS methods due to the efficient implementation of distance measures by matrix multiplications.\nIn this paper a novel style projected clus- we propose tering method for domain generalized semantic segmenta- tion, which achieves the style and semantic representation of unseen images based on known data  In particular; style projection projects arbitrary unseen styles into the style rep resentation space of source domains and achieves the re- tention of specific style information between different do- Semantic clustering predicts the class of each pixel mains. by the minimal similarity distance to semantic bases, which realizes the semantic representation for unseen images and promotes the generalization ability: Through the evaluation multiple urban-scene datasets, we demonstrate the su- on generalization performance of our proposed method pperior existing DGSS methods. over\nThis work was supported in part by Acknowledgments: the National Natural  Science Foundation of China under Grants 62131003 and 620210013 gratefully And we aC- knowledge the support of MindSpore (https WWW mindspore cn\nReferences \n Now the paper ends. \nHow could the proposed style projected clustering method be extended to improve domain generalization for other tasks, beyond semantic segmentation?", "output": "The key ideas of the proposed method are style projection and semantic clustering, which achieve representation of unseen data based on known data. These ideas could potentially be generalized to other tasks beyond semantic segmentation by making the following modifications:\n\n1. Represent image styles in a way suitable for the task. Instead of mean and variance of shallow features, other style representations could be used.\n\n2. Build semantic representation spaces specific to the task. The semantic bases would correspond to the different classes or outputs of the task.\n\n3. Modify the semantic clustering to predict outputs appropriate for the task. Instead of predicting pixel-wise semantic classes, the method could be adjusted to predict object detections, image captions, etc. \n\n4. Use loss functions that match the task. The loss would need to be adapted for the specific task outputs, beyond pixel-wise cross-entropy loss.\n\nIn summary, the key concepts of style projection and semantic clustering could potentially be generalized to domain generalization for other tasks by modifying the specific representations, spaces and predictions in a way that matches the structure and outputs of the new task. The effectiveness would depend on how well the style and semantic information can be captured to achieve good generalization on unseen domains for that particular task."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nchain-of-thought Recent works have shown that (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However; prompt-based CoT methods are de- pendent on very large models such as GPT-3 17SB which are prohibitive to deploy at scale large In this paper; we use these models as rea- reasoning in soning teachers t0 enable complex smaller models and reduce model size require- magnitude. We pro- ments by several orders of pose Fine-tune-CoT, a method that generates reasoning samples from very large teacher mod- els to fine-tune smaller models_ We evaluate wide range of public models our method on We find that Fine-tune- and complex tasks. CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many Additionally, we extend OuT method by tasks. leveraging the teacher model's ability to gener- ate multiple distinct rationales for each original Enriching the fine-tuning data with sample_ such diverse reasoning results in a substantial performance boost across datasets_ even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models\nFigure 1: Fine-tune-CoT uses teacher-generated rea- soning to teach students. very large We prompt teacher model, such as GPT-3 17SB, to solve complex questions via zero-shot chain-of-thought reasoning: We then use the reasoning samples to fine-tune a much smaller student model. See Figure 2 for details.\nTo solve complex tasks, recent works show that it is possible to elicit reasoning abilities by prompting perform chain-of-thought (CoT) reason- LLMs to ing; generate i.e , series of intermediate reason- ing steps. This can be achieved by providing CoT demonstrations as exemplars in prompting Wei et al,, 2022b). More recently, Kojima et al. (2022) found that LLMs can be prompted to perform CoT reasoning simply by providing a natural language instruction t0 think step-by-step.\nIntroduction\nLanguage models (LMs) have demonstrated re- markable performance in a wide range of down- Recently, large language models stream tasks. (LLMs) have demonstrated in-context generaliza- capabilities:  performing downstream tasks tion simply by conditioning 0n few in-context exem plars or plain natural language task descriptions 2020; Sun et al,, 2021). Despite (Brown et al., these advancements, even the largest LLMs have been found to struggle with complex tasks which require multiple reasoning steps (Rae et al,,2021).\nmajor drawback of prompt-based CoT rea- soning methods, however; is their reliance on ex tremely large models that span hundreds of billions of parameters (Wei et al,, 2022b; Kojima et al. prohibitive to deploy These models 2022). are overwhelming computational at scale due to re- quirements and inference costs Wei et al,, 2022b).\n'Our code   implementation and data available are at https:Ilgithub com/itsnamgyulreasoning-teacher\nFigure 2: Detailed overview of Our proposed Fine-tune-CoT method. Step I: large teacher model is prompted a very to solve complex questions (yellow) by generating multi-step reasoning explanations green) . Step 2: completions are filtered based on the correctness of the final prediction (red) The question, rationale, and answer are used to multi-step solution. Step 3: the curated reasoning reasoning sample comprised of the prompt and compose small, lightweight student to exhibit reasoning capabilities The application of an samples are used to fine-tune reasoning LM-based teacher enables diverse generating multiple distinct rationales for each original sample to enrich the fine-tuning data. This boosts the performance of student models without any additional human annotation\nthat complex tasks can have multiple solutions with distinct reasoning Evans, 2010), we generate paths multiple reasoning solutions from teacher models using stochastic sampling to augment the training data for student models? . We find that this is a simple yet highly effective approach to maximizing student performance, which has not been explicitly recognized in concurrent works on fine-tuning with CoT reasoning Huang et al,,2022; Li et al,,2022b; Magister et al,, 2022; Fu et al,,2023).\nTherefore, we strive to enable complex reasoning in small models which are more feasible for large- scale deployment:\napproach named In this light; we propose an Fine-tune-CoT, which utilizes the reasoning capa- bilities of very large LMs to teach small models how We apply existing to solve complex tasks zero-shot CoT prompting (Kojima et al,, 2022) to generate rationales from very large teacher models, and use them to fine-tune smaller student models? We illustrate this in Figure 2 We note that stan- dard fine-tuning without rationales has been shown to be inadequate for solving reasoning tasks with small models (Talmor et al., 2018). While there have been attempts to fine-tune small models with hand-annotated reasoning steps (Nye et al,, 2021; they Cobbe et al,,2021), often require task-specific training setups and high-quality rationales which are costly to annotate (Wei et al,, 2022b). In con approach can be readily applied to novel trast, our downstream tasks without hand-crafted reasoning or task engineering:\n12 tasks using We evaluate our method on wide range of publicly available models We find reasoning per- that Fine-tune-CoT can elicit notable formance in small models while preserving much of the versatility of prompt-based CoT reasoning, which previously required > IOOB parameter mod- Wei et al,, 2022b). Diverse reasoning enables els in performance at the minor cost remarkable gains of additional teacher inference at development time , by exploiting our unique learning setup. This en- outperform larger ables models as small as 0.3B to students, and even the 17SB teacher model in some tasks. Our ablations show that performance is con-\nWe also propose a novel extension to our method, termed diverse reasoning, to maximize the teaching effects of Fine-tune-CoT: Inspired by the intuition\n?Diverse reasoning is orthogonal to existing data augmen - tation techniques Meng Yoo et al,, 2021; et al,, 2022) which aim to augment new question-answer pairs rather than diverse reasoning solutions for complex questions_\n2This can be interpreted as a variant of knowledge distilla- tion (Hinton et al,,2015)\nated by a very large model. Using various LLM- generated explanations for fine-tuning smaller mod- els has been successfully used in prior work (Li 2022a), with a focus on specific single tasks_ et al. mentioned a similar  approach to Also, is ours (Huang et al,,2022); however we note that this in concurrent work focuses on using Few-shot-CoT to self-generate fine-tuning examples by and for very large proprietary models. There is a brief glimpse into fine-tuning on smaller distilled models, but the results are limited to one dataset and very large teacher models that are inaccessible to the general community. In contrast; we provide a rich set of results and qualitativelquantitative analysis on using wide range of datasets, open-source models that are small and accessible to everyone.\nsistently scalable across all axes considered: di- reasoning, dataset size, teacher performance, verse This shows the potential and student model size. performance in of our method to enable reliable small models that are feasible for use in real-world applications Lastly, we conduct thorough sample studies and analyses which shed light on crucial details previous overlooked in fine-tuning for CoT and provide intuition on the emergence of reason- ing abilities in small models.\nRelated Work 2\ntransfer language Downstream in models\nMuch previous work established a \"pre-train and paradigm for enhancing LLM fine-tune perfor- mance on downstream tasks (Radford et al., 2018; 2019; Vaswani et al,, 2017; Devlin DDong et al.. et al,, 2018). However; fine-tuning is not always easily applicable (Hendrycks et al,, 2020). More recent literature exhibits a paradigm shift towards \"prompting\" the model to predict the desired out- put (Liu et al,, 2021; Raffel et al,, 2020). Large LMs can exhibit strong performance in this set- Brown et al,, 2020). For smaller models to be ting able to perform similarly, additional engineering is usually required (Gao et al,, 2021; Schick and Schiitze, 2021b; Schick et al,, 2020). For more using complex tasks, the idea of samples with ex plicit reasoning steps for fine-tuning a model (Nye et al,, 2021; Cobbe et al,, 2021) preceded the ap- proach of chain-of-thought (CoT) prompting (Wei et al,, 2022b), which enables very large LMs to perform well.\nKnowledge distillation Typically, knowledge distillation (KD) refers to training small models derived from large models in order to reduce model size and latency, while still preserving accuracy and capacity to generalize (Hinton et al,, 2015; Sanh et al,, 2019). Essentially, KD is a form of model compression, making efficient deployment capacity-limited devices possible (Bucilua et al. to 2006). We note that our work could also be con- sidered a distant variant of KD (Gou et al,,2021), improving prompt-based meth- similar t0 works on ods such as Yoo et al. (2021); Schick and Schiitze (2021b,a); Zelikman et al. (2022), or works on data- free distillation (Micaelli and Storkey, 2019; Nayak et al,,2019; Shen et al,, 2021), where the transfer data is synthetically generated from large teacher model.  Similarly, sequence-level distillation, i.e training a student model on sequence distributions of a larger teacher; can make neural machine trans- Kim and Rush; 2016). lation more efficient De spite being similar in spirit, our method still dis- tinguishes itself from such previous work: The role of the teacher model in our method is to teach the notion of intermediate reasoning: It is not the specific output that is the main supervising signal reasoning; but rather the generation'$ structure for Hence, we do not standard KD loss func- use tion that reflects trying to match the teacher output: Adding reasoning to this, we note that our diverse is also unusual in the context of KD, where it is e.g: sufficient in practice to only generate one teacher sequence for sequence level distillation.\nChain-of-thought reasoning In few-shot CoT prompting, the model learns to generate intermedi- ate reasoning steps that lead to & problem solution, being fed examples of step-by-step reason- after performance on a wide ing: This enables very good (Wang et al,, 2022). Addition range of tasks ally, LLMs can perform well in an unsupervised task-agnostic setting, using Zero-shot-CoT (Ko- requires no fine-tuning Or jima et al,,2022). This specific conditioning, and substantially outper- task learning and sometimes forms standard zero-shot even few-shot learning 0n a wide number of tasks.\nYet; work has shown that CoT prior re - quires extremely large models for optimal perfor- mance (Hoffmann et al,, 2022; Chowdhery et al showing 2022). we contrast this by In our work, how to utilize CoT reasoning methods for smaller rationales gener- models by fine-tuning them on\n3 Chain-of-Thought Fine-Tuning\nFinally; Step 3. Fine-tune fine-tune small we pre-trained student model on the assembled reason - training objective ing samples: We use the same during pre-training, i.e,, autoregres- of that used sive language modeling objective, Or next-token prediction (Radford et al,,2018).\nWe propose Fine-tune-CoT; task-agnostic ap- proach to enable chain-of-thought reasoning in The core idea is to generate reason- small LMs. ing samples from very large teacher models using CoT prompting and subsequently fine-tune small student models using the generated samples. This approach preserves the versatility of prompt-based CoT methods while overcoming their reliance on prohibitively large models. To maximize versatility and minimize teacher inference costs, we use the task-agnostic Zero-shot-CoT prompting method (Kojima et al,, 2022) on teacher models, as it does long require any reasoning examples o infer- not We discuss our choice of teacher ence context. CoT prompting method in Section 7.3. In the fol- lowing, we characterize Fine-tune-CoT in three We also provide a visual overview distinct steps. in Figure 2_\nDiverse reasoning To maximize the teaching ef- fects of Fine-tune-CoT; we can generate multiple reasoning explanations for each training sample. This approach is motivated by the intuition that multiple reasoning can be used to solve com- paths plex tasks, i.e., type-2 tasks (Evans, 2010). We posit that this unique feature of complex tasks, in tandem with the stochastic generation abilities of reasoning the teacher model, can enable diverse to significantly boost reasoning supervision sim ply through additional teacher inference. In de- given sample Si, instead of applying tail, for Zero-shot-CoT using greedy decoding to obtain a single explanation-answer pair (ei, wi) , we use a stochastic sampling strategy, i.e,, temperature sam- pling with large T, to obtain D distinct generations {(rij, Gij)}? . Subsequent reasoning sample cura- tion and fine-tuning then proceed as before. We refer to D as the degree of reasoning diversity. A Wang et al. (2022) similar approach is used in Huang et al. (2022), where multiple CoT outputs generated and marginalized to find the optimal are answer. However; the effects of such diverse rea- soning on teaching student models has not been acknowledged or thoroughly investigated in con- current work (Huang et al,,2022; Li et al,, 2022a; Magister et al,, 2022; Fu et al. 2023). We note that diverse reasoning imposes an important trade- off between the development cost and inference costlquality of student models which we discuss in Section 5.3.\nStep 1. Reasoning generation First;, utilize we large teacher model to generate CoT reasoning explanations for a given task: Consider a standard sample Si consisting of a question qi and its true Using Zero-shot-CoT4 we prompt the answer Wi- reasoning explanation teacher model to generate rationale, Ti to solve question gi and make a final or resulting text sequence prediction Gi- The answer including the prompt and generations, takes the following form: \"Q: <qi> . A: Let's think step by <ai? step. <ri> Therefore, the answer is\nStep 2. Curation Next; we filter the generated samples and reformat them into prompt-completion For filtering, we simply compare the final pairs. prediction of the teacher model ai with the ground- following previous works (Zelik- truth answer Wi, man et al,, 2022; Huang et al,, 2022). Note that this filtering incurs some loss of training samples. where &i repackage For all instances we Ci, = into a reasoning sample Si (Si,ri, 6i, (pi, Ci), a prompt-completion pair:  To maximize inference- special-character based de- time efficiency, use we limiters to minimize token usage. Specifically, Pi ~Zqi> and ci each take the form of and <ri> ### We note that answer-based filter- ~~> <ai> END\" ing does not ensure the correctness of the ratio- nales, especially for multi-choice questions We provide an analysis in Appendix E.l regarding this important detail which has not been addressed in concurrent work.\nExperiments 4\nTasks and datasets We evaluate our method on 12 datasets pertaining to four categories of complex reasoning, following Kojima et al. (2022). These include arithmetic (SingleEq, AddSub, MultiArith, GSMSK, SVAMP), other (Date Understanding; Tracking Shuffled Objects), symbolic (Last Let- ter Concatenation, Coin Flip), and common sense (CommonSenseQA , StrategyQA) reasoning: We provide details and references in Appendix B.\nNote that Zero-shot-CoT is itself a two-step prompting method. The reasoning (blue) is generated in the first step and answer prediction (red) is generated in the second step.\nof OpenAI models on 12 tasks under Fine-tune-CoT (with Table 1: Fine-tune-CoT Performance. Accuracy 9 refers to random-guess performance derived based on the diverse reasoning) and baseline methods_ Random reasoning, we report results for maximum degree D considered: number of choices in multi-choice tasks. For diverse 8 for other datasets. We omit diverse reasoning for large D 64 for MultiArith and SVAMP; D datasets due to Tracking Shuffled Objects due to absence of prompts. resource constraints and Few-shot-CoT for\nFor teacher models, use four Models vari- we Brown et al,, 2020) , provided ants of GPT-3 175B by the OpenAI API. Unless otherwise stated, we text-davinci-002 based on InstructGPT use 175B (Ouyang et al,, 2022) as the teacher for Fine- tune-CoT. For student models, we consider four experiments, popular model families. For our main they babbage, curie} as we use GPT-3 {ada, readily available for fine-tuning via the Ope- are nAI API: Due to the blackbox nature of the API; we also consider various open-source models un- der controlled settings. We use GPT-2 {Small, Medium; Large} (Radford et al,, 2019) and T5- {Small, Base, Large} (Raffel et al,, 2020) as rep resentative model families for decoder-only and encoder-decoder architectures, respectively: We also use the instruction-tuned version of T5, Flan - TS-{Small, Base, Large} (Chung et al, 2022), to in- vestigate the effects of instruction tuning on student models, to applying Fine-tune-CoT: These stu- prior dent models are 25-2500x smaller than the teacher model, thus considerably more feasible for real- We provide details on models world deployment: and API usage in Appendix C.\nTable 2: CoT methods are Taxonomy of methods. Few- interpretable due to reasoning output: While more shot-CoT can utilize few in-context samples, fine-tuning can utilize any number of training samples via model Fine-tune-CoT benefits from the reasoning updates. capabilities of teacher models\nzero-shot prompting: For vanilla fine-tuning, we format the prompt and completion as <qi> ###' and respectively. We clarify the taxon <ai> END' For text generation, omy of methods in Table 2 we greedy decoding following Wei et al. (2022b); use Kojima et al. (2022) throughout our experiments, except for diverse reasoning: For diverse reasoning on the teacher; we use temperature sampling with following Wang et al. (2022). We provide T = 0.7 experimental details in Appendix A\n4.1 Results\npresent the reasoning In this section, perfor-, we using mance of models Fine-tune-CoT and diverse reasoning: We compare with various baselines and demonstrate the scalability of our method across four axes: degree of diverse reasoning (Figure 3), dataset size (Figure 4), performance of the teacher (Figure 5), and size of the student model (Figure 6)-\nWe provide a comparison of Baseline methods Fine-tune-CoT (ours) with four baseline methods: fine-tuning, standard zero-shot prompting; vanilla Zero-shot-CoT (Kojima et al,, 2022) , and Few-shot- CoT (Wei et al,, 2022b). Given a training sample {(qi, Gi simple format \"Q: <qi> for fi, we use a\nfindings on GPT-3 models in the We present our main text and defer results on open-source models Appendix G, with a brief summary at the end of to this section_\nelicits   complex   reasoning   in Fine-tune-CoT Table 1 summarizes the accuracy small models of student models using the proposed Fine-tune- CoT; compared to prompt-based CoT baselines as fine-tuning: well as standard While Zero-shot-CoT exhibits remarkable performance on the very large 17SB model (Kojima et al,, 2022), it fails to en- able complex reasoning in all three smaller mod- els, showing near-negligible performance across all tasks. We also find that small models are unable approach these tasks under standard zero-shot to prompting: On the other hand, Fine-tune-CoT elic - its notable reasoning performance, demonstrating using significant gains over Zero-shot-CoT when smaller models and outperforming both fine-tuning and Few-shot-CoT in more than half of the tasks_ For complex arithmetic, Fine-tune-CoT achieves a notable 33% accuracy on MultiArith while Zero- shot-CoT only reaches 5%. Few-shot-CoT and fine-tuning only achieve 10% ad 159, respec- reasoning tasks, our tively. For two commonsense method outperforms the near-random performance of Zero-shot-CoT by 379 and 5%, respectively: Furthermore, it surpasses Few-shot-CoT on Com monSenseQA by 329 and performs similarly on StrategyQA. We observe that Fine-tune-CoT perfor- mance is most notable for tasks that are not overly complex, which include other reasoning tasks (Date Understanding, Shuffled Objects) and symbolic rea- soning (Last Letter; Coin Flip), significantly out- performing other baselines. See Appendix Table 9 for performance of all students.\nFigure 3: Diverse reasoning performance: Accuracy (%) of GPT-3 student models under Fine-tune-CoT with varying degrees of diverse reasoning D. Baseline per- formance of the largest model under vanilla fine-tuning and Few-shot-CoT are shown for comparison. Diverse applicable to the baselines. reasoning is not\nDiverse reasoning substantially improves Fine- tune-CoT performance: To examine the learn- ing effects of diverse reasoning and compare it with two baselines given by fine-tuning and Few-shot- CoT; we apply Fine-tune-CoT using 1-64 reason- ing explanations per sample across three model SVAMPS . Figure 3 shows scales on MultiArith and that diverse reasoning can significantly improve the performance of student models using Fine-tune- CoT: For the 6.7B student model, we find a boost of around 26% on MultiArith; and around 17% on using diverse reasoning SVAMP We also note that always leads to outperforming the baseline within the respective model size, and can even boost per formance of our method beyond that of a larger model that does not use diverse reasoning: This even includes the teacher in two cases (Date Under- standing, Last Letter). Moreover; we find that di- reasoning can boost the performance of Fine- verse tune-CoT to surpass that of both Few-shot-CoT and vanilla fine-tuning across all model sizes. We that due to our focus on complex tasks, the di- posit versity of reasoning and linguistic templates paths substantially aid in teaching student models to can reason\nSmall models can outperform very large teach- reasoning ers in Table 1 also shows that Fine- tune-CoT is highly effective 0n small models com- pared to the large 17SB teacher model. For the tasks Shuffled Objects and Coin Flip, Fine-tune- outperform the teacher model CoT is shown to lusing either 1.3B or 6.7B parameters, i.e. reduc - ing the number of required parameters by approx 25-100x. We also find that Fine-tune-CoT with the very small 0.3B model consistently outperforms the 6.7B model under Zero-shot-CoT; demonstrat- ing that our method is able to unlock a wider range 'capabilities compared to the baseline, even when of model size is vastly reduced:\nFine-tune-CoT consistently benefits from more We perform an ablation on dataset size to data: study the performance scalability of our method We see that the performance of with dataset size.\n5For diverse reasoning; we generate teacher rationales stochastically with T 0.7 instead of greedy decoding, which accounts for small differences in absolute performance num- bers between Table 1 and diverse reasoning with D\nFigure 4: Effects of dataset size. Accuracy of the % GPT-3 6.7B student model by dataset size under vanilla fine-tuning vs Fine-tune-CoT (with diverse reasoning) Baseline performance under Few-shot-CoT is shown reasoning is not applicable for comparison: Diverse show diverse reasoning We to standard fine-tuning performance with D = 64 for MultiArith and SVAMP; D 8 for others_\nFigure 5: Effects of teacher performance on students: %) of teacher models (Zero-shot-CoT) and Accuracy their corresponding GPT-3 6.7B student models (Fine- tune-CoT) Baseline performance under vanilla fine- tuning and Few-shot-CoT are shown for comparison applicable to Few-shot-CoT Teacher models are not which uses few human-annotated examples.\nFine-tune-CoT performance scales with model Finally, we explore the effect Size for small LMs of scaling up student model size 0n our method, and compare it with the effects of increasingly larger student models in Few-shot-CoT as well as vanilla fine-tuning: We can observe that the performance of Fine-tune-CoT is consistently scalable with stu- dent size (Figure 6). In contrast; the two baselines do not always exhibit the same behavior: in Date Understanding, neither Few-shot-CoT nor vanilla fine-tuning results in scalable performance.\nthe 6.7B model clearly scales with the size of the In comparison, dataset; independent of the task vanilla fine-tuning does not always exhibit this be- havior: In fact, for Date Understanding, we find that an increase in dataset size harms the performance fine-tuning: Furthermore, Fine-tune-CoT sees of additional benefits from diverse reasoning, which applicable in standard fine-tuning: is not\nNext; we Better reasoners are better teachers can ask the question of whether the performance of the teacher is correlated with that of their student when using Fine-tune-CoT: To test this, we use dif- keep- ferent versions of GPT-3 as teacher models, ing the size of the student model constant at 6.7B parameters (Figure 5). We find that student per- formance indeed scales with teacher performance, particularly in the less complex tasks Date Under- standing and Last Letter: There, the performance of the student matches the performance of the teacher very closely. This also fits with our observations in Appendix D, which show that the successes and failures of teachers are correlated with those of We note that this scaling effect is the students. given in knowledge distillation, in contrast not where more accurate teachers do not always result in better students (Menon et al., 2021).\nResults on open-source student models Over- findings on TS, Flan-TS, and GPT-2 show all, our similar trends to those observed on GPT-3. Small models exhibit near-random performance under standard zero-shot or CoT prompting in nearly all Notable, find that encoder-decoder mod- cases we TS and Flan-TS, show noteworthy els, i.e. perfor fine-tuning, suggesting that mance under standard causal masking may be a bottleneck to reasoning in decoder-based language models in the absence of CoT output: Fine-tune-CoT consistently outper- prompt-based baselines and is comparable O forms superior to vanilla fine-tuning: Diverse reasoning improves performance even further; often exhibit- ing significant gains  We report our full findings on open-source models in Appendix G.\nfiltering suggest However; ablations on rationale that these incorrect rationales can aid in student Secondly, supervision (Appendix E.l) find we that common maximum sequence lengths used for CoT generations often lead to incomplete answers_ We observe that reasoning length differs among datasets, and longer generations typically improve accuracy; but may not be beneficial for fine-tuning (Appendix E.2) Lastly, we find that many datasets comprised of samples that share common tem- are plates, potentially compromising the validity of To address this, we our random train-test splits evaluate our method on manual template-wise data splits, and confirm that students retain meaningful reasoning capabilities (Appendix E.3).\nFigure 6: Effects of student model scale. Accuracy 9 of GPT-3 student models of various sizes under Few- fine-tuning, and Fine-tune-CoT (with shot-CoT; vanilla diverse reasoning). The hatched portion indicates the using performance boost of Fine-tune-CoT when diverse reasoning with D 64 on MultiArith and SVAMP; D = 8 on others_\nDiscussion 5\nAccessibility of Fine-tune-CoT 5.1\nOwing to the versatility of the teacher generation method, i.e., Zero-shot-CoT; our method can be readily applied to any complex task without task- specific engineering: Rationales can be readily gen using publicly available APIs such as those erated provided by OpenAI or Anthropic. This makes it viable to obtain CoT training data in low-resource which not oly outperforms standard scenarios, fine-tuning; but elicits the student to output inter pretable explanations. Fine-tuning and inference performed on much on student models can also be more accessible hardware, in contrast to very large models. This can reduce long-term inference costs and minimize environmental impact while making wide community: our method fully accessible to\nAnalysis 4.2\nSample study To identify the strengths and weak- perform thorough sam- nesses of our method, we ple study across all datasets and methods. Across large portion all arithmetic tasks, we find that a of errors arises from calculations. MultiArith and SVAMP also show many semantic errors, but these are significantly reduced with diverse reasoning: For difficult tasks such as GSMSK and AQUA, we found that all methods tend to struggle. We found that our method is highly effective in text-based tasks, excluding commonsense reasoning, as well as tasks that contain common linguistic patterns On the other hand, we find that students under Zero-shot-CoT often repeat questions O produce incoherent repetitive statements_ While Few-shot- CoT elicits step-by-step sentences, the student mod- els rarely seem to understand the semantics of the question, and generations often contain logical or For details o our sample commonsense errors_ study; see Appendix D.\nViability of Fine-tune-CoT 5.2\nWhile Fine-tune-CoT elicits notable complex rea soning capabilities in small models, performance on some difficult datasets would not be consid- ered viable for real-world use, such as 30.33% on findings in Section 4.1 in- SVAMP However; our dicates significant potential for improvement, as our method is shown to be uniquely scalable with (1) diverse reasoning, (2) dataset size, (3) teacher model performance, and (4) student model size reasoning and better teacher The use of diverse models is especially promising, as these can bene- fit from improved teacher LLM performance and inference costs in the future. In addition, it is pOS sible to incorporate recent CoT methods, which lead to significant performance improvements, in student models, which we discuss in Section 7.3.\nNuances of fine-tuning on CoT reasoning We shed light on nuances that have often been over- looked in previous O concurrent work (Wei et al. 2022b; Li et al,, 2022a; Magister et al,, 2022). First; acknowledge the possibility that correct sam we ples may contain incorrect reasoning: In fact, we find that 27.6% of correct teacher completions for Date Understanding contained reasoning errors_\n5.3 Tradeoffs of Fine-tune-CoT\nof supervision also lead to a gradual reduction in example, reasoning errors_ For we found a clear distinction between Zero-, Few-shot-CoT and Fine- tune-CoT (with diverse reasoning) in the frequency and severity of semantic errors, i.e,, understanding complex questions, and calculation errors. This suggests that explicit supervision on reasoning can also lead to the emergence of reasoning abilities Second, we qualitatively find that students show ca pabilities that are reminiscent of the larger teacher model. We found that students can recognize com- mon semantics and reasoning cues of the given task; and is able to imitate the process of splitting large tasks into subtasks This suggests that it is possible to learn reasoning abilities pertaining to a particular domain: that this is possible in We posit small models due to the limited domain of reason- ing, and may not be applicable in reasoning tasks that require large domains of knowledge.\nThe aforementioned opportunities to enhance Fine- tune-CoT also pose many important tradeoffs. We leave further analysis to future work:\nDegree of diverse reasoning The performance benefits of diverse reasoning come at the cost of Therefore, diverse additional teacher inference. reasoning poses tradeoff between development cost vs inference costlquality In other words, per- formance gains from diverse reasoning may be uti- lized to enhance student performance O alleviate the need for larger student models. This must also be taken into account for fair evaluation of similar distillation methods in the future\nData acquisition Data annotation and diverse reasoning can both be used to enlarge fine-tuning data, but each have their associated costs_ We note that the cost of diverse reasoning is linear to the number of generated rationale and the number of original samples. Despite this, it can still be a cost- effective alternative to hand-annotating additional preliminary cost analysis in Appendix F data. A shows that the pareto front of data-acquisition-cost performance always incorporates diverse reason- to ing: We expect that the cost benefits of diverse reasoning will continue to improve with improve- ments in teacher model performance and efficiency:\nof   emergent Distillation abilities Chain-of- thought reasoning has been recognized as a prime example of emergent abilities in very large lan- findings guage models (Wei et al,, 2022a). Our ability; show that it is possible to distill this un- der certain domains, to much smaller models sim- ply through fine-tuning: The potential for distilla - tion implies that future advancements in language models may lead to emergent abilities that are not only pertinent to those larger models, but could cascading benefits to also have a broader impact, smaller models.\nEmergence of CoT reasoning 5.4\nThe emergence of abilities such as CoT reasoning point of interest in recent works has become a Wei et al ,2022b,a; Schaeffer et al,,2023). We note that the efficacy of Fine-tune-CoT on small models does not disprove this emergence, as our method is based fine-tuning: However; we believe our results can on provide some insight into this phenomena.\nConclusion 6\nWe have proposed Fine-tune-CoT; method that uses LLMs as reasoning teachers to transfer the broad reasoning capabilities previously found in >I00B models to student models as small as 0.3B We propose diverse reasoning as novel approach teaching effects, exploiting the to maximize these unique characteristics of this new learning setup to vastly improve performance. Our extensive ex- periments show that Fine-tune-CoT elicits signifi reasoning performance in small models, thus cant demonstrating the distillation of CoT reasoning which has been considered an emergent ability of By leveraging publicly available models scale. with zero-shot prompting, we demonstrate a task- agnostic approach to elicit reasoning performance in small models, making complex reasoning feasi- ble for real-world deployment and accessible to the broader community:\nWhy does Fine-tune-CoT work in small mod- In a seminal work, Wei et al. (2022b) sug els? gests that CoT reasoning is an emergent ability of specifically, a complicated phenom scale more involving variety of emergent abilities, such ena as semantic understanding, symbol mapping, arith- metic ability: However; our sample studies suggest that Fine-tune-CoT elicits these emergent abilities even in relatively small models (see Appendix D) We explain this from two perspectives. First, Wei et al. (2022b) demonstrated the emergence of rea- soning abilties by identifying a reduction in the larger frequency of reasoning errors with model Similarly, we find that more potent forms scale.\nLimitations\nHowever; 2022)_ our choice to use Zero- et al: shot-CoT to generate reasoning samples from the teacher model is motivated by the fact that Few shot-CoT requires a significantly larger inference With the current pricing models based context. on token usage, the typical setup of 8-shot CoT approximately 8 times more compared would cost to Zero-shot-CoT: Therefore, we see a tradeoff be- using` the inference budget for Few-shot-CoT tween reasoning with Zero-shot- using and it for diverse CoT: On the other hand, we also note that recent works introduce various ways to improve CoT rea- soning performance substantially (often to near- perfect levels), which can be applied to our student These include refinement over repeated models. inferences (Wang et al,, 2022; Li et al,, 2022b) and self-improvement (Zelikman et al,, 2022; Huang (Wang et al,, 2022). In particular; self-consistency et al,, 2022) can be utilizied on unlabeled samples teaching signal. In contrast; to maximize the we aim to achieve CoT reasoning without the infer- ence time cost incurred by very large LMs. Future work is needed to incorporate these methods into minimizing development Fine-tune-CoT while and inference costs\n7.1 Towards concise answers\nSample studies show that rationales output from may occasionally be repetitive student   models and digressive. This is undesirable in terms of inference-time efficiency as well as interpretability: optimization to inference computation, As a minor fine-tuning sample templates uS we construct our ing special-character based delimiters instead of Huang natural language used in concurrent work et al,, 2022) to minimize sequence length. Prelimi- nary findings showed this had no significant impact reasoning performance. More importantly, it is on desirable to train student models to generate con cise answers in terms of substance. Appendix E.2 hints at the possibility for this, showing that fine- reasoning samples causes the tuning o shorter student model to also produce shorter rationales.\nExploring a wider array of models 7.2\nWe note that the performance of our method is currently not state-of-the-art: However; it can ben- efit from advances in teacher models as well as example,  future other prompting  methods. For array of teachers, such work should include a wider as the highly versatile ChatGPT, which typically long generates detailed responses that may be able to impart more knowledge to the student:   More recent models such as GPT-4 have demonstrated significant advances in complex reasoning abili- ties, which may improve the efficacy of Fine-tune- CoT on very difficult datasets, such as GSMSK Conversely; our method could prove even more advantageous when to recent models with applied improved efficiency, such as those based on the re- cent LLaMA model (Touvron et al., 2023), which has sparked a proliferation of work focused on com pact language models. Both of these avenues are promising for future work:\nConnection with knowledge distillation 7.4\nWe assume that there is a lot of potential in strength- ening the connections between knowledge distilla- tion and our method. We have already seen in this work that our method shares some characteristics such as the fact that the knowledge of with KD; using intermediate reasoning imparted by also in correct samples can have positive effects on student accuracy; akin to \"dark knowledge\" (Menon et al. 2021) that is transferred by training on teacher out- put logits and not one-hot labels. We have seen that this leads to a quantity-quality tradeoff when it ability of the student model to gener- comes t0 the alize: having fewer but perfectly curated reasoning samples is not necessarily as helpful as having a larger amount of reasoning samples that might not always be fully correct: On the other hand, we have also found that more accurate teachers do lead to more accurate students, which is not always the case in KD (Miiller et al,,2019). It would therefore be of interest for future work t0 formalize the con nection of Fine-tune-CoT with classic KD methods, and potentially test the use of a different distillation loss function that takes the teacher's actual output into account.\n73 Better CoT inference methods\nreasoning and better teacher use of diverse The or student models is especially promising, as it is possible to leverage future improvements in model performance and decreased inference costs. How can also consider other ways to boost ever; we performance, such using different prompting as methods. For example, previous work shows that Few-shot-CoT (Wei et al,, 2022b) can improve ac- curacy over Zero-shot-CoT by wide margin, eg going from 78.7% to 93.0% on MultiArith (Kojima\nReferences \n Now the paper ends. \nHow can the Fine-tune-CoT method be improved to generate more concise and interpretable reasoning steps from student models?", "output": "Based on the discussion in the paper, there are a few potential ways to improve Fine-tune-CoT to produce more concise reasoning from student models:\n\n1. Use shorter CoT output from the teacher model as fine-tuning samples. The paper notes that fine-tuning on shorter teacher rationales tends to produce shorter student rationales. So using more concise CoT samples from the teacher could encourage conciseness in the student.\n\n2. Optimize the student's fine-tuning objective for sequence length. The paper currently uses a standard auto-regressive language modeling objective. Adding a penalty for long sequences could push the student to be more concise.\n\n3. Directly train the student to generate concise reasoning. For example, the student could be explicitly optimized to minimize the number of reasoning steps or the total number of tokens in its CoT output.\n\n4. Improve interpretability through data augmentation. Techniques like paraphrasing could be used to augment the training data with multiple concise phrasings of the same reasoning, improving the student's ability to produce interpretable output.\n\nIn summary, there are a number of ways the Fine-tune-CoT method could potentially be extended to encourage more concise and interpretable reasoning from student models, including optimizing the teacher output, fine-tuning objective, and training data for conciseness. Future work is needed to explore these directions and evaluate their effectiveness."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\nhave been also developed, showcasing the superi- lority of legal-oriented PLMs over generic ones on downstream legal NLP tasks\ndetailed   analy- In this work, conduct we performance of legal-oriented pre- sis on the trained language models (PLMs). We exam the interplay ine their original   ob- between jective,  acquired  knowledge, and  legal  lan guage understanding capacities which we de- fine as the upstream, probing, and downstream performance, respectively: We consider not size but also the pre-training only the models corpora used as important dimensions in our study: To this end, we release multinational English legal corpus (LEXFILES) and legal knowledge probing benchmark (LEGALLAMA) to facilitate training and  detailed analysis of legal-oriented PLMs. We release two new le- gal PLMs  trained on LEXFILES   and evaluate them   alongside   others LEGALLAMA and on We find that probing performance LEXGLUE: strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by legal  knowledge the model's   size and prior which can be estimated by upstream and prob- findings, ing performance. Based these on we can conclude that both dimensions are im portant for those seeking the development of domain-specific PLMs.\nDespite this impressive progress, there is still thorough study o (a) how PLMs trained not under different settings (pre-training corpora, size of the model) perform across different legal sub- corpora, and (b) what sort of knowledge such mod- acquired from pre-training, and (c) how els have important is domain (legal) specificity Vs general (cross-domain) legal knowledge. Furthermore, of- ten times, legal NLP relies o datasets without drawing clear lines and comparisons between the they - various legal systems legal sys may reflect: A tem may be defined as a set of rules adopted and enforced at a given governance level, which may be national, regional or international (Friedman and Hayden, 2017), e.g,, UK, EU, US, CoE, etc\nWe define the upstream evaluation as the task PLMs are explicitly designed to do: Masked Lan- Modelling (MLM) (Devlin et al, 2019). We guage then probe for specific legal concepts that are legal- system specific, in a similar fashion as Petroni et al. using the \"LAnguage Models Analy (2019) did sis\" (LAMA) framework.   Finally, we assess the PLMs performance in LexGLUE (Chalkidis et al. 2022a) downstream tasks  More importantly, we explore how the aforementioned factors (upstream, and probing performance) interplay and relate to downstream performance. Our contributions are:\nIntroduction\nFollowing closely the advances in the development of NLP technologies, the legal NLP literature is flourishing with the release of many new resources including large legal corpora (Henderson* et al. 2022) , datasets (Chalkidis et al,, 2021a; Koreeda and Manning, 2021; Zheng et al,, 2021; Chalkidis et al,,2022a; Habernal et al,, 2022) , and pre-trained legal-oriented language models (PLMs) (Chalkidis Zheng et al,, 2021; Xiao et al,, 2021). et al,, 2020; Benchmark suites (Chalkidis et al,, 2022a; Hwang et al,, 2022; Niklaus et al,, 2023) to evaluate the performance of PLMs in a more systematic way\nWe release LEXFILES , new diverse English (a) corpus  including 11 sub-corpora legal that legislation and case law from 6 primar- cover ily English-speaking legal systems (EU, CoE, Canada, US, UK, India). The corpus comprises approx. 6 million documents which sum up to approx. 19 billion tokens. We release 2 new legal-oriented PLMs, dubbed (b) LexLMs, warm-started from the RoBERTa (Liu et al,, 2019) models, and further pre-trained on the LEXFILES for IM additional steps\nEqual contribution\npresent the sampling Table 1: Core statistics of the newly introduced LEXFILES corpuS_ In the last column we smoothing percentages used t0 train our LexLM models (Section 4.1).\nJustice (CJEU), published in EUR-Lex. UK Legislation. We release 52.5 UK laws pub- lished in UKLEGISLATION.GOVUK, the official website of the UK National Archives.2 (d) UK Case Law: We release 47K UK court deci- sions published in the British and Irish Legal Information Institute (BAILII) database.3 US Legislation. We re-distribute 518 US state (e) statutes (legislation) originally published by Henderson* et al. (2022). US Case Law: We release 4.6M US decisions (opinions) published by Court Listener;4 web database hosted by the Free Law Project:5 US Contracts We release 622K US contracts (agreements) obtained from US Securities and Exchange Commission (SEC) filings, which publicly available from the SEC-EDGAR' are database. (h) Canadian Legislation. We release 6K Cana- dian laws (acts, regulations) published in the oflicial legislation portal of Canada. Canadian Case Law: We re-distribute 13.SK Canadian decisions (opinions) originally pub- lished by Henderson et al. (2022). (j) ECtHR Case Law: We release 12.SK decisions ruled by the European Court of Human rights\ndiverse probing We release LEGALLAMA, (c) benchmark suite comprising 8 sub-tasks that acquaintance of legal knowl aims t0 assess the edge that PLMs acquired in pre-training: 7 PLMs on both LEXFILES and (d) We evaluate LEGALLAMA, analyzing their performance out of the box per LEXFILES sub-corpus and LEGAL- LAMA tasks_ We also fine-tune and evaluate these models in selected LEXGLUE tasks, and examine the interplay between MLM, probing, and downstream performance.\nLeXFiles Corpus 2\nThe LEXFILES is a new diverse English multina- tional legal corpus that we created including 11 distinct sub-corpora (Table 1) that cover legislation and case law from 6 primarily English-speaking legal systems (EU, CoE, Canada, US, UK, India). The corpus contains approx: 19 billion tokens. In comparison, the PILE OF LAW cOrpus released by Henderson* et al. (2022) comprises 32 billion in total, where the majority (26/30) of sub-corpora come from the United States of America (USA)  hence the corpus as a whole is biased towards the US legal system in general, and the federal or state significant extent jurisdiction in particular, to The LEXFILES '$ sub-corpora are:\nEU Legislation. We release 93.7K EU laws (a) (regulations, decisions, directives) published in EUR-Lex, the website of the EU Publica- tion Office. We release 29.8K EU court (b) EU Case Law: decisions, mainly issued from the Court of\nhttps legislation. gov uk WWW\n'https 'WWW. bailii.org\n'https: courtlistener com WWW\nSWe release decisions published from 1965 on-wards (cf: post Civil Rights Act), as a hard threshold for cases that possi- bly rely on out-dated and discriminatory law standards_ The rest of the sub-corpora include more recent documents_\n6https edgar sec. gov_ WWW\n'https: leur lex. europa eu/\nhttps lois.justice.gC.ca_ /laws eng_\npublished in HUDOC,8 the database (ECtHR of ECtHR (k) Indian Case Law:   We include 34.8K Indian Supreme Court cases originally published by Malik et al. (2021).\nThe LEXFILES is pre-split into training and test provide ground for comparing the subsets to a fair performance of PLMs that have not been trained in training set: training subset of the the We use the LEXFILES COrpUS t0 train 2 new transformer-based languages models, dubbed LEXLMs (Section 4.1), and evaluate their MLM performance across many other already available PLMs (Section 4.2).\nFigure 1: Example from the  Terminology (US)' sub- ~drug trafficking\" has task Multi-token LAMA where been replaced with two [MASK] tokens. Given the rank- ings of each predicted token, we compute the recipro- cal rank (RR) and obtain a mean reciprocal rank (MRR) over the [MASK] tokens_\nLEGALLAMA Benchmark 3\nLAnguage Model Analysis (LAMA) (Petroni et al. 2019) is probing task that is designed to assess specific capabilities of PLMs The general frame- work of LAMA is to let PLMs predict a target to- ken behind a [MASK] given its context, e.g;, Paris is the capital of where the answer is [MASK] new probing bench- France' LEGALLAMA is mark suite inspired by this framework. It includes acquaintance of 8 sub-tasks that aim to assess the legal knowledge that PLMs acquired in the pre- training phase in a zero-shot fashion. Such tasks cannot be resolved by laypersons Or even law pro- fessionals that are not experts in the specific fields acquaintance of le- of law in many cases_ The knowledge can be interpreted as some form of Igal primitive understanding of the law, specific for as pects in very controlled (limited) settings ~limited specific jurisdiction -. legal concepts under a As Sahlgren and Carlsson (2021) mentioned:\nperformance of PLMs We evaluate the overall lusing the macro-averaged Mean Reciprocal Rank Voorhees and Tice, 2000) over the set of (MRR) labels (not the entire vocabulary). 10 In the case of multi-token targets, we average the MRR over 11 the predicted tokens_ Note that LEGALLAMA examples come from the test subset of the related LexFiles sub-corpora in order to have a fair compar- ison between models trained or not on the LexFiles training sets. We provide a concrete example in Figure 1, and describe the tasks in detail:\nArticles (CoE) ECHR In this task; have we paragraphs from the court assessment section of paragraphs ECtHR decisions. We extract those ifrom the newly introduced ECHR corpus presented The paragraphs include references to in Section 2. ECHR articles, e.g,, \"Article [MASK] of the Con- where [MASK] is the article number: For vention applicant complained under Article example, \"The [2] of the Convention that the prison authorities had failed to protect her son'$ right to life by tak- Given a paragraph, ing the necessary measures: where the article number is masked, the model has predict the associated article number given the to context: The dataset is composed of 5,072 test in- stances containing on average 69 tokens and 13 unique article numbers to predict.\nRather than asking whether a language model understands or not; we should ask to what extent, and in which way, a model understands.\nWe further extend the LAMA framework by al- lowing PLMs to predict multi-token targets. Take Drug for example the Trafficking\" offence under Drug-Related ' crimes of the US legislation: the Using the RoBERTa tokenizer; this term is split into and \"Trafficking' Drug' two tokens, that is We replace thus the \"drug traflicking\" phrase with two [MASK] tokens, and then ask the model to predict these tokens simultaneously.\nT0We decided to report only MRR results in the main paper for the sake of clarity. Moreover; MRR avoids penalizing for Detailed results including Precision near-identical outcomes (P@1) are available in Appendix C at\nA stricter evaluation would be to consider a multi-token 1[ prediction valid only if all the sub-tokens are properly pre- We decided to average the MRR to con- dicted by the PLM sider minor variations and errors_\nShttps: /hudoc echr coe.int/eng\n9In discussion on the LEGAL- Appendix A we present LAMA tasks' level of difficulty:\nContractual  Section  Titles (US) In this task; reusing the we have sections from US contracts dataset of Tuggener et al. (2020). Contractual sec- tions are usually numbered and titled, e.g;, \"10. [Arbitration]. Any controversy dispute or claim directly or indirectly arising out of or relating to this Agreement [_J\": The section titles reflect the content (subject matter) of the section, and are com section, where the section monly re-used. Given title is masked, the model has to predict the ass0- ciated title given the context. The dataset is com- posed of 1,527 test instances containing on average 85 tokens and 20 unique section titles to predict:\nWe extract those paragraphs from the US case law cOrpus, presented in Section 2. We select a sub- set of legal terms per legal topic (e.g , finance law property law; family law using the legal vocab- ularies provided by the Legal Information Insti- tute (LII) of the Cornell Law School.13 We filter paragraphs that use the specified legal terms out The [marital privilege] against self- For example, incrimination is [_] grounded upon the theory that just aS one may not be convicted by his own com- pelled testimony; so may he not be convicted by the testimony of his spouse. paragraph; where Given a legal term is masked, the model has to predict a the associated legal term given the context The task is composed of 5,829 test instances containing on average 308 tokens and 92 legal terms from topics to predict:\nContract Types (US) In this task; we have intro- ductory paragraphs from US contracts_ We extract those paragraphs from the newly introduced corpus of US contracts, presented in Section 2 Introduc - tory paragraphs usually start with the contract title revealing the contract type, e-g-, \"Service Agree and follow with the names of the involved ment parties, and their roles in this agreement: For exam This [Purchase] Agreement is entered into ple, day of January 2020 by and between A this 23rd (the \"Purchaser\") and B (the \"Seller\"). \" . Given an introductory paragraph, where the contract type is masked, the model has to predict the associated The task is composed of type given the context: containing 0n average 150 tO- 1,089 test instances kens and 15 unique types of contracts to predict:\nLegal Terminology (EU) In this task; we have paragraphs from CJEU judgments (opinions) We extract those paragraphs from the newly introduced EU case law corpus, presented in Section 2. We select a subset of legal terms based on the sub- ject matters provided by the database of the courts 14 (CURIA) We filter out paragraphs that use the specified legal terms. For example, The guiding principle at the basis of EU [data protection] law is that of a self-determined decision of an individ- ual who is capable of making choices about the use and processing of his or her data. Given a paragraph, where a legal term is masked, the model predict the associated legal term given the has to The task is composed of 2,127 test in- context. stances containing 0n average 164 tokens and 42 legal terms from 23 topics to predict:\nCrime Charges (US) In this task; we have para - graphs from US court judgments (opinions). We extract those paragraphs from the US case law cor- pus, presented in Section 2. We select list of criminal offenses (e.g,, \"Sexual Assault\"), catego rized into 11 major categories (e.g,, Sex-related) 12 from the FindLaw website. We filter out para- graphs that refer the specified criminal charges ver- batim. For example, \"A person commits the crime of [burglary] in the first degree when he or she en- building with the ters or remains unlawfully in a against _ intent to commit a crime a person or prop- paragraph, where a criminal erty therein Given a charge is masked, the model has to predict the asso- ciated criminal charge given the context: The task is composed of 4,518 test instances containing on average 118 tokens and 59 charges to predict:\nLegal Terminology (CoE) In this task; we have paragraphs from ECtHR decisions We extract those paragraphs from the newly introduced ECHR corpus presented in Section 2 We select a subset of legal terms (legal issues) based on the keywords 15 provided by the database of the courts (HUDOC) We filter out paragraphs that use the specified le- For example, \"The applicants alleged terms_ Igal that their relatives' [right to life] was violated in village guards: that they were deliberately killed by paragraph; where a legal term is masked, Given a the model has to predict the associated legal term The task is composed of 6,803 given the context:\nLegal Terminology (US) In this task; we have paragraphs from US court judgments (opinions).\nTShttps: 'WWw . law. cornell. eduf\nI4https Icuria.europa . eu/\nThttps 'WWw . findlaw com/criminal_ criminal-charges html\nIShttps: coe. int/Documents_ echr 'HUDOC | WWW Keywords ENG. pdf\nspecifications of the examined models We report the number of parameters, the size of vocabulary, Key Table 2: the number of accumulated training tokens, and the nature of pre-trainig corpora:\nPoL-BERT (Large) is a legal-oriented RoBERTa model released by Henderson* et al. (2022) trained from scratch for 2M steps on the PILE OF LAW, a corpus consisting of approx: 256GB of English, mainly US, language legal and administrative text.\ntest instances containing O average 97 tokens and 250 legal terms from 15 articles to predict:\nCriminal Code Sections (Canada). In this task; paragraphs from the Criminal Court of we have Canada'$ decisions containing Section Numbers of the Criminal Code of Canada (CCC)I6. For exam Section [680] of the Criminal Code provides ple, that a bail review is to be conducted by a panel of this court where directed by the Chief Justice. paragraph, where a criminal code's section Given a is masked, the model has to predict the associated section number; paragraph; and sub-paragraph (if The task is composed of any) given the context: containing on average 72 tokens 321 test instances and 144 different section numbers to predict:\nLexLM   (Base/Large) newly   released are our RoBERTa  models. a  series of best- We follow practices in language model development:\nWe warm-start (initialize) our models from Ia) the original RoBERTa checkpoints (base o large) of Liu et al. (2019) (b) We train a new tokenizer of SOk BPEs, but we embeddings for all lexically reuse the original overlapping tokens (Pfeiffer et al,, 2021). We continue pre-training our models on the diverse LEXFILES (Section 2) corpus for ad- ditional IM steps with batches of 512 sam ples, and a 20/309 masking rate (Wettig et al. 2023) , for base/large models, respectively exponential (d) We use a sentence sampler with smoothing of the sub-corpora sampling rate following Conneau et al. (2019) since there disparate proportion of tokens across sub- is a corpora (Table 1) and we aim to preserve per- corpus capacity (avoid overfitting). (e) We consider mixed cased models, similar to all recently developed large PLMs.\nIn Appendix D, we present the full list of vocabu- (masked terms) grouped in categories (clusters) lary when applicable- per LEGALLAMA sub-task:\nExperiments 4\nPre-trained Language Models 4.1\nlarge language models to assess their We consider performance with respect to the upstream (MLM), probing, and downstream evaluation:\n(Base/Large) original RoBERTa the are RoBERTa models (Liu et al,,2019) trained for 64k steps with very large batches on generic corpora; (knowledge). thus do not have any clear legal prior\nAdditional details on LexLM models pre-training can be found in Appendix B.\nLegalBERT legal-oriented (Base) BERT is model (Devlin et al,, 2019) released by Chalkidis et al. (2020) trained for IM steps on legal corpora from EU, UK, CoE, and USA.\nUpstream Evaluation 4.2\npresent the upstream (MLM) per- In Table 3, we formance for all PLMs across the LEXFILES sub- The performance is measured in terms corpora of accuracy; i.e. Precision @ ] of the masked to- predicted:  The accuracy is thus averaged ken to be over all the masked tokens for each task: We also provide the average across all tasks, per model. We observe that results vary across models trained in very different settings (model $ capacity, pre-\nCaseLawBERT (Base) is another legal-oriented BERT released by Zheng et al. (2021). CaseLaw BERT (which we will refer to as CL-BERT hence- forth) is trained from scratch for ZM steps on the Harvard Law case corpus, which comprises 3.4M legal decisions from US federal and state courts.\nI6https: I /laws-lois.justice.gc.ca/eng/acts C-46/index.html\nModelling Table 3: Upstream evaluation measured in terms of accuracy (Precision@ 1) on the Masked Language (MLM) task across all LEXFILES sub-corpora\ntraining corpora) , while the results also vary across legal sub-corpora:\nContracts) that were included in its training: We hypothesize that these results are related to the pre- training data diversity since LexLMs have been including many trained in a more diverse corpus more documents from different legal systems with sampling smoothing to preserve capacity per sub- corpus. The larger LexLM model has the capacity greater detail. to cover all sub-corpora to a\nWe want to remind the reader that the upstream evaluation offers a rough idea of a model's capabili- ties since it relies on random masked sub-words, in of those can be generic and thus many which case highly predictable (e.g: preposition \"of\") This phe- nomenon further motivates the construction of the LEGALLAMA benchmark, in which case only \"legal knowledge sensitive\" words have been masked.\nIn general, larger models pre-trained on the same corpora (RoBERTas, LexLMs) perform better com- pared to smaller ones, but in-domain pre-training is important factor for upstream a much more perfor-| mance, e.g,, LegalBERT outperforms RoBERTa-L\nType  of Documents: In of   differences terms sub-corpora, we observe that the perfor-, across legislation is better compared to mance on case law in 3/4 legal systems, where we have both (EU, UK, US, Canada) , with US contractual language being predictable for the models which the most have been trained on it (LexLMs, LegalBERT)\n43 Probing Evaluation\nwe present the results across all exam In Table 4 ined PLMs on LEGALLAMA. We analyze the results perspectives: the knowledge from two core prior and the probing task:\nComparison large of PLMs: Overall, the LexLM model outperforms the rest, being 39 more compared to the Znd best accurate on average mod- els (base versions of LexLM, and LegalBERT) Such results are expected since LexLMs have been Legal- diverse corpus, similarly trained in to BERT, compared to CL-BERT; PoL-BERT, and which have been trained US corpora. Over- on specialization harms the two US-centric models in outperformed even they great extend since are from the generic RoBERTa models.\nKnowledge: The pre-training corpus has Prior significant impact O the probing performance RoBERTa models, having little to legal prior; no expected to achieve worst performance on all were Surprisingly, CL-BERT and PoL probing tasks BERT achieve on-par or sometimes worst perfor- mance than RoBERTa (Base & Large) in most tasks. Being trained on the \"Harvard Law Case\" corpus (CL-BERT) and the PILE OF LAW (PoL-BERT), we would have expected better performance than model without legal Their pre-training cor- prior pora might be lacking diversity, which might cause their poor performance even on Legal-US probing\nWe also observe that LegalBERT outperforms the similarly-sized LexLM in specific sub-corpora (Both EU, UK legislation, ECtHR case law; and US\nstatistics regarding the average number of tokens in the input (#T), the number Table 4: The 8 LEGALLAMA tasks along of labels to predict from (#L), and the average number of tokens per label (#T/L) with the Mean Reciprocal Rank results of the 7 examined PLMs:\nLegalBERT (Base), being trained on UK, tasks. EU and USA data illustrates important improve- ment over models without legal prior (RoBERTa) having only US legal prior (CaseLaw and PoL or being BERT) LExLM models, trained on the new LEXFILES dataset, show performance improvement LegalBERT across all tasks, especially on the over task of predicting Section Numbers of the Crim inal Code of Canada: Regarding the size of the we are able to compare the cased versions model, of RoBERTa Base/Large and LexLM Base/Large. expected, the larger versions offer better As perfor- mance than the smaller ones 0n every task:\nWe further demonstrate the label complexity im pact in Figure 2. The output space does not seem performance, to have a correlation with the models since the selected Legal Terminology Topic Clus- ters (US) has only 7 possible labels, whereas the Criminal Code Section (Canada) has 144 possible being Finally, Crime Charges, labels. the hard- est task to solve, has on average 118 tokens as input and 59 possible labels with moderate com plexity, similar to the Terminology tasks (EU and CoE) This suggests that the difliculty of the task is not only driven by the labels' complexity but may rather lie in the lack of contextualization. Take for example the following sentence:\nThis case involves perhaps the first prosecu- tion under New York's new [computer crime] statute, Penal Law article 156, which went into effect on November 1, 1986, just days before the incidents charged herein:\"\nThe only contextual hint the PLMs have to predict the correct tokens ([computer crime]) is the utter- Penal Law article 156, which went into effect ance This is the opposite task of on November 1, 1986' predicting article numbers given a context, which is much more diflicult than predicting the actual context because the output space is larger 17\nFigure 2: Models performance on LEGALLAMA 's test set with respect to the label complexity: Labels with more than three tokens are much harder to predict:\nProbing Tasks: We characterize the difliculty of the tasks by their semantic level, the output space (the number of labels to predict from), and the label complexity (how many tokens per label) We ex - different characteristics in Table 4 pose the tasks Given the best-performing model (LexLM-L), we Charges and Legal Terminology can see that Crime (US and EU) are the hardest tasks to solve. Look- ing at Table 4, we can see that these three tasks are characterized by higher label complexity (>2).\nDownstream Evaluation 4.4\nFor downstream evaluation, experi- conduct we ments for 6 legal classification tasks, 5 part of Chalkidis et al,, 2022a), covering US LEXGLUE contracts, US, EU, and ECHR law.\nECtHR (Task B) (Chalkidis et al,, 2021b) is multi-label topic classification task; where given\nI7The actual tokens predicted by the best-performing exam- ined PLM were \"sexual and abuse\"\nTable 5: Test Results for all models across all downstream tasks after fine-tuning for a single epoch:\nthe facts of an ECtHR case, the model has to predict the alleged violated ECHR article among 10 such Prohibition of Torture' articles (e.g,, \"Art 3. \"Art Right to Fair Trial\"). 6\nTuggener et al,, 2020) is a single-label LEDGAR multi-class topic classification task, where given contractual paragraph, the model has to predict one of the correct topic among 100 topics (e.g: Limitation of Liability' Arbitration\nFigure Development 3: Results of RoBERTa and training epochs large on ECtHR across 5 LexLM\nKoreeda and Manning, 2021) is a ContractNLI contract-based Natural Language Inference (NLI) task, where given an Non-Disclosure Agreement (NDA) and one out 17 templated hypotheses (e.g: The Party may share some Confidential Informa- third-parties \"), the model has to tion with some predict if the hypothesis is (entailed, contradicted, or is neutral) t0 the terms of the NDA\nlearning rate of le-5 for a single with epoch leading to number of updates. We small are interested to examine how fast each model con- vergence based on its prior knowledge; in other words, what can a model learn in a single pass over training data? Finetuning models for many epochs large datasets will eventually lead to a full over re-parameterization of the models, in which case the importance of prior knowledge will diminish compromise the goal of our study (Figure 3).18\nSCOTUS Chalkidis et al,, 2022a) is a single-label multi-class topic classification task, where given Supreme Court of US (SCOTUS) opinion, the a model has to predict the relevant area among 14 \"Civil Rights' (e.g  issue areas 'Judicial Power\nFor all tasks; we use standard N-way classifiers with a classification head (Devlin et al,, 2019). For ECtHR , and SCOTUS, involving long documents, Longformer (Beltagy et al,, 2020) we warm-start models from each PLM's parameters to encode up We evaluate classification to 2048 tokens. perfor- mance with micro-Fl ((F1) and macro-FL (mFi following Chalkidis et al. (2022a). across tasks\nZheng et CaseHOLD multiple al., 2021) is choice QA classification task, where given a para- graph from a US legal opinion where a legal rule (holding) is masked, the model has to predict the applicable rule among 5 alternatives (the correct one and 2 irrelevant presented in other cases)\nwe present the test results Results In Table 5, across all tasks/datasets. We analyze the results perspectives: model's capacity (size) , and from two prior legal knowledge abducted via pre-training:\nEURLEX Chalkidis et al., 202la) is a multi-label topic classification task, where given an EU law, predict the correct EUROVOC the model has to concept among hundred concepts (e.g , 'Environ- Policy\" International Trade\" ) mental\n18In models fully converge after approx. most tasks, 5 epochs with improved performance, and the relative differ- ences between generic and legal-oriented models are dimin - Chalkidis et al,, 2022a) ished\nWe fine-tune all examined PLMs (Section 4.1)\nPLMs. We also released two new legal PLMs and evaluate them alongside others on LEGALLAMA and LEXGLUE. Based on our analysis (Section 4), following general observations: we make the\nModel s capacity (size) strongly correlates with the overall downstream performance Across all 2/6 exceptions (CNLI and Case- tasks, there are where  LegalBERT  outperforms HOLD) larger using sentence pairs, a setup PLMs. Both tasks are used in BERT's pre-training, but not in RoBERTa, which may bring LegalBERT; a BERT-based model, co-considering the min- in a better initial condition imal updates steps, compared to all large models following the RoBERTa pre-training setup, which of sentences or optimized based on do no use pairs sentence-level objective (NSP)\n(a) The use of diverse legal corpora leads to better overall upstream performance (Section 4.2). (b) We find that probing performance strongly correlates with upstream performance in re- lated legal topics (Section 4.3). For both upstream, and probing performance, the selection of pre-training corpora has a larger effect compared to model's ca- much pacity (Sections 4.2-4.3). Nonetheless, larger models pre-trained on similar corpora have better overall performance. performance is mainly driven by (d) Downstream legal knowl- the model's capacity and prior edge which can be estimated by upstream and probing performance (Section 4.4).\nLegal Knowledge also plays an important role fol- lowing the model s capacity (size). We observe that LexLM-B trained in the diverse LEXFILES COrpus outperforms the equally-sized RoBERTa-B model in 5/6 tasks, while LegalBERT and CL-BERT out- perform it oly in 3 out of 6 tasks In this case, acquaintance of legal the results are mixed, i.e., knowledge as expressed by upstream (Section 4.2) probing (Section 4.3) performance does not and correlate with downstream performance.\nplan to further analyze the In future work, we learning dynamics of legal language models by comparing their representations with representa- tions derived from legal knowledge bases Given the availability of the new resources, the develop- instruction-following (Wei et al,, 2021) ment of fine-tuned legal-oriented GPT-like (Ouyang et al. 2022) models is also an anticipated direction:\nIn the case of large-sized models, LexLM-L out- perform RoBERTa-L across all tasks, while PoL BERT trained on the US-biased PILE OF LAW cor- outperformed by RoBERTa-L in 5 out of 6 pus is Given the results with respect to upstream tasks. and probing performance, RoBERTa-L has a better legal prior; so in these regards, acquaintance of fully legal knowledge correlates with downstream performance in the large models' regime.\nLimitations\nDiversity of Corpora While the newly intro- duced LEXFILES COrpus is significantly more di- compared to the PILE OF LAW corpus of Hen- verse derson* et al. (2022), it is still an English-only covering only 6 legal systems (EU, UK, corpus Despite, the fact that CoE, US, India, Canada). we can train better models (LexLMs) and evaluate these models across these corpora, in future work, we should extend our analysis to cover even more languages and legal systems, and a higher granu- larity in the labeling of legal fields within these systems. Not only will this help support the inclu- sion of other legal traditions but also adding more linguistic and cultural diversity will help us better understand the robustness of existing methods.\nRelease of Resources 5\nWe release our code base to assure reproducibility and let others extend our study by experimenting develop new ones.19 with other PLMs, The or new LexLM models (Section 4.1), the LEXFILES corpus 20 (Section 2), and the LEGALLAMA bench- mark 21 (Section 4.3) are available o Hugging Face Hub (Lhoest et al,, 2021).22\nConclusions and Future Work 6\nIn this work; we introduced a multinational En- glish legal corpus (LEXFILES) and a legal knowl edge probing benchmark (LEGALLAMA) to facili- training and detailed analysis of legal-oriented tate\nSimilarly, the newly introduced LEGALLAMA benchmark consists of 8 sub-tasks targeting EU, US, and Canadian jurisdictions in a very ECHR, controlled setting; where examples were automat- ically extracted. While on this benchmark, legal- significant de- oriented PLMs has demonstrated a understanding\" of legal language and legal gree of\nI9https /github _ com/coastalcph/lexlms\nZOhttps /huggingface co/datasets_ lexlms_ lex files\nZhttps /huggingface.co/datasets/lexlms, legal_lama\nZ2https /huggingface co/lexlms\nreported herein to explore these important results topics.\ntopics, this benchmark should be further expanded acquaintance with more sub-tasks to evaluate the of legal knowledge across more legal systems and topics, and possibly cleansed from both very easy and unsolvable examples.\nEthics Statement\nThe scope of this work is to examine the perfor- mance of legal-oriented PLMs from a multi-facet perspective and broaden the discussion to help prac - assisting technology for legal titioners build profes- sionals and laypersons. We believe that this is an important application field, where research should be conducted (Tsarapatsanis and Aletras, 2021) to improve legal services and democratize law, while also highlighting (informing the audience on) the various multi-aspect shortcomings seeking re- sponsible and ethical (fair) deployment of legal- oriented technologies.\nModel  Considerations In this work we con sider encoder-only (BERT-like) models up to ap- 350M parameters, while recent work on the prox development of Large Language Models (LLMs) (Kaplan et al,, 2020; Brown et al,, 2020; Hoffmann et al,, 2022; Chowdhery et al,, 2022) is mainly tar- geting billion-parameter-sized models (10-1OOBs of parameters that usually follow a decoder-only, e.g,, GPT (Radford and Narasimhan, 2018), or encoder-decoder; e.g,, TS (Raffel et al,, 2020), ar Moreover; new paradigms of training chitecture. PLMs have been introduced, such as instruction- based finetuning (Wei et al,, 2021), and alignment via Reinforcement Learning from Human Feed- back (RLHF) Stiennon et al,, 2020; Ouyang et al. Latest GPT models (Ouyang et al,, 2022) 2022). have recently shown significant zero-shot progress on law-related tasks such as bar examination ques - tion answering (Katz et al,, 2023). Thus, future work should follow the most recent advances by larger auto-regressive GTP-like pre-training much models that seem to lead to emergent zero-shot and few-shot capabilities.\nIn this direction, we introduce new resources covering various legal systems to build new mod- els that better represent law and better assess their capabilities. All newly developed and published re- publicly available data, most sources are based on of them scattered on several web portals.\nAcknowledgments\nThis work funded the partly by In- was Fund https novation Denmark (IFD, [innovationsfonden.dk/en) and the Fonds de recherche du Quebec Nature  et  technolo- https: / / frq-gouv.qc.ca/ gies (FRQNT; nature-et-technologies;\nEvaluation   Considerations In Section 3_ we present how we account for and evaluate multi- token   expressions (terms the LEGALLAMA on benchmark; we are open to ideas on how we should possibly improve the current approach to provide a fairer and more robust evaluation framework across all models. Similarly, in Section 4.4, we fine-tune all examined PLMs for a single epoch to avoid ex over-reparameterization and better estimate treme how model's knowledge affects convergence and performance. Nonetheless, there are possibly bet- approaches to control for these aspects, e.g: ter Adapter-based (Riickle et al,, 2021) finetuning, O other approaches, such as LoRA (Hu et al,,2022).\nReferences \n Now the paper ends. \nQuestion: How could the authors improve the diversity of the corpora used in their work to ensure it is more representative of different legal systems and cultures?", "output": "Answer: The authors mention in the \"Limitations\" section that while the LEXFILES corpus they introduce is more diverse than previous corpora by covering 6 legal systems, it is still limited to only English documents and a small number of jurisdictions. To improve the diversity of their corpora, the authors could:\n\n1) Expand the corpus to include legal texts in multiple languages, not just English. This would help capture the nuances of different legal systems that use other languages. \n\n2) Expand the number of legal systems and jurisdictions covered beyond the current 6. They could include corpora from countries across different continents and regions to gain a truly global perspective.\n\n3) Introduce a finer-grained labeling system for legal fields within and across jurisdictions. Currently the corpora seem to be labeled at a high level (e.g. by legal system). More detailed labels could capture the nuances between different legal topics within a system.\n\n4) Increase the cultural and linguistic diversity by including non-Western legal systems and underrepresented minority languages. This would improve the robustness and fairness of the models trained on the corpora.\n\nIn summary, by expanding the corpus in these directions to cover more languages, jurisdictions, legal topics and underrepresented cultures, the authors could significantly improve the diversity and representativeness of the corpora to achieve a truly global perspective on legal language. This would also likely improve the performance and robustness of the legal-oriented models trained on the corpora."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n Abstract\n(Qi et al,, 2021) and machine translation Michel et al,,2019)).\nNatural Language Processing (NLP) models based on Machine Learning (ML) are suscep algo tible to adversarial attacks malicious rithms that imperceptibly modify input text to making incorrect predictions. force models into However; evaluations of these attacks ignore the property of imperceptibility or study it un- der limited settings. This entails that adver- sarial perturbations would not pass any human quality and do not represent real threats gate To bypass to human-checked NLP systems_ this limitation and enable proper assessment (and later; improvement) of NLP model robust- ness, we have surveyed 378 human participants about the perceptibility of text adversarial ex amples produced by state-of-the-art methods_ Our results underline that existing text attacks impractical in real-world scenarios where are humans are involved This contrasts with pre- vious smaller-scale human studies, which re- ported overly optimistic conclusions regarding hope Through our work; attack success_ we position human perceptibility as a first-class to success criterion for text attacks, and provide guidance for research to build effective attack algorithms and, in turn, design appropriate de- fence mechanisms_\nIt is critical to properly assess model robustness against adversarial attacks to design relevant de- fence mechanisms. This is why research has inves- tigated different attack algorithms based on para- phrasing (Iyyer et al,, 2018), character-level (Gao et al,, 2018; Pruthi et al,, 2019) and word-level (Garg and Ramakrishnan, 2020; Ren et al,, 2019) perturbations, and made these algorithms available in standardized libraries (Morris et al,, 2020b; Zeng et al., 2021).\nFor the many NLP systems that interact with hu- mans, we argue that effective adversarial attacks should produce text that is both valid and natural_ Validity refers to the property that humans perceive properties of interest\" for an ad- the same semantic produced versarial text as for the original text it was from. Naturalness refers to the perception that an adversarial text was produced by humans. Adver- sarial texts that are invalid and/or unnatural can still cause failed NLP model decisions, however; their ultimate effect on humans is negligible because would fail to convey the intended meaning they hate speech that is not perceived as hateful) O1 Ke.g would be suspected to be computer-generated they phishing email using awkward vocabulary ke.g , a and grammar).\nIntroduction\nLike many other machine learning models, Natural Language Processing (NLP) models are susceptible to adversarial attacks. In NLP these attacks aim to cause failures (e.g:  incorrect decisions) in the model by slightly perturbing the input text in such a way that its original meaning is preserved.\nUnfortunately, the scientific literature on adver- sarial text attacks has neglected (and sometimes ignored) the inclusion of human perception as an essential evaluation criterion see Table 1. We found that (i) 3 studies do not include humans at all in their evaluation; (ii) merely 12 studies consider they naturalness, and only do so under limited set- tings. Indeed, these studies involve a single attack, one O two naturalness criteria, less than 10 partici- pants, and they disregard the impact of parameters and factors like perturbation size and language pro-\nResearch has reported on the potential of adver- sarial attacks to affect real-world models interact- ing with human users, such as Google's Perspec- tive and Facebook's fastText (Li et al,, 2019)) More learning tasks generally, these attacks cover various including classification and seq2seq (fake news (Li 2020), toxic content (Li et al,, 2019), spam et al: Kuchipudi et al,,2020)) , style transfer messages\nIn the case of classification tasks; these semantics proper- ties boil down t0 the class labels_\nficiency: Instead, the studies rely o automated metrics (i.e cosine distance to measure semantic similarity), but these are not suitable proxies for human perception (Morris et al,, 2020a).\nThe absence of systematic analysis of adversar- leading to ial texts aS perceived by humans risks overestimation of their semantic quality and, in to fallacious model robustness turn, assessment and misguidance during the design of defences This was hinted in the seminal work from Morris et al. (2020a), where a 10-participant survey on discrepancy one dataset and two attacks revealed a between the human-perceived naturalness of adver- sarial examples.\nAdversarial examples Figure 1: NLP model against with perturbations in red. a) Invalid adversarial exam- ple generated by (Morris et al,, 2020a). b) Unnatural adversarial example generated by Ali et al. (2021).\nTherefore, in this paper; we present the first ex tensive study that evaluates the human-perceived validity and naturalness of adversarial texts. We surveyed 378 participants in assessing, based on five criteria, over 3000 texts (original and adversar- ial) coming from three datasets and produced by nine state-of-the-art attacks.\nhuman eyes and propagate.\nlike hate speech, spam, Fortunately, fake news phishing, and many other malicious text contents ultimately targets human eyes and has not only to bypass automated quality (such as detec- gates but also fool human understanding and judg- tors of propagat- Indeed, to achieve their ment: goal ing erroneous information, adversarial fake news should still relay wrong information they should and be perceived as a text be *valid \" fake news they seemingly written by humans should be i.e. example from Figure 1 is 'natural\" The fake news unnatural because it uses irrelevant proper nouns like \"Slut Tower' Donald Hobo\" that do not or exist in reality and this makes the fake news inef- fective. We, therefore, argue that invalid andlor un- natural examples do not constitute relevant threats.\nOur investigations first reveal that the partici- pants would classify 28.149 of adversarial exam ples into a different class than the original exam ple. This means that the adversarial perturbations change human understanding of the modified text and, thus, fail to achieve their purpose. Irrespec- tive of the classification task, participants detect 60.39 of adversarial examples as computer-altered; they identify 52.389 of the exact altered can even These findings contrast the overly optimistic word_ conclusions regarding attack success rates from previous small-scale human studies. Our results underline that existing attacks are not effective in real-world scenarios where humans interact with NLP systems_ hope to pO Through our work; we sition human perception as first-class success criterion for text attacks, and provide guidance for research to build effective attack algorithms and, in turn, design appropriate defence mechanisms.\nThus, the goal of adversarial text attacks be- produce examples that change model de- comes to perceived by humans as valid and cision and are Our study aims to assess, using human natural. evaluators, whether state-of-the-art text adversarial The answer to this question attacks meet this goal. remains unknown today because, as revealed by existing attacks (see Table 1) only our survey of six papers cover both validity and naturalness, five of them do so with less than 10 human participants, and Textbugger (Li et al, 2019) that has the largest number of participants assesses naturalness only Nevertheless; at   word level, not sentence level. all these papers evaluate the effectiveness of the attack they introduce rarely with another specific baseline) and there is a lack of standardized studies considering them all.\nMotivation 2\nConsider the example of fake news shown in Fig- Ali et al. (2021) have shown ure lb (\"Original\" ). that this example is detected by existing fake news detectors based on NLP machine learning models. However; the same authors have also revealed that; changes specific words to produce if one a new sen Adversarial\" ) , the same detector would fail tence recognize the modified sentence as fake news_ to This means that fake news could ultimately reach\nthe   validity  and study, For naturalness our requirements led to consider word-based at- uS Indeed, character-based attacks are easily tacks.\nHuman evaluation performed on quality of adversarial examples by existing literature Table 1: The terms abbreviated are Suspiciousness(S.) , Detectability(D.) , Grammaticality(G.) , Meaning(M.). N/A indicates information is not available.\nalso confronts participants to original examples they and checks if correctly classify these exam ples (Section 5.1). A statistical difference between accuracy on adversarial examples com humans pared to original examples would indicate that a significant portion of adversarial examples is in- valid.\ndetectable by humans and are even reversible with spelling and grammar check methods (Sakaguchi et al,, 2017). In word-based attacks, the size of the perturbation $ is typically defined as the number of modified words\nResearch questions and metrics 3\nIn addition to validity, we study next the degree to which adversarial texts are natural.\nResearch questions 3.1\nOur study firstly investigates the validity of adver- sarial examples as perceived by humans.\nability To answer this question, we measure the of humans to suspect that a piece of text has been computer altered (with adversarial perturbations) An adversarial example is thus evaluated as less natural, the more it raises suspicion (to have been altered) among the participants.\nValidity is the ability of the adversarial example to preserve the class label given to the original text (Chen et al,, 2022). Figure la) illustrates a case of example, which changes the an invalid adversarial positive sentiment of the original example. Thus, we aim to compare the label that human partici- pants would give to an adversarial example with the label of the original example. To determine the ground original label, we use as a reference the truth\" label indicated in the original datasets used in experiments that is, we assume that this orig our inal label is the most likely to be given by human To validate this assumption; our study evaluators\nThe suspicion that a text seems computer-altered might arise from different sources, for example specific words, typos, lack of seman- the use of tic coherence etc. Thus, in addition to evaluating suspiciousness we refine our analysis in order to unveil some reasons why humans may found an suspicious We investigate adversarial text to be three additional naturalness criteria:\nDetectability is the degree to which humans\nReported metrics 3.2\nrecognize which words of a given adver- can detectability sarial sentence we altered. High would indicate that the choice of words signifi- cantly affect the naturalness of these examples detectability in (or lack thereof) We assess settings:   wherein humans do not know two how many words have been altered (unknown they Iol)) and wherein know the exact number of altered words (known Ifl). Grammaticality is the degree to which an adversarial text respects the rules of gram- mar: The presence of grammar errors in a text might raise the suspicion of human evaluators_ However; grammar errors may also occur in original (human-written) text  Therefore, we study both the total number of grammar errors in adversarial examples (\"error presence' and the number of introduced errors compared to original texts \"error introduction\" ) The latter is a better evaluator for the quality of generated adversarial text. A high relative amount of grammar errors could explain the suspiciousness of the adversarial examples (or lack thereof) Meaningfulness is the degree to which the ad- versarial text clearly communicates a message that is understandable by the reader: We assess meaningfulness of adversarial text first in the isolation (\"clarity\")) , and then check whether humans believe the meaning of the original text has been preserved under the adversarial perturbation \"preservation\" ). We hypothe- size that adversarial texts with significantly altered meanings are more suspicious\nThroughout our study, we compute different met- rics for each attack separately and all attacks alto- gether:\nValidity: the percentage of human-assigned la- bels to adversarial text that match the ground truth provided with the datasets.\nSuspiciousness: the percentage of adversarial recognized as \"computer altered\" texts\nDetectability: of  perturbed the   percentage words in an adversarial text that are detected as modified.\nGrammaticality: the percentage of adversarial texts where human evaluators detected present er- rors (errors introduced by the attack), did not detect or were not sure.\nMeaningfulness: the average value of clarity of meaning and meaning preservation, as measured options on a 1-4 Likert scale (the Likert scale are given in Figure 2).\n33 Statistical tests\nsignificance of differences To assess the we ob- serve, we rely on different statistical tests chosen based on the concerned metrics.\nProportion tests are used for validity and sus- they picion, because are measured as propor- tions_ Mann Whitney U tests are used for detectabil- ity, grammaticality and meaningfulness be- cause their data are ordinal and may not fol- low normal distribution (which this test does not assume). We compute the standardized Z value because our data samples are larger than 30, and the test statistic U is roughly normally distributed: Pearson correlation tests are used t0 assess the existence of linear correlations between the perturbation size and validitylnaturalness.\nFinally, because the perturbation size is known to impact success rate and human perceptibility of ad versarial attacks in other domains (Simonetto et al. 2021; Dyrmishi et al,, 2022), we investigate the relationship between the number of altered words and validitylnaturalness.\nWe perform all these tests with a significance level of & 0.01.\nStudy design 4\nRQ3: How does perturbation size impact the valid- ity and naturalness of adversarial examples?\n4.1 Adversarial texts\nTo generate the adversarial texts presented to par- ticipants, we used the TextAttack library (Morris et al,, 2020b), which is regularly kept up to date with state-of-the-art attacks, including word-based ones\nAlthough there is a general acceptance that lower perturbation sizes are preferred, the actual magni- tude of the effect that perturbation size causes on perception has not been studied before. text\n4.1.1 Attacks\nused nine word-based attacks from In total, we the   library: Three   of them( BERTAttack (Li et al,, 2020), BAE(Garg and Ramakrishnan, 2020), CLARE(Li et al,, 2021)) belong to the family of attacks that uses masked language models to in- troduce perturbations to the original text: Three others (FGA(Jia et al,, 2019), IGA(Wang et al. 2019), PSO(Zang et al,, 2020)) evolution- use ary   algorithms the   original evolve to text to- The remaining three wards an adversarial one (Kuleshov(Kuleshov et al,,2018) , PWWS(Ren et al. 2019), TextFooler(Jin et al,, 2020)) use greedy For all the attacks, we used the search strategies: default parameters provided by the original authors We excluded only Hotflip attack because it was not compatible with the latest Bert-based models and Alzantot attack, for which we used its improved and faster version FGA. You can refer to Table 1 for details related to the human study performed by the original authors.\nFigure 2: The online questionnaire structure.\n4.2 Datasets\nnon-technical audiences and avoid biases. We do provide any hints to participants about the word not replacement strategy (i.e. synonym replacement). In addition to this explanation, we clarify to the participants the intended use of the data collected from this study:\nWe attacked  models trained on three sentiment IMDB analysis datasets: movie reviews (Maas et al,, 2011), Rotten Tomatoes movie reviews (Pang and Lee, 2005) and Yelp polarity service reviews (Zhang et al,, 2015). We reuse the already avail- able DistilBERT models in the TextAttack library that are trained on these three datasets. Sentiment validity and analysis is a relevant task to assess naturalness, and is easily understandable by any participant, even without domain knowledge. We limited the study to only one task to avoid the extra burden of switching between tasks for the partici- pants We include this choice in the section Lim- itations as a study with diverse tasks and datasets interesting (i.e datasets with more formal would be language)\nThe first part of the questionnaire shows exam ples in isolation and without extra information. It contains questions about validity, suspiciousness, detectability (unlimited choices), grammaticality (presence of grammar errors), and meaningfulness (clarity). We display only one text at a time, and each participant receives five random adversarial texts shuffled with five random original texts We exclude the five original texts used as the initial for the adversarial generation process, to en- ppoint participants do not look at two versions of sure that the same text. Question number 5 on detectability will appear only if the participant answers \"com- puter altered\" to question 4.\nOn each dataset; we ran the selected nine word- level attacks, which resulted in 25 283 successful adversarial examples in total:\nThe second part focuses on detectability (exact number). Adversarial examples and their exact number n of perturbed words are shown, and par- they ticipants have to choose the n words believe have been altered. Each participant evaluates four they adversarial examples did not see in the first questionnaire part:\nQuestionnaire 4.3\nusing questionnaire We collected the data an online with three parts, presented in Figure 2 The begin- ning of the questionnaire contains the description of computer-altered text as a text altered automat- ically by a program by replacing some words with others We do not use the term \" adversarial ex to make the questionnaire accessible to amples'\nThe third part shows original and adversarial examples together It contains questions about\ngrammaticality (errors introduction) and meaning (preservation) . Each participant sees the same four adversarial examples (s)he had in the second part and their corresponding original examples.\nwe have (randomly) se- For each participant; lected the displayed adversarial examples in order to ensure a balance between the different attacks and perturbation sizes Each participant sees nine adversarial examples in total (one per attack) with different  perturbation sizes (chosen  uniformly) More details about this distribution are presented in Appendix A.l.\nTable 2: Percentage of correctly labelled adversarial positive or negative sentiment according to the texts as attack method.\nParticipants 4.4\nquestionnaire. In total, 378 adults answered our advertising Among them, 178 were recruited by private and communication channels (i.e. public on LinkedIn, university networks) The rest were re- cruited through the Prolific crowdsourcing plat- form. Prolific participants had 80% minimum ap- E3 per questionnaire, proval rate and paid were with an average reward of f9.89/h. All valid Pro- lific submissions passed two attention checks. For representation of the population, we a real-world advertised the study to targeted English language proficiency levels. As a result, 59 participants had working proficiency; 183 had professional limited proficiency; and 136 were native/bilingual.\nInsight I: Five out of nine adversarial attacks significant portion (>259) of adversar generate ial examples that humans would interpret with the wrong These examples would not achieve label: their intended in human-checked NLP sys- goal tems\nRQ2: Naturalness 5.2\nWe report below our results for the different natu- ralness criteria. The detailed results, globally and for each attack, are shown in Table 3.\nYou can find the complete dataset with the gen- erated adversarial sentences and the answers from the questionnaire in this link? .\nSuspiciousness 5.2.1\nHumans perceive 60.33% of adversarial examples This is significantly being computer altered. as more than the 21.43% of the original examples that raised suspicion (right-tailed proportion test 9.53e-124 ) 23.63, p This latter per- of Z = centage indicates the level of suspiciousness that attacks should target to be considered natural. A per-attack analysis (see Table 3) reveals that all attacks produce a significant number of examples perceived unnatural, from 46.55% (FGA) to 68.5% (PSO):\nResults and Analysis 5\nRQI: Validity 5.1\nTo 71.86% of all adversarial examples, participants according have associated the correct class label (= to the dataset ground truth). This contrasts with original examples, which human participants label correctly with 88.78%. This difference is statis- tically significant (left-tailed proportion test with Z 12.79, p 9.92e 38). = =\nInsight 2: Humans suspect that the majority of the examples (60.33%) produced by adversarial text attacks have been altered by a computer: This demonstrates a lack of naturalness in these exam- ples:\nTable 2 shows the detailed human accuracy num bers for each attack separately: Five of the nine attacks exhibit a statistical difference to original ex amples (the four others have over 80% of correctly labelled adversarial examples, without significant difference with the original examples) Humans have (almost) the same accuracy as random for two of these attacks, ranging between 50 and 60%.\nDetectability 5.2.2\nperturbation aware of the When humans are not they can detect only 45.28% of the altered size, words in examples they found to be computer al- This percentage increases t0 52.38%, when tered.\nhttps: Ifigshare. com/articles/dataset/ACL_ 2023_Human_Study_Adversarial_Text 7z/23035472\nTable 3: Human evaluation results about the naturalness of adversarial text. Downwards arrowsk indicate lower preferred: Upward arrows indicate higher values are preferred. Suspicion, Detectability and Grammati- values are cality values are percentages, while Meaning values are average of Likert scale items from 1-4.\nchanges drastically when they also see the tacks corresponding original examples (e.g. BERTAttack has the highest error rate with 55.4%, and CLARE has the lowest with 16.49), indicating that again this criterion is not relevant to explain naturalness\nthe actual perturbation size is known (with statis- tical significant according to a Mann-Whitney U 4.4e-8)_ Test with Z -73.49,p These con = clusions remain valid for all attacks taken individ- ually, with a detection rate ranging from 30.3% to 53.29 (8 unknown) and from 39.4% to 65.9% (0 known)\nPlease note that the grammar error presence and settings introduction are studied in two different (ref. section 3.1 and 4.3 with different sets of texts hence can not be compared against each other: We only comment on the results separately: can\nInsight 3: detect   almost   half Humans can (45.28%) of the perturbed words in adversarial text This indicates that the perturbations introduced by imperceptible: attacks are not\nInsight 4: Humans perceive grammar errors in 40% of adversarial examples. However; there is no positive correlation between perceived grammati- cality and naturalness_\nGrammaticality 5.23\nHumans perceive grammar errors in 38.9% of ad- versarial texts and claim that 40.6% of adversarial not present in their original texts contain errors Surprisingly, however; humans are counterparts_ likely to report grammar errors in examples more they they perceive as original, than in those deem computer-altered (73.09 versus 44.6%)(4. There positive correlation between grammati- is thus no cality and naturalness.\nPercentage of adversarial text labelled as Table 4: computer-altered according to grammar errors\nMeaning 5.2.4\nOne possible explanation is that human percep- tion of grammar mistakes significantly differs from automated grammar checks. Indeed, the Language Tool grammar checker (Naber et al,, 2003) reports that only 17.7% adversarial examples contain er- IOrS, which is significantly less than the 40.6% that humans reported. This teaches us that automated grammar checks cannot substitute for human stud- ies to assess grammaticality:\nHumans give an average rating of 2.60 (on a 1-4 meaning clarity of adversarial Likert scale) to the This is less than original texts, which re- texts. rating ceives an average of 3.44 with statistical significance based on Mann Whitney U test, with -142). Furthermore, par -412.10,p = 1.43e Z ticipants have mixed opinions regarding meaning preservation from original texts to adversarial texts rating of 2.11) on a 1-4 scale. (average\nHumans report varying rates of grammar errors across different attacks The rates are highest for CLARE (53.8% , which is significantly more than the lowest rate (BERTAttack, 23.7%) . Human per- ception of the grammaticality of the different at-\nTo check whether lack of clarity indicates a lack of perceived naturalness, we show in Table 5, for rating; the percentage of adversarial texts with each rating that humans perceived as computer al- this\nInsight 6: The perturbation size negatively af- fects suspiciousness and meaning, and has no im- pact on validity or detectability:\nWe observe a decreasing monotonic relation tered: between rating and suspiciousness This indicates that the more an adversarial text lacks clarity, the likely to consider it unnatural: more humans are\nMisc. results 6\nWe conducted an analysis to check whether human perception of naturalness and validity is related to their language proficiency We found out that language proficiency only affects some aspects of validity results_ People with naturalness and not professional proficiency are more suspicious, they achieve a higher accuracy at detecting adversarial groups(64.6% text compared to the other two VS 54.89 and 57.0%). Regarding grammaticality, peo- ple with higher proficiency level report more added errors to the original examples by adversarial at- tacks. Lastly, for the meaning preservation there is a statistical difference only between two profi- ciencies, natives give a lower score compared to limited working proficiency. For detailed results refer to Table & in Appendix\nTable 5: Percentage of adversarial texts labelled as computer-altered according to clarity of meaning score\nhave All attacks an average clarity score ranging from 2.26 (PWWS) to 3.06 (FGA), which tends to confirm the link between naturalness and meaning Meaning preservation ranges  from clarity: 1.7 Interestingly, the attacks with higher to 2.67. preservation rating (FGA, IGA, TextFooler) tends validity score (reported in Table2), to have a higher exception. though Kuleshov is an\nInsight 5: Humans find adversarial text less clear than original texts, while clarity is an impor- tant factor for perceived naturalness. Moreover; attacks that preserve the original meaning tend to produce more valid examples.\nDiscussion and conclusion\nOur study unveils that a significant portion of ad- versarial examples produced by state-of-the-art text attacks would not pass human quality These gates. examples are either invalid (labelled differently or unnatural (perceived from intended) as com puter altered). This means that the practical success rate of these attacks in systems interacting with humans would be lower than reported in purely model-focused evaluations_\nRQ3: How does perturbation size impact 5.3 the validity and naturalness of adversarial examples?\nPearson correlation tests have revealed that pertur- validity and detectabil- bation size does not affect ity, but correlates with suspiciousness, grammat- icality and meaning clarity. Figure 3 shows the graphs where correlation was established (the others are in Appendix A.2). Thus, adversarial ex amples are perceived as less natural as more word have been altered (positive correlation). On the contrary, fewer grammatical errors are reported by We performed an humans for higher perturbations. automated check with Language Tool, which gave opposite results, more grammatical errors are the present for larger perturbations. This demon- again strates the mismatch between human perception Or knowledge of grammar errors and a predefined set of rules from automatic checkers However; as reminder; error presence is not the most relevant metric when evaluating adversarial text: Error in- troduction should be considered more important: perturba- larger Finally, adversarial examples with tion size have less clear meaning and preserve less meaning: original text'$\nThrough our investigations, we discovered that validity is related to the meaning preservation of the original text by adversarial perturbations. As for naturalness, it appears that the detectability of (at meaning clarity least one) altered words, as well as are strong factors determining the suspiciousness computer-altered. The (per- of a text to have been ceived) presence of grammar errors is not a relevant criterion to determine naturalness. However; gram- maticality may still make sense in contexts where exchanged texts rarely contain grammar mistakes (e.g. in professional or formal environments)\nMore generally, the relevant criteria to evaluate the quality of adversarial examples depend on the Our goal, considered use case and threat model: therefore, is not to qualify an existing attack as worse than claimed , but rather to raise awareness that different threat scenarios may require different We, therefore, encourage re- evaluation criteria.\nFigure 3: Effect of perturbation size\n3000 sentences, they all target the sentiment analysis classification task. Muennighoff et al (2022) have recently released large-scale benchmark that covers dozens of text-related tasks and datasets that can further validate our It would be especially interesting to study: consider datasets that use more formal lan - guage (i.e. journalistic). we consider in this study have a The texts maximum length of 50 While this words_ allows the evaluation of a higher number of texts, the human perception of perturbations in longer texts might differ: We considered a uniform distribution of gen- erated adversarial texts per bin for each attack However; their real distribution in the wild might differ from our assumed one. speakers revolve around the All our texts and English language, while the problems that text adversarial attacks raise (such as fake news and misinformation) are global. Languages where grammar is more fluid, that allow more freedom in the positioning of the words or where subtle changes in tone significantly im pact the semantics can open vulnerabilities and hence require further studies.\nsearchers in adversarial attacks to precisely specify which systems and assumptions their study targets, and to justify the choice of evaluation criteria ac- cordingly\nprevious studies In particular, we corroborate that discourage the use of automated checks to re- place human validation (Morris et al,, 2020a) . Our study has revealed that human perception of gram maticality does not match the results of grammar checking tools. We thus argue that humans play an essential role in the evaluation of adversarial text attacks unless these attacks target specific systems that do not involve or impact humans at all.\nInterestingly, none of the existing attacks domi- A careful observation of Tables nate on all criteria. 2 and 3 reveals that six attacks (over nine) lie on the Pareto front (considering Our evaluation criteria This implies that different attacks objectives). as fit better in different threat models.\nUltimately, we believe that our results shape rel- evant directions for future research designing on These directions include further adversarial text. understanding the human factors that impact the (im)perceptibility of adversarial examples, and the elaboration of new attacks optimizing these factors (in addition to model failure). The design of rele- vant attacks constitutes a critical step towards safer NLP models, because understanding systems' secu- rity threats paves the way for building appropriate defence mechanisms.\nEthical considerations\nThis study investigates perception of humans o adversarial examples, which are modified texts that change the decision of a NLP model. aim to While these examples can be used by malicious actors, our bring they goal is to understand the threat and take informed decisions on preparing effective defences against these threats.\nLimitations\nOur study focuses word replacement at- on tacks_ While these attacks are the most com- perception of mon in the literature, the human attacks that rely on insertion or deletion can differ from our conclusions. While we evaluated three datasets and over\nThe texts shown to participants f this study were collected from open platforms, and it may\ncontain inappropriate language. To mitigate this we asked only participants 18+ years old to issue, take the survey:\nGarg Siddhant and Goutham Ramakrishnan: 2020. BAE: BERT-based adversarial examples for text clas - Proceedings of the 2020 Conference on sification. In Empirical Methods in Natural Language Processing EMNLP) , pages 6174-6181, Online. Association for Computational Linguistics Iyyer; John Wieting, Kevin Gimpel, and Luke Mohit Zettlemoyer: 2018. Adversarial example generation with syntactically controlled paraphrase networks arXiv preprint arKiv: 1804.06059. Robin Jia, Aditi Raghunathan, Kerem Goksel, and Percy Liang; 2019. Certified robustness to adversarial word In Proceedings of the 2019 Confer- substitutions_ Empirical Methods in Natural Language Pro- ence on cessing and the 9th International Joint Conference on Natural Language Processing EMNLP-IJCNLP) pages 4129-4142, Hong Kong; China. Association for Computational Linguistics. Tianyi Zhou, Zhijing   Jin, Joey Di  Jin; and Peter Szolovits. 2020. Is bert really robust? strong base - line for natural language attack on text classification Proceedings of the AAAI con- and entailment In intelligence, volume 34, pages ference on artificial 8018-8025. Bhargav Teja   Nannapaneni, Kuchipudi, Ravi and Adversarial machine learning for Qi Liao. 2020. Proceedings of the ISth International spam filters_ In Availability, Reliability and Security, Conference on pages 1-6. Volodymyr Kuleshov, Shantanu Thakoor; Tingfung Lau and Stefano Ermon. 2018 Adversarial examples for natural language classification problems Dianqi Li, Yizhe Zhang; Hao Peng; Liqun Chen, Chris Ming- Ting Sun, and Bill Dolan. 2021. Con- Brockett; textualized perturbation for textual adversarial attack_ Proceedings of the 2021 Conference of the North In American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 5053-5069, Online. Association for Computa- tional Linguistics. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting and Wang: 2019. Textbugger: Generating adversarial text real-world applications. In NETWORK AND against DISTRIBUTED SYSTEM SECURITY SYMPOSIUM Linyang Li, Ruotian Ma, Qipeng Guo,Xiangyang Xue, and Xipeng Qiu. 2020. BERT-ATTACK: Adversar- ial attack against BERT using BERT: In Proceed of the 2020 Conference on Empirical Methods ings in Natural Language Processing EMNLP ) , pages 6193-6202, Online. Association for Computational Linguistics. Andrew L. Maas, Raymond E. Daly, Peter T: Pham, Dan Huang, Andrew Y Ng, and Christopher Potts 2011. Learning word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the In Ssociation for Comnutational Linouictics_ Human\nAcknowledgements\nSalijona Dyrmishi's work is supported by the Lux embourg National Research Funds (FNR) AFR Grant 14585105.\nReferences \n Now the paper ends. \nShould invalid and/or unnatural adversarial text examples be considered real threats?", "output": "My answer would be no. As the paper argues:\n\n\"Invalid andlor unnatural examples do not constitute relevant threats. Adversarial examples that are invalid and/or unnatural can still cause failed NLP model decisions, however; their ultimate effect on humans is negligible because they would fail to convey the intended meaning (e.g. hate speech that is not perceived as hate speech or   would be suspected  to be computer-generated like   an awkward phishing email using vocabulary and grammar )\"\n\nThe paper argues that for adversarial text examples to pose a real threat, they need to be valid (preserve the original class label and meaning) and natural (not perceived as computer-altered) in order to actually reach humans and deceive them. Invalid and unnatural examples would likely be detected by humans and therefore not achieve their intended effect.\n\nIn summary, based on the arguments in the paper, I would say no - invalid and/or unnatural adversarial text examples should not be considered real threats as humans would likely detect them. They only pose a theoretical threat to NLP models, not a practical threat in scenarios where humans are involved."}, {"instruction": "Below is a paper. Memorize the paper and answer my question after the paper.\n The paper begins. \n ABSTRACT\nusing DETR is the first end-to-end object detector a transformer encoder-decoder competitive performance but low computational architecture and demonstrates efficiency o high resolution feature maps_ The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with de- formable attention, which achieves 10x faster convergence and improved perfor- mance. Deformable DETR uses the multiscale feature to ameliorate performance, however; the number of encoder tokens increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck: In our prelim- experiment, we observe that the detection performance hardly deteriorates inary part of the encoder token is updated  Inspired by this observation, even if only DETR that selectively updates only the tokens expected to we propose Sparse be referenced by the decoder; thus help the model effectively detect objects. In ad- dition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead DETR achieves better performance than Deformable We validate that Sparse DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 389 and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at httns kakaobrain aithub sparse-detr com\nINTRODUCTION\nIn recent years, we have witnessed the dramatic advancement and the success of object detection in deep learning: Diverse object detection methods have been proposed, but the existing algorithms that perform positive matching with the ground truth as require non-maximum suppres- a heuristic way post-processing of near-duplicate predictions sion (NMS Recently, Carion et al: 2020) has intro fully end-to-end detector DETR by eliminating the need for NMS post-processing through duced a set-based objective. The training objective is designed by employing the Hungarian algorithm that considers both classification and regression costs, and achieves highly competitive performance However; DETR is unable to use multi-scale features such as feature pyramid networks (Lin et al. which are commonly used in object detection to improve the detection of small objects 2017 The main reason is increased memory usage and computation by adding Transformer Vaswani et al ability to detect small objects is relatively pOOI: 2017 architecture. As a result; its\ndeformable-attention inspired by the To address this problem, [Zhu has proposed 2021 et al: and reduced the quadratic complexity to linear complex - deformable convolution Dai et al.12017 ity through key sparsification in the attention module: By using deformable attention, deformable DETR addresses the slow convergence and high complexity issue of DETR, which enables the en- input and significantly improves performance 0n detecting coder to use multi-scale features as an small objects. However; using the multi-scale features as an encoder input increases the number of tokens to be processed by about 20 times_ Eventually, despite efficient computation for the same\nduring Corresponding author: an internship at KakaoBrain_ fWork is done Equal contribution.\ntoken length, the overall complexity increases back again, making the model inference slower even than vanilla DETR\nIn general, natural images often contain large background regions irrelevant to the objects of interest, and accordingly, in end-to-end detectors, the tokens corresponding to the background also occupy significant portion In addition, the importance of each regional feature is not identical, which has been proven by the two-stage detectors successfully do their job by focusing only on the foreground It suggests that there exists considerable regional redundancy that can be reduced in the detection focusing seeking to devise an efficient detector tasks and on the salient regions is necessary and we observe the following: (a) during inference preliminary experiments, natural direction. In our of a fully-converged Deformable DETR model on the COCO validation dataset; the encoder tokens retraining referenced by the decoder account for only about 45% of the total, and (b) a new detector updating only the encoder tokens preferred by the decoder from another fully-trained detector; while barely suffers performance loss(0.1 AP degradation) . See Appendix] 9for the details.\nInspired by this observation, we propose a learnable decoder cross-attention map predictor t0 spar- existing methods sify encoder tokens In the Carion et al.I2020; Zhu et al.120217 the encoder takes the backbone features combined with corresponding positional embeddings, as all the tokens, i.e. Meanwhile, our approach distinguishes encoder tokens to be refer- input without discrimination. Therefore, this can enced later in the decoder and considers only those tokens in self-attention_ significantly reduce the number of encoder tokens involved in the computation and reduce the total We further propose the encoder auxiliary loss for selected encoder tokens to computational cost. improve detection performance while minimizing computational overhead.  The proposed auxiliary loss not only improves performance, but also allows training a larger number of encoder layers_\nExtensive experiments on the COCO 2017 benchmark Lin et al. 2014 demonstrate that Sparse DETR effectively reduces computational cost while achieving better detection performance. With- out bells and whistles, Sparse DETR using Swin-T (Liu et al,7[2021 backbone achieves 48.2 AP 389 reduction of the entire computational cost compared to the 48.0 AP baseline and 49.2 AP with using In the case of the experiment that achieves 48.2 AP only 10% of encoder with 23% reduction tokens, the computational cost of the transformer encoder block is reduced by approximately 82%.\nWe summarize our contributions as follows:\nWe propose encoder token sparsification method for an efficient end-to-end object detector; efficiency enables stack by which we lighten the attention complexity in the encoder: This ing more encoder layers than Deformable DETR, leading to performance improvement within the same computational budget: sparsification criteria t0 sample the informative subset from the en- We propose two novel Map tire token set: Objectness Score (OS) and Decoder cross-Attention DAM) Based on the decoder cross-attention map criterion, the sparsified model preserves detection perfor- mance even when using only 10% of the whole tokens. We adopt an encoder auxiliary loss only for the selected tokens This additional loss learning process, but also greatly improves performance, with only not only stabilizes the marginally increased training time.\nRELATED WORK 2\nEfficient computation in vision transformers: It is a well-known problem that the attention com- putation in Transformers incurs the high time and memory complexity. The vision transformers need digest even bigger token sets as input $0 that a large body of works 2018; Child] Parmar et al: to 2020; Katharopoulos et al.7/2020, 2019} [Wang et al.] 2019a Ho et al. Choromanski et al. et al_ has been proposed lightweight attention mechanisms for them: 2021 Kitaev et al.12020 Most of single-scale attention module, which those works shed light on the complexity that resides only in hinders direct extension to the multi-scale features generally required in object detection\nOne of the promising approaches for the lighter transformer attention is input-dependent token spar- and IA-RED? Dynamic ViT Rao et al. /2021 Pan et al.| /20217 sification similar t0 our work both propose jointly-learned token selectors generating the sparsity patterns to be overlaid on the Those approaches mainly focus on sparsifying a backbone network evaluated on the input tokens_ classification tasks, while our interest lies in a sparse encoder of the end-to-end object detectors.\nsharing the spirit with ours in that they On the other hand, there has been a line of works aim at Zhu et al.7/20217 sparse transformers in the DETR-based framework: Deformable DETR conducts sparse attention computation by sampling only a fraction of the entire key set with learnable 2-d off- reasonable computational cost: which enables to use multi-scale feature maps with It can be sets viewed as a key sparsification method but with dense queries while our approach further reduces the Wang shortens the token length of et al.1/20217 query set pursuing even more sparsity. PnP-DETR introducing the Polling and Pull (PnP) module to sample the foreground the transformer encoder by tokens and condense the background tokens into a smaller set. However; their method cannot naively be integrated with Deformable DETR, since their sparsification breaks the 2d spatial structure of the On the contrary, Sparse DETR preserves the 2d token set assumed in the deformable attention: sample space of the set and can be seamlessly combined with the deformable attention, which facil- handling the multi-scale features. Thus, our approach benefits from both the deformable itates gets key sampling and the proposed query sparsification: we propose explicit objectives for Most of all the token selection network, whereas the aforementioned works have no explicit objective implying selection strategy, merely relying on the final detection objective. their beliefs in a good\nAuxiliary loss Auxiliary Loss. Lee et al.7[2015 [Szegedy et al. 7[2015) is widely adopted to deliver gradients to the early layers of matching networks. DETR variants employ auxiliary Hungarian deep layer directly layer objectives at the end of every decoder with extra FFN heads so that each decoder learns to detect the correct number of objects out of the decoder'$ outputs Unlike the decoder' s object queries whose number is relatively small(e-g: 300), the number of encoder s tokens has much Thus, extending the layerwise auxiliary loss to the using larger scales when multi-scale features. multi-scale encoder increases the training time cost by feeding too many tokens to the attached FFN In Sparse DETR, thanks to the sparsity already induced in the encoder; heads. we can instantly economize that cost while enjoying the auxiliary gradients in a wider range of intermediate layers.\nAPPROACH 3\nformulating generalized saliency-based we present our main contributions: In this section, (a token sparsification scheme for the encoder; (b) proposing the effective saliency criteria with which and (c) employing the encoder auxiliary losses and the top-k that scheme practically work; can Before describing the details, we revisit the decoder query selection to improve the performance. kkey components of DETR Carion et al.1/2020 and Deformable DETR Zhu et al.1/2021\n3.1 PRELIMINARY\nspatial feature map Xfeat RNXD DETR DETR takes the flattened from a backbone network into the transformer encoder; where N denotes the number of tokens (i.e. features) and D denotes token dimension The encoder iteratively updates by several vanilla self-attention modules. Then, the Xfeat transformer decoder takes both the refined encoder tokens (i.e. encoder output) and M learnable bounding object queries {qi}i-1-M as inputs and predicts a tuple of a class score [0, 1] and a [0, 1]4 for each object query qi, denoted as {yi {(ci, bi)}, where C denotes the number box b = AIl components including the backbone network are jointly trained by performing the of classes_ bipartite matching between the ground truth {Yi_ and predictions {yi}\nDeformable DETR. Deformable DETR replaces the vanilla dense attention, which is the main This significantly reduces computational bottleneck in DETR, with a deformable attention module_ the computational cost and improves the convergence.  Suppose that we have the same size of keys set of queries (denoted as 024 (Sk: and a set of (denoted which [Sq [ as Slk) means The conventional dense attention computes the attention weight Aqk for every {(4, = N . pair Sk }, resulting in quadratic complexity with respect to N. (q? Deformable attention k \u20ac \u20ac q reduces this quadratic complexity into the linear one by oly considering relevant keys for each query: Specifically, deformable attention computes attention weight Aqk for all queries and a small Sgk set of keys: {(9,k) where C Sk and |Sqk: K < N. q \u20ac nqk \u20ac Sqk:} =\nDue to this key sparsification, Deformable DETR is able to use the multi-scale features of the back bone network, improving the detection performance of small objects significantly: Paradoxically, US- ing the multi-scale feature increases the number of tokens in the transformer encoder by about 20x\nFigure Attention complexity. The circles in the square matrix represent the attention between 1 keys and queries. The gray/white circles correspond to preserved/removed connection respectively, and darker gray on the diagonal positions means where the token attends to itself: Dense attention a in DETR takes quadratic complexity: (b) Deformable DETR uses key sparsification, thus takes linear complexity: (c) Sparse DETR further uses query sparsification: Attention in Sparse DETR also takes linear complexity; but is much lighter than Deformable DETR s_\ncompared to DETR, making that the encoder becomes the computational bottleneck of deformable This motivates us to develop a sparsification method to reduce the number of tokens in the DETR encoder aggressively, which is described in the next sections\n3.2 ENCODER TOKEN SPARSIFICATION\nwe introduce our token sparsification scheme that the encoder module selectively In this section, from a small number of encoder tokens_ This encoder token subset is obtained the backbone refines feature map Xfeat with which is described in the subsequent section a certain criterion For features that are not updated in this process, the values of Xfeat are passed through the encoder layers without being changed:\nRd _ Rthat measures saliency of each token Formally, suppose that we have a scoring network g We then define p0-salient regions 0G as the top-p% tokens with the highest scores, for in Xfeat: N. Then, the i-th encoder layer updates given keeping ratio p, i.e. S = |n2g| [S4l [Sa | = 0 = the features Xi-1 by:\nwhere DefAttn refers to deformable attention, LN 20161 to layer normalization Ba et al. and Even in the case of unselected tokens, the values are still passed FFN to a feed-forward network. through the encoder layer; so they can be referenced as keys when updating the selected tokens. This losing the means that the unselected tokens can hand over information t0 the selected tokens without minimizing the computational cost: value of themselves while Here, we use deformable attention sparsification is applicable regardless of refining tokens in G, but the proposed encoder token for which attention method the encoder uses\nComplexity of Attention Modules in Encoder: Deformable DETR reduces the attention com- plexity through key sparsification, we further reduce the attention complexity through query and Conventional dense attention in DETR requires quadratic com- sparsification, as shown in Fig [] plexity O(N2) , Deformable attention requires linear complexity N is the query length: where Sparse attention requires only keys O(NK), where K N is the number of for each query: O(SK ), where S N is the number of salient encoder queries_\n3.3 FINDING SALIENT ENCODER TOKENS\nsalient token set 02G from a backbone feature In this section, we introduce how to find We Xfeat * determining saliency using a method for a cross attention map from the transformer de- propose Before presenting OUr approach, we first discuss a simple effective method based on the coder yet The limitation of this simple approach objectness scores obtained from separate detection head: motivates US to develop an advanced one, which is described in the following paragraph.\nFigure 2: scoring network by predicting binarized Decoder cross- on how to learn Illustration Map backpropagation Attention (DAM), where dashed orange arrow means The bottom path: box shows the forward/backward passes in Sparse DETR,and the top boxes present how to construct learning the scoring network. See Appendix[A Zlfor implementation details of scoring net: DAM for\nMeasuring objectness per each input token (i.e. Objectness Score: feature Xfeat of encoder is very natural to determine which ones from a backbone feature should be further updated in the pretrained backbone network is transformer encoder It is widely known that feature map from able to find the saliency of objects, which is why the region proposal network RPN) has been successfully adopted in many object detectors Ren et al.][2015; Dai et al.][2016  He et al.[2017 Inspired by this observation, we introduce an additional detection head and Hungarian loss t0 the backbone feature map, where the structure of the newly added head is the same as the one of the final detection head in the decoder: Then, we can select the top-p% encoder tokens with the highest This approach is effective to sparsify encoder tokens, but we salient token set $2g. class scores as believe that it is sub-optimal to the transformer decoder; because the selected encoder tokens from the separate detection head are not explicitly considered for the decoder:\nMap: subset of encoder to We consider another approach to select Decoder Cross-Attention kens that are highly relevant to the decoder in We observe that the cross- explicit manner: a more the transformer decoder could be used for measuring the saliency, because the from attention maps decoder gradually attends to a subset of encoder output tokens that are favorable to detect objects as scoring network that predicts a pseudo ground training continues Motivated by this, we introduce a truth of the saliency defined by decoder cross-attention maps, and use it to determine which encoder Fig: tokens should be further refined on the fly: scoring network, and I2summarizes how t0 train details are presented below:\nTo determine the saliency of each input token of encoder Xfeat; we have to aggregate the decoder cross-attentions between all object queries and the encoder output This procedure produces a single map of the same size as the feature map from the backbone which is defined as Decoder cross- (DAM). In the case of the dense attention; DAM can be easily obtained by summing Map Attention up attention maps from every decoder layer: In case of deformable attention, for each encoder token_ the corresponding value of DAM can be obtained by accumulating the attention weights of decoder object queries whose attention offsets are directed toward the encoder output tokens. Refer t0 the AppendixAA Zfor the details in the DAM creation_\nwe binarize DAM so that the top-p% (by attention weights) To train the scoring network, of encoder This is because our tokens is only retained. is t0 find a subset of encoder tokens that the goal small decoder references the most, rather than precisely predicting how much each encoder token will be referenced by the decoder: This binarized DAM implies the one-hot target that indicates whether each encoder token is included in the top-p% most referenced encoder tokens Then, we consider 4-layer scoring network g to predict how likely a given encoder token is included in the top-p % most referenced tokens, and the network is trained by minimizing the binary crOSs entropy (BCE) loss between the binarized DAM and prediction:\nFigure 3: Sparse DETR architecture Sparse DETR introduces three additional components: the scoring network, (b) auxiliary heads in the encoder; and (c) the auxiliary head to select the top-k saliency of encoder tokens by using scoring tokens for the decoder: Sparse DETR measures the the refining network, and selects the top-p% tokens, which is referred to as (1) in the diagram. After only the selected tokens in the encoder blocks, the auxiliary head selects the top-k tokens from the encoder output, which is served as the decoder object queries. This process is referred to as (2) in the diagram. In addition, we remark that additional auxiliary heads in each encoder block play a key achieving improved performance Only sparsified encoder tokens are passed to the encoder role in auxiliary heads for efficiency: All auxiliary heads in the encoder and decoder are trained with Hungarian loss as described in Deformable DETR 2021 Zhu et al\nmeans the binarized DAM value of the ith encoder token. bin where DAM\nOne may say that since DAM in the early phase of training is not accurate, pruning out the encoder tokens based on the result in the decoder degrades the final performance Or hurts the convergence empirically observe that the optimization is very stable even in the early phase of However; we training, and achieves better performance compared to the method based on objectness score. We describe detailed comparisons in the experiments section:\n3.4 ADDITIONAL COMPONENTS\nIn this section; we introduce two additional components: (a) auxiliary losses on the encoder tokens and (b) top-k decoder queries selection_ We empirically observe that these greatly help improve the optimization. including final performance and stabilize the The overall architecture of Sparse DETR Fig these components is depicted in 3\nAuxiliary Loss In DETR variants, auxiliary detection heads are attached to decoder Encoder layers, but not to encoder layers. significantly larger number of encoder tokens (about Due to 18k tokens) compared to decoder tokens (about 300) , encoder auxiliary heads will heavily increase the computational cost: In Sparse DETR, however; only part of encoder tokens are refined by the encoder; and adding auxiliary heads only for sparsified encoder tokens is not a big burden.\nWe empirically observe that applying an auxiliary detection head along with Hungarian loss on the selected tokens stabilizes the convergence of deeper encoders by alleviating the vanishing gradient We conjecture that; following the analysis in issue and even improves the detection performance. applying Hungarian loss at the intermediate layers helps distinguish the confusing Sun et al:] (2021 features in the encoder; which contributes to the detection performance in the final head.\nIn DETR and Deformable DETR, decoder queries are given by oly Top-k Decoder Queries learnable object queries O with predicted reference points via another head after the encoder: In Efficient DETR part of encoder output as input, similar to Rol Yao et al. /2021 decoder takes Pooling Ren et al.712015). Here, an auxiliary detection head is attached to the encoder output Xenc and the head calculates the objectness (class) score of each encoder output: Based on the score, the top-k encoder outputs are passed as decoder queries, similar to objectness score-based encoder token sparsification: Since this outperforms the methods based on learnable object queries O the two-stage scheme, we include this top-k decoder query selection in our final architecture.\nTable 1: Detection results of Sparse DETR on COCO 2017 val set: Top-k & BBR denotes that sample the top-k object queries instead of using the learned object queries Yao et al./2021 and we perform bounding box refinement in the decoder block Zhu et al.1/2021), respectively: Note that the proposed encoder auxiliary loss is only to Sparse DETR. FLOPs and FPS are measured applied 2021 The results marked by t; + are the reported ones from in the same way as used in Zhu et al: and Wang et al.7(2021 2021 respectively: Zhu et al:\nExPERIMENTS 4\nWe compare Sparse DETR with the conventional object detectors, including the recently proposed ones in the DETR family: In addition, we conduct an ablation study to support our claims in Sec- tion[] presenting the performance comparison between token selection criteria (OS vs. DAM), the effectiveness of the encoder auxiliary loss, and the dynamic sparsification during inference.\nHe et al. //2016) and Swin Transformer Implementation Details: We use ResNet-50 Liu et al. pre-trained backbone networks, where Swin Transformer is one of the state-of-the-art ar- 2021] as stack 6 encoder and 6 decoder layers, each with an auxiliary head at We chitecture in the ViT family: the end. We train the model on a 4x VIOO GPU machine with a total batch size of 16, for 50 epochs, learning rate is 0.0002 and decayed by 1/1O at the 40 epoch: where the initial Unless otherwise specified, we use the same hyperparameters as in Deformable DETR.\nCOMPARISON WITH OBJECT DETECTION BASELINES 4.1\nWe compare Sparse DETR with Faster-RCNN with FPN Lin et al.1/2017 Baselines. DETR Car- ion et al. 7/2020), Deformable DETR and PnP DETR (Wang et al 1/2021 Zhu et al.12021 We also compare with DETR and Deformable DETR that uses Swin-Tiny Liu et al./2021 as a backbone Here . for brevity, we denote Deformable DETR with the top-k object query selection and bound- In Sparse DETR , encoder tokens are sparsified with ing box refinement as Deformable DETR+. keeping ratios of 10%, 209, 309, 409, and 509, using DAM criterion We demonstrate detection performance and inference costs on COCO val20l7 dataset.\nshows the results of Sparse DETR and the other baselines on COCO val2017 Result: Table] set  Remarkably, on the ResNet-50 backbone, Sparse DETR with keeping ratio over 30% outper-\nFigure 5: Correlation Correla- graph: tion graphs of OS and DAM during training:\nFigure 4: Selection criteria.  Comparison of the performance with respect to encoder token selection criteria for different backbones_\nforms all the baselines while minimizing the computational cost: Even with the keeping ratio re- duced down to 10%, Sparse DETR still performs better than most baselines except for Deformable More surprisingly, on the Swin-T backbone, Sparse DETR oly with the keeping ratio DETR+ 10% outperforms all the baselines with no exception, while improving FPS by 38% compared to Deformable DETR+\nRemark that, compared to the most competitive baseline, Deformable DETR+, the improvement in APL is relatively noticeable on the Swin-T backbone even under the extreme sparsity of 10%, while the performance gap on the ResNet-SO backbone comes evenly from different sizes of objects We wider region of information than single token in Swin-T can hold conjecture that it is because the one in ResNet-5O, So even if we aggressively sparsify the encoder token, the network seems to have enough information to detect objects.\nCoMPARISON BETWEEN TOKEN SELECTION CRITERIA 4.2\nTo verify the benefits of the proposed saliency criteria, we compare three token spar- Baselines. Map sification criteria: random; Objectness Score (OS), and Decoder cross-Attention (DAM): The Note that the proposed encoder auxiliary random baseline samples a fixed ratio of arbitrary tokens_ loss is applied for all the methods_\nthe random strategy suffers noticeable performance degradation As illustrated in Fig: Result: On the other hand, the DAM-based model outperforms the OS-based model at every ratio and almost See the AppendixEA catches up with the non-sparse baseline when using SO% of encoder tokens experiment for detailed results of this\nTo analyze the reason that DAM-based model outperforms its counterpart, we measure the overlap between the encoder tokens referred by the decoder and the tokens refined by the encoder: As metric, we compute a scalar correlation Corr as:\nwhere @p is the encoder token set referred by the decoder and DAMz is the DAM-value corre- This Corr metric indicates the ratio of tokens polished by the encoder among sponding to token \u20ac the tokens referred by the decoder:\nFig: [] demonstrates that Corr of DAM-based model rises higher than that of OS-based model sparsification method for the decoder; This result implies that DAM-based model is a more suitable because the tokens referenced by the decoder are explicitly refined in the encoder; which achieves better detection performance. See the Appendix]Afor detailed results of this experiment:\nEFFECTIVENESS OF THE ENCODER AUXILIARY LOSS 4.3\nOwing to the sparsified token set in Our model apply the auxiliary loss to the encoder layers we can without sacrificing too much computational cost. Apart from improved efficiency and performance, we find another benefit of the encoder auxiliary loss that allows us to safely stack more encoder layers without failing to converge.\nthe encoder auxiliary loss not only enhances detection performance, but also As shown in Fig: consistently increases detection performance as the encoder layers doubled t0 12 However; we ob- serve that the training without its assistance utterly fails when using 12 encoder layers_ We argue that gradient propagated through decoder cross-attention vanishes as we stack more encoder lay- gradients from the auxiliary loss are required. The observations reported in ers, thus intermediate Appendix]A Jsupports this assertion and Appendix]ddetails the results of Fig:\n4.4 DYNAMIC SPARSIFICATION FOR INFERENCE STAGE\nTo deploy the models in various hardware conditions of real-world applications, one often should retrain them at different scales according to the performance-computation trade-off required. We fixed sparsity can adapt well to dynamic sparsity at inference evaluate if our model trained with (shows the performance under the var- time to check out Sparse DETR can avoid that hassle. Figurel during using ` ied keeping ratio inference when the model trained the Swin-T backbone and 30% p) When the inference keeping ratio is small , the per- of encoder tokens with the DAM-based method. formance of dynamic sparsification is slightly degraded, but the overall performance is satisfactory keeping ratios given that only a single model is used. at various\nPnP DETR introduces dynamic ratio training to achieve similar performance to the fixed keeping ra- significant performance degradation tio counterpart. However; without the additional trick, it suffers AP drop when traininglinference keeping ratio is 0.33/0.5, despite the increased for instance, 5.0 On the contrary, Sparse DETR achieves 0.2 AP improvement in number of encoder tokens a simi- To conclude, our method shows lar condition where the traininglinference keeping ratio is 0.3/0.5. showing better robustness compared to PnP DETR without further treatment; greater potential of dynamic adaptation to different hardware environments Note that any technique such as dynamic training is orthogonal to Our method and introducing it may bring even more robustness ratio\nCONCLUSION 5\nIn this paper; we have presented encoder token sparsification algorithm that lowers the computational cost of the encoder; which is a computational bottleneck in the DETR and Deformable DETR. By doing So, the proposed Sparse DETR architecture outperforms the Deformable DETR even when using only 10 % of the encoder token, and decreases the overall computation by 389, and increases the FPS by 42% compared to the Deformable DETR. We hope that our proposed method will provide insights t0 effectively detect objects in the transformer structure in the future.\nREFERENCES \n Now the paper ends. \nWhat are the limitations and potential drawbacks of the proposed Sparse DETR approach?", "output": "Potential answers:\n\n1. The proposed approach relies on a learned token selection network to determine salient encoder tokens. This requires training an additional scoring network, which may introduce extra complexity. There is also the potential for the learned scoring network to fail, resulting in poor token selection.\n\n2. The encoder auxiliary loss further increases computational cost, though the authors claim it is not a major overhead due to the sparsity in encoder tokens. Still, the additional loss introduces some extra computation and training overhead. \n\n3. The dynamic sparsification approach during inference has not been rigorously tested and compared to more established techniques like dynamic sparsity training. It is not clear how well it would generalize beyond the experiments in the paper.\n\n4. The sparsification is applied uniformly across encoder layers, but some layers may benefit from higher or lower sparsity than others. Applying different sparsity levels per layer could potentially further improve efficiency and performance.\n\n5. The approach relies on heuristics like selecting the top-p% salient tokens based on attention or objectness scores. There may be better, data-driven approaches to determining optimal sparsity levels.\n\nIn summary, while the proposed Sparse DETR approach shows promising gains in efficiency, there are some additional complexities introduced as well as room for improvement in the sparsification techniques and generalizability of the approach. Further experimentation and analysis would help strengthen the conclusions and address some of the limitations."}]