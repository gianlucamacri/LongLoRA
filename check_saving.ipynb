{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "config_original = transformers.AutoConfig.from_pretrained(\n",
    "        model_name_or_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9f0c3932ef4f1aa915140721ab2a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_original = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config_original,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_or_path = '../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save/full_model_hf/'\n",
    "config_saved = transformers.AutoConfig.from_pretrained(\n",
    "        saved_model_name_or_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save/full_model_hf/ were not used when initializing LlamaForCausalLM: ['model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'lm_head.0.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save/full_model_hf/ and are newly initialized: ['model.layers.3.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'lm_head.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_saved = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    saved_model_name_or_path,\n",
    "    config=config_saved,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32001, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_or_path_3 = '../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_3/full_model/'\n",
    "config_saved_3 = transformers.AutoConfig.from_pretrained(\n",
    "        saved_model_name_or_path_3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_3/full_model/ were not used when initializing LlamaForCausalLM: ['model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'lm_head.0.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_3/full_model/ and are newly initialized: ['model.layers.26.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'lm_head.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_saved_3 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    saved_model_name_or_path_3,\n",
    "    config=config_saved_3,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32001, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_3.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_or_path_2 = '../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_2/full_model_hf/'\n",
    "config_saved_2 = transformers.AutoConfig.from_pretrained(\n",
    "        saved_model_name_or_path_2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_2/full_model_hf/ were not used when initializing LlamaForCausalLM: ['model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'lm_head.0.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save_2/full_model_hf/ and are newly initialized: ['model.layers.26.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'lm_head.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_saved_2 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    saved_model_name_or_path_2,\n",
    "    config=config_saved_2,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32001, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_2.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(125175015)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the two models are equal, meaning that the training does not mody the base model\n",
    "(peft_model_saved_3.model.model.embed_tokens.weight == model_saved_2.model.embed_tokens.weight).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131076096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_3.model.embed_tokens.weight.shape[0]*model_saved_3.model.embed_tokens.weight.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549797317735188"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "125175015/131076096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../output/hdd_output/LongLoRA/camera_7_8k_llama2_chat_sftq_test_save/full_model_hf/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = peft.PeftConfig.from_pretrained('/'.join(saved_model_name_or_path_3.split('/')[:-2]))\n",
    "peft_model_saved_3 = peft.PeftModel.from_pretrained(model_saved_3, '/'.join(saved_model_name_or_path_3.split('/')[:-2]), config=config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = peft.PeftConfig.from_pretrained('/'.join(saved_model_name_or_path_2.split('/')[:-2]))\n",
    "peft_model_saved_2 = peft.PeftModel.from_pretrained(model_saved_2, '/'.join(saved_model_name_or_path_2.split('/')[:-2]), config=config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_saved_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k for k,v in peft_model_saved.named_parameters() if v.requires_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(peft_model_saved.model.model.embed_tokens.weight == model_saved.model.embed_tokens.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(peft_model_saved_2.model.model.embed_tokens.weight == model_saved_2.model.embed_tokens.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_saved.base_model.save_pretrained('test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el.weight[:32000,:].shape == model_original.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1921e-06, -1.7881e-06, -4.2915e-06,  ...,  8.3447e-07,\n",
       "         -6.4373e-06,  8.9407e-07],\n",
       "        [ 1.8387e-03, -3.8147e-03,  9.6130e-04,  ..., -9.0332e-03,\n",
       "          2.6550e-03, -3.7537e-03],\n",
       "        [ 1.0193e-02,  9.7656e-03, -5.2795e-03,  ...,  2.9297e-03,\n",
       "          4.0817e-04, -5.0964e-03],\n",
       "        ...,\n",
       "        [-1.3550e-02, -3.5095e-03, -1.8921e-02,  ..., -9.3384e-03,\n",
       "          8.7891e-03, -1.2741e-03],\n",
       "        [-1.0681e-02,  8.9722e-03,  1.2573e-02,  ..., -3.3691e-02,\n",
       "         -1.6235e-02,  3.0212e-03],\n",
       "        [-9.0942e-03, -1.8082e-03, -6.9809e-04,  ...,  3.8452e-03,\n",
       "         -1.2085e-02,  7.2861e-04]], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_original.model.embed_tokens.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongLora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
